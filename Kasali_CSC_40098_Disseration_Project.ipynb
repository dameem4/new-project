{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6GPXXZvRedrX",
        "QZ5xmqzjFiqv",
        "v7Zz96DHvufe",
        "GOsKxSfZbCIX",
        "ooL6a8rV0s9H",
        "ZJ_dAfRLt9hI",
        "KGrBm27j8Qcr",
        "9V_Q1g7cZDuL",
        "HJeiXIYMpsf_",
        "30nhpSTzt272",
        "wdUESoKqHZ1g",
        "V8J2EhNrHcM6"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dameem4/new-project/blob/master/Kasali_CSC_40098_Disseration_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDpFbmTe4usX"
      },
      "source": [
        "# Table of Contents\n",
        "\n",
        "<details><summary>\n",
        "<font color='Blue'> I. Installation of MMF & dependencies </font></summary>\n",
        "\n",
        "- Install MMF from source\n",
        "</details>\n",
        "\n",
        "<details><summary>\n",
        "<font color='Blue'> II. Download the datasets & convert them into MMF format </font></summary>\n",
        "\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "<details><summary>\n",
        "<font color='Blue'> III. Feature Extraction </font></summary>\n",
        "\n",
        "</details>\n",
        "\n",
        "<details><summary>\n",
        "<font color='Blue'> IV. Fine-tuning pre-trained VisualBERT models on Hateful Memes </font></summary>\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "<details><summary>\n",
        "<font color='Blue'> V. Generate predictions for the Challenge (`test_unseen.jsonl`) </font></summary>\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE9_rJZfF_Rf"
      },
      "source": [
        "## <font color='green'> <b> I. Installation of MMF & dependencies </b> </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7V112rAEiK9"
      },
      "source": [
        "Please set your `$HOME` directory.\\\n",
        "**e.g.** For *Linux* users it can be: `\"/home\"`,\\\n",
        "For *Colab* it would be: `\"/content\"`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ATqmsZVmEhXc",
        "outputId": "6d514591-76ef-43a1-a529-f2467f6071af"
      },
      "source": [
        "import os\n",
        "home = \"/content\"\n",
        "os.chdir(home)\n",
        "os.getcwd()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JeJ7JdvoKPB"
      },
      "source": [
        "# Install specified versions of `torch` and `torchvision`, before installing mmf (causes an issue)\n",
        "!pip install torch torchvision -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rna7NAZtwaod"
      },
      "source": [
        "#### *Install MMF from source* \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtyIgblgvdoY"
      },
      "source": [
        "# Clone the following repo where mmf does not install default image features, \n",
        "# since we will use our own features\n",
        "!git clone https://github.com/facebookresearch/mmf.git\n",
        "%cd /content/mmf\n",
        "!pip install --editable ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf mmf"
      ],
      "metadata": {
        "id": "N9ExXvGgNUTa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHzXv9ogv3K8"
      },
      "source": [
        "os.chdir(os.path.join(home, \"mmf\"))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyX6Qos3Olyg"
      },
      "source": [
        "---\n",
        "## <font color='green'> <b> II. Download the datasets & convert them into *MMF* format </b> </font> <font color='red'><b> --Action required!-- </b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHJeqCgrEb_c"
      },
      "source": [
        "### <font color='Orchid'> <b> Hateful Memes  dataset </b> </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please download the `Hateful Memes Dataset` from the official challenge webpage: https://hatefulmemeschallenge.com/#download\n",
        "\n",
        "After filling the form the `hateful_memes.zip` file will be downloaded, which includes all the required data including images. Please define the variable `PATH_TO_ZIP_FILE` in the following code cell which stores the full path of the downloaded `.zip` file:\n"
      ],
      "metadata": {
        "id": "9JXlUJY4nZsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_ZIP_FILE = \"/content/drive/Othercomputers/MyLaptop/memes/hateful_memes.zip\"\n",
        "!cp -r $PATH_TO_ZIP_FILE /content/mmf/"
      ],
      "metadata": {
        "id": "FyBgXNuBn8yi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iyIDajcnTrx",
        "outputId": "fa946544-b140-49dd-a29c-f5b619b4aa13"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mmf"
      ],
      "metadata": {
        "id": "fP0goEU5Mgsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the mmf folder to Python Path\n",
        "os.environ['PYTHONPATH'] += \":/content/mmf/\""
      ],
      "metadata": {
        "id": "klaSRv6pqyaD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TJqJoKnoc1B"
      },
      "source": [
        "!mmf_convert_hm --zip_file=\"hateful_memes.zip\" --password DontTellYou"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mmf"
      ],
      "metadata": {
        "id": "F-DoI_i7Fvq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gb8KJT1aBWzr"
      },
      "source": [
        "# Check how many images we have in total\n",
        "!ls /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/images/img/ | wc -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8txdoDJgV5H-"
      },
      "source": [
        "That means there are `12.140` **'uniquely named'** images in total and you might recall that the sizes of each set was the following:\n",
        "\n",
        "- `|train.jsonl| = 8.500`\n",
        "- `|dev_seen.jsonl| = 500`\n",
        "- `|dev_unseen.jsonl| = 540`\n",
        "- `|test_seen.jsonl| = 1.000`\n",
        "- `|test_unseen.jsonl| = 2.000`\n",
        "\n",
        "Well, this makes `8.500 + 500 + 540 + 1.000 + 2.000 = 12.540` in total. \\\n",
        "> *Is there something wrong?*\\\n",
        "> **TL;DR:** Nope. Some images in `dev_seen` are used in `dev_unseen`, too. To be specific, they have `400` common images. Hence, in total we have `12.540 - 400 = 12.140` *'unique'* images.\\\n",
        "See <font color='orange'> <b> Extras </b> </font> --> <font color='Gold'><b> Number of 'unique' (based on file names) images </b></font> at the end of this script to see the explanation in detail."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKUDTNzLxvXf"
      },
      "source": [
        "# Free up the disk by removing .zip, .tar files\n",
        "!rm -rf /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/images/hateful_memes.zip\n",
        "!rm -rf $home/mmf/hateful_memes.zip"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eECHBkozZ8R"
      },
      "source": [
        "### <font color='Orchid'> <b> Memotion dataset </b> </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KVJRAfSDLlR"
      },
      "source": [
        "There are 2 options for downloading the dataset: \n",
        "1. download the dataset (a `.zip` file) using `Kaggle API`\\\n",
        "OR\n",
        "2. download the dataset (a `.zip` file) from [Kaggle](https://www.kaggle.com/williamscott701/memotion-dataset-7k) directly, **(<font color='red' >preferred </font> if you're not familiar with Kaggle API)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feH6betK2mqB"
      },
      "source": [
        "#### <font color='Thistle'> <b> 1. Download Memotion dataset using `Kaggle API` </b> </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgtqtKPGpKQb"
      },
      "source": [
        "Check out the official documentation to get more information on Kaggle API and how to create a Kaggle API Key:\n",
        "- [Link#1](https://github.com/Kaggle/kaggle-api#api-credentials)\n",
        "- [Link#2](https://www.kaggle.com/docs/api)\n",
        "\n",
        "The API Key is stored in a file named `kaggle.jsonl`, which has the folowing line inside: \n",
        "`{\"username\":\"your_user_name\",\"key\":\"some_values_here\"}`\n",
        "\n",
        "> Upload the `kaggle.json` file to your `$HOME` directory and run the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgWGOly32rnV"
      },
      "source": [
        "# Install kaggle library\n",
        "!pip install -q kaggle\n",
        "# Create a directory where API key will be stored\n",
        "!mkdir -p ~/.kaggle\n",
        "# Move the API key to where Kaggle expects it to be\n",
        "!mv $home/kaggle.json ~/.kaggle/\n",
        "# Give according rights to the file\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "# Finally, download the dataset (.zip file)\n",
        "!kaggle datasets download -d williamscott701/memotion-dataset-7k\n",
        "# Unzip the data \n",
        "!unzip -qq memotion-dataset-7k.zip -d $home/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PFMUbGc2rzy"
      },
      "source": [
        "#### <font color='Thistle'> <b> 2. Download Memotion dataset directly from [Kaggle](https://www.kaggle.com/williamscott701/memotion-dataset-7k) </b> </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FJiGtSLEsI4"
      },
      "source": [
        "Download the dataset and put the `.zip` file into your `$HOME` directory and then run the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QNouszeEo8i"
      },
      "source": [
        "# Unzip the data \n",
        "# !unzip memotion-dataset-7k.zip -d $home/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwaTcWOctsA8"
      },
      "source": [
        "#### <font color='Thistle'> <b> Labeling Memotion Dataset </b> </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "791PwkoLFGmD"
      },
      "source": [
        "We have added `Memotion Dataset` to `Hateful Memes Dataset` and fine-tuned some models on the *aggregated* data. But there was no significant improvement seen neither on the `ROC-AUC score`, nor on the `accuracy`. We then discovered that the dataset is *horribly* labeled. Therefore, one needs to label the dataset.\n",
        "\n",
        "So we went through the dataset and cherry-picked the memes that would be suitable for the challenge, considering the idea of `Hateful Memes Challenge`.\n",
        "\n",
        "The following cell can be run to clone a repository which includes helpful scripts for the project such as; a script for labeling the `Memotion Dataset` and saving the data in the same format as the `Hateful Memes Dataset`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRqJ83FrVmSp",
        "outputId": "57508694-c3b2-4b19-c080-e4d3e1d2091b"
      },
      "source": [
        "os.chdir(home)\n",
        "!git clone https://github.com/facebookresearch/detectron2.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'hateful_memes-hate_detectron'...\n",
            "remote: Enumerating objects: 47, done.\u001b[K\n",
            "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 47 (delta 19), reused 43 (delta 15), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (47/47), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZG2LoOnX23V"
      },
      "source": [
        "Labeling the dataset is not necessary for reproducing our results but one can check out the [labeling script](https://github.com/rizavelioglu/hateful_memes-hate_detectron/tree/main/utils/label_memotion.py) and execute the following line of code to run the script and see how the labeling is done.\n",
        "\n",
        "```\n",
        "# Start labeling Memotion dataset and save it at the end\n",
        "%run $home/hateful_memes-hate_detectron/utils/label_memotion.py --home $home\n",
        "```\n",
        "\n",
        "---\n",
        "> In total, we have labeled $328$ memes.\\\n",
        "Check out the following file to find the ones we labeled: [/hateful_memes-hate_detectron/utils/label_memotion.jsonl](https://github.com/rizavelioglu/hateful_memes-hate_detectron/tree/main/utils/label_memotion.jsonl)\n",
        "\n",
        "Next, we move those labeled images from `Memotion Dataset` into the same folder where the images from `Hateful Memes Dataset` are, so that when the image features are being extracted all the images are inside the same folder.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrmqLTIhu_Mc"
      },
      "source": [
        "import pandas as pd\n",
        "# read the .jsonl file and get the img column\n",
        "labeled_memo_samples = pd.read_json(os.path.join(home, \"hateful_memes-hate_detectron/utils/label_memotion.jsonl\"), lines=True)['img']\n",
        "# parse the img entries and get the image names\n",
        "labeled_memo_samples = [i.split('/')[1] for i in list(labeled_memo_samples)]\n",
        "\n",
        "img_dir = os.path.join(home, f\"memotion_dataset_7k/images/\")\n",
        "for img in labeled_memo_samples:\n",
        "    os.rename(f\"{img_dir+img}\", f\"/root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/images/img/{img}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlCn72uPUxF_",
        "outputId": "cb60f1da-d182-435b-f458-5f7e41f61f76"
      },
      "source": [
        "# Check how many images we have in total\n",
        "!ls /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/images/img/ | wc -l"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12468\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tu5bDeGm8Q-c"
      },
      "source": [
        "### <font color='Orchid'> <b> Merging the two datasets to get a larger training data</b> </font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVYF-w_aCxWV"
      },
      "source": [
        "Simply execute the following cell which concatanates;\n",
        "- Labeled Memotion dataset,\n",
        "- Hateful Memes' training data, and\n",
        "- 100 images from `dev_seen.jsonl` that are not in `dev_unseen.jsonl`\n",
        "\n",
        "and generates `train_v10.jsonl`, which will be used for fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUL8Tgkh8WUz"
      },
      "source": [
        "!python $home/hateful_memes-hate_detectron/utils/concat_memotion-hm.py --home $home"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzAxev4zeXxx"
      },
      "source": [
        "---\n",
        "## <font color='green'> <b> III. Feature Extraction </b> </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CTm-rxS0e97"
      },
      "source": [
        "### <font color='lightgreen'> <b> Extract image features using [`mmf/tools/scripts/features/extract_features_vmb.py`](https://github.com/facebookresearch/mmf/blob/master/tools/scripts/features/extract_features_vmb.py) </b> </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yq5AjtXH74Ep"
      },
      "source": [
        "#### Install packages & repos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQfgIJ-004A5"
      },
      "source": [
        "import os\n",
        "os.chdir(home)\n",
        "!git clone https://gitlab.com/vedanuj/vqa-maskrcnn-benchmark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4Lkv0F3-3pD"
      },
      "source": [
        "!pip install ninja yacs cython matplotlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCzMvexR6so_"
      },
      "source": [
        "os.chdir(os.path.join(home, \"vqa-maskrcnn-benchmark\"))\n",
        "!rm -rf build\n",
        "!python setup.py build develop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oj_Fwxta79d2"
      },
      "source": [
        "#### Extract!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLFTDh735BDr"
      },
      "source": [
        "# !wget https://dl.fbaipublicfiles.com/pythia/detectron_model/FAST_RCNN_MLP_DIM2048_FPN_DIM512.pkl\n",
        "# !wget https://dl.fbaipublicfiles.com/pythia/detectron_model/e2e_faster_rcnn_X-101-64x4d-FPN_1x_MLP_2048_FPN_512.yaml\n",
        "os.chdir(os.path.join(home, \"mmf/tools/scripts/features/\"))\n",
        "out_folder = os.path.join(home, \"features/\")\n",
        "\n",
        "!python extract_features_vmb.py --config_file \"https://dl.fbaipublicfiles.com/pythia/detectron_model/detectron_model_x152.yaml\" \\\n",
        "                                --model_name \"X-152\" \\\n",
        "                                --output_folder $out_folder \\\n",
        "                                --image_dir \"/root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/images/img/\" \\\n",
        "                                --num_features 100 \\\n",
        "                                # --exclude_list \"/content/exclude.txt\"\n",
        "                                # --feature_name \"fc6\" \\\n",
        "                                # --confidence_threshold 0. \\"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJ19s6lUiL9w"
      },
      "source": [
        "---\n",
        "## <font color='green'> <b> IV. Fine-tuning pre-trained VisualBERT models on Hateful Memes </b> </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZyV2LQGAgB1"
      },
      "source": [
        "*italicised text*### <font color='Violet'> <b> Fine tuning  </b> </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKfM87TjYpxY"
      },
      "source": [
        "\"\"\"\n",
        "Uncomment it if needed\n",
        "\"\"\"\n",
        "\n",
        "# os.chdir(home)\n",
        "# # Define where image features are\n",
        "# feats_dir = os.path.join(home, \"features\")\n",
        "# # Define where train.jsonl is\n",
        "# train_dir = os.path.join(home, \"train_v9.jsonl\")\n",
        "\n",
        "# !mmf_run config=\"projects/visual_bert/configs/hateful_memes/from_coco.yaml\" \\\n",
        "#         model=\"visual_bert\" \\\n",
        "#         dataset=hateful_memes \\\n",
        "#         run_type=train_val \\\n",
        "#         checkpoint.max_to_keep=1 \\\n",
        "#         checkpoint.resume_zoo=visual_bert.pretrained.cc.full \\\n",
        "#         training.tensorboard=True \\\n",
        "#         training.checkpoint_interval=50 \\\n",
        "#         training.evaluation_interval=50 \\\n",
        "#         training.max_updates=3000 \\\n",
        "#         training.log_interval=100 \\\n",
        "#         dataset_config.hateful_memes.max_features=100 \\\n",
        "#         dataset_config.hateful_memes.annotations.train[0]=$train_dir \\\n",
        "#         dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_unseen.jsonl \\\n",
        "#         dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_unseen.jsonl \\\n",
        "#         dataset_config.hateful_memes.features.train[0]=$feats_dir \\\n",
        "#         dataset_config.hateful_memes.features.val[0]=$feats_dir \\\n",
        "#         dataset_config.hateful_memes.features.test[0]=$feats_dir \\\n",
        "#         training.lr_ratio=0.3 \\\n",
        "#         training.use_warmup=True \\\n",
        "#         training.batch_size=32 \\\n",
        "#         optimizer.params.lr=5.0e-05 \\\n",
        "#         env.save_dir=./sub1 \\\n",
        "#         env.tensorboard_logdir=logs/fit/sub1 \\"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5BfCb_YdeX4"
      },
      "source": [
        "##### **Visualize losses/accuracy via Tensorboard**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeFuYAVzI_Nx"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "# %load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVB4eQ1gOw4j"
      },
      "source": [
        "# %tensorboard --logdir logs/fit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTUjwxmHGxVm"
      },
      "source": [
        "---\n",
        "## <font color='green'> <b> V. Generate predictions for the Challenge (`test_unseen.jsonl`) </b> </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8zO7Tj98Nfp"
      },
      "source": [
        "*italicised text*### <font color='Thistle'> <b> Testing Phase 1 </b> </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GB3EFVnMIC9h"
      },
      "source": [
        "\"\"\"\n",
        "Uncomment it if needed\n",
        "\"\"\"\n",
        "\n",
        "# os.chdir(home)\n",
        "# # where checkpoint is\n",
        "# ckpt_dir = os.path.join(home, \"sub1/best.ckpt\")\n",
        "# feats_dir = os.path.join(home, \"features/feats_hm\")\n",
        "\n",
        "# !mmf_predict config=\"projects/visual_bert/configs/hateful_memes/defaults.yaml\" \\\n",
        "#     model=\"visual_bert\" \\\n",
        "#     dataset=hateful_memes \\\n",
        "#     run_type=test \\\n",
        "#     checkpoint.resume_file=$ckpt_dir \\\n",
        "#     checkpoint.reset.optimizer=True \\\n",
        "#     dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_unseen.jsonl \\\n",
        "#     dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_unseen.jsonl \\\n",
        "#     dataset_config.hateful_memes.features.train[0]=$feats_dir \\\n",
        "#     dataset_config.hateful_memes.features.val[0]=$feats_dir \\\n",
        "#     dataset_config.hateful_memes.features.test[0]=$feats_dir \\"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GPXXZvRedrX"
      },
      "source": [
        "\n",
        "### <font color='Gold'> <b> Image feature type conversion </b> </font>\n",
        "Convert image features from `.npy` --> `.lmdb` and vice versa\n",
        "\n",
        "You can also try to use the .npy files directly. Just point to the folder which contains those files in your config. lmdb is not a necessary requirement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZ5xmqzjFiqv"
      },
      "source": [
        "#### <font color='PaleGoldenrod'> <b> Convert .npy files to .lmdb </b> </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0N_mJ5MQFlKe"
      },
      "source": [
        "import argparse\n",
        "import glob\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "import lmdb\n",
        "import numpy as np\n",
        "import tqdm\n",
        "\n",
        "\n",
        "class LMDBConversion():\n",
        "    def __init__(self, features_folder, lmdb_path):\n",
        "        self.features_folder = features_folder\n",
        "        self.lmdb_path = lmdb_path\n",
        "\n",
        "    def convert(self):\n",
        "        env = lmdb.open(self.lmdb_path, map_size=1099511627776)\n",
        "        id_list = []\n",
        "        features = glob.glob(\n",
        "            os.path.join(self.features_folder, \"**\", \"*.npy\"), recursive=True\n",
        "        )\n",
        "\n",
        "\n",
        "        with env.begin(write=True) as txn:\n",
        "            for infile in tqdm.tqdm(features):\n",
        "                reader = np.load(infile, allow_pickle=True)\n",
        "                item = {}\n",
        "                split = os.path.relpath(infile, self.features_folder).split(\n",
        "                    \".npy\"\n",
        "                )[0]\n",
        "                item[\"feature_path\"] = split\n",
        "                key = split.encode()\n",
        "                id_list.append(key)\n",
        "\n",
        "                item[\"features\"] = reader.item().get(\"features\")\n",
        "                item[\"image_height\"] = reader.item().get(\"image_height\")\n",
        "                item[\"image_width\"] = reader.item().get(\"image_width\")\n",
        "                item[\"num_boxes\"] = reader.item().get(\"num_boxes\")\n",
        "                item[\"objects\"] = reader.item().get(\"objects\")\n",
        "                item[\"cls_prob\"] = reader.item().get(\"cls_prob\", None)\n",
        "                item[\"bbox\"] = reader.item().get(\"bbox\")\n",
        "\n",
        "                txn.put(key, pickle.dumps(item))\n",
        "\n",
        "            txn.put(b\"keys\", pickle.dumps(id_list))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCEy9BfnFsVW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "39cf2c61-5558-4d7d-db6c-27d9d0ad002b"
      },
      "source": [
        "features_folder = '/content/features/'\n",
        "lmdb_path = \"/content/\"\n",
        "lmdb_converter = LMDBConversion(features_folder, lmdb_path)\n",
        "lmdb_converter.convert()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 16522/16522 [01:30<00:00, 181.86it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7Zz96DHvufe"
      },
      "source": [
        "#### <font color='PaleGoldenrod'> <b> Convert .lmdb to .npy </b> </font>\n",
        "just to check if everything's okay"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_4VqHRlLBz_"
      },
      "source": [
        "features_folder = \"/content/features_from_lmdb/\"\n",
        "lmdb_path = \"/content/drive/MyDrive/\"\n",
        "\n",
        "def extract():\n",
        "    os.makedirs(features_folder, exist_ok=True)\n",
        "    env = lmdb.open(\n",
        "        lmdb_path,\n",
        "        max_readers=1,\n",
        "        readonly=True,\n",
        "        lock=False,\n",
        "        readahead=False,\n",
        "        meminit=False,\n",
        "    )\n",
        "    with env.begin(write=False) as txn:\n",
        "        _image_ids = pickle.loads(txn.get(b\"keys\"))\n",
        "        for img_id in tqdm.tqdm(_image_ids):\n",
        "            item = pickle.loads(txn.get(img_id))\n",
        "            img_id = img_id.decode(\"utf-8\")\n",
        "            \n",
        "            tmp_dict = {\n",
        "                \"image_id\"    : img_id,\n",
        "                \"bbox\"        : item[\"bbox\"],\n",
        "                \"num_boxes\"   : item[\"num_boxes\"],\n",
        "                \"image_height\": item[\"image_height\"],\n",
        "                \"image_width\" : item[\"image_width\"],\n",
        "                \"objects\"     : item[\"objects\"],\n",
        "                \"cls_prob\"    : item[\"cls_prob\"],\n",
        "            }\n",
        "\n",
        "            info_file_base_name = str(img_id) + \"_info.npy\"\n",
        "            file_base_name = str(img_id) + \".npy\"\n",
        "\n",
        "            np.save(\n",
        "                os.path.join(features_folder, file_base_name),\n",
        "                item[\"features\"],\n",
        "            )\n",
        "            np.save(\n",
        "                os.path.join(features_folder, info_file_base_name),\n",
        "                tmp_dict,\n",
        "            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiU5UlJZLdyg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "97c35a12-00e3-4797-dd2d-cade53633aef"
      },
      "source": [
        "extract()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 16522/16522 [04:24<00:00, 62.54it/s] \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_yEziJrtG_E"
      },
      "source": [
        "data = np.load(\"/content/features_from_lmdb/01243.npy\", allow_pickle=True)\n",
        "data_info = np.load(\"/content/features_from_lmdb/01243_info.npy\", allow_pickle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAiWj4QetV-9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 958
        },
        "outputId": "647d8629-1b5b-4c53-8005-c4c3fad5f72d"
      },
      "source": [
        "data_info.item()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bbox': array([[1.39431671e+02, 2.48186661e+02, 4.36927917e+02, 7.72290344e+02],\n",
              "        [1.02417374e+02, 2.38131317e+02, 3.80355438e+02, 6.80631226e+02],\n",
              "        [2.33086594e+02, 3.11982147e+02, 3.19798279e+02, 3.83706116e+02],\n",
              "        [2.26076691e+02, 3.67014771e+02, 3.88840271e+02, 5.06836884e+02],\n",
              "        [1.80767273e+02, 1.68776794e+02, 5.02467957e+02, 6.86551453e+02],\n",
              "        [1.05255386e+02, 2.35436798e+02, 4.55437012e+02, 6.23370544e+02],\n",
              "        [1.89080795e+02, 6.48054321e+02, 2.63529999e+02, 7.08438538e+02],\n",
              "        [1.52574844e+02, 2.76297699e+02, 4.75713959e+02, 6.58706787e+02],\n",
              "        [6.44385071e+01, 1.51709244e+02, 4.10103607e+02, 5.57978638e+02],\n",
              "        [1.49728775e+02, 1.49498413e+02, 4.97210022e+02, 5.43969360e+02],\n",
              "        [3.32468300e+01, 2.24651459e+02, 3.89471802e+02, 6.22743164e+02],\n",
              "        [7.07439804e+01, 1.73837357e+02, 5.24785095e+02, 5.11764557e+02],\n",
              "        [1.02569702e+02, 1.88668671e+02, 2.00243805e+02, 2.64704437e+02],\n",
              "        [6.80242767e+01, 3.26593445e+02, 4.18418884e+02, 7.36024597e+02],\n",
              "        [1.88232239e+02, 3.70306244e+02, 3.39500977e+02, 5.11890900e+02],\n",
              "        [1.18925705e+02, 3.18542938e+02, 5.23130981e+02, 7.26392517e+02],\n",
              "        [1.96913086e+02, 1.80159363e+02, 5.25000000e+02, 5.78617920e+02],\n",
              "        [1.32729828e+02, 9.72000504e+01, 4.14658844e+02, 4.83092133e+02],\n",
              "        [9.62953949e+00, 2.99616455e+02, 3.93895477e+02, 7.00357117e+02],\n",
              "        [1.80882446e+02, 3.15203644e+02, 4.72627289e+02, 7.84119934e+02],\n",
              "        [1.46937897e+02, 2.04017242e+02, 4.22434113e+02, 3.89029205e+02],\n",
              "        [1.63674759e+02, 9.84303665e+01, 5.25000000e+02, 4.55100220e+02],\n",
              "        [2.17622421e+02, 1.25299858e+02, 5.25000000e+02, 5.10953186e+02],\n",
              "        [1.43202637e+02, 3.79688934e+02, 4.89972748e+02, 7.50397278e+02],\n",
              "        [2.68071957e+01, 2.85289001e+02, 4.93618195e+02, 6.67867981e+02],\n",
              "        [6.37104034e+00, 1.10916748e+02, 4.82961243e+02, 4.56360352e+02],\n",
              "        [0.00000000e+00, 5.07552612e+02, 5.04203247e+02, 8.00000000e+02],\n",
              "        [7.65641556e+01, 4.47373077e+02, 5.25000000e+02, 7.98994568e+02],\n",
              "        [2.46828747e+00, 3.62331268e+02, 4.73057465e+02, 7.37618347e+02],\n",
              "        [3.29232502e+00, 1.11041145e+02, 3.54867981e+02, 5.11391083e+02],\n",
              "        [0.00000000e+00, 4.69154358e+01, 4.02923615e+02, 4.41583679e+02],\n",
              "        [0.00000000e+00, 2.38482729e-01, 5.06900879e+02, 1.93357452e+02],\n",
              "        [2.84813862e+01, 4.65717529e+02, 4.11312286e+02, 7.98123047e+02],\n",
              "        [2.08527203e+01, 2.02935982e+01, 3.98246429e+02, 1.24067238e+02],\n",
              "        [2.23878906e+02, 2.55431656e+02, 5.25000000e+02, 6.76580750e+02],\n",
              "        [2.40209854e+02, 1.32229462e+02, 5.25000000e+02, 6.47340759e+02]],\n",
              "       dtype=float32),\n",
              " 'cls_prob': array([0.97277766, 0.9675791 , 0.9496083 , 0.9433131 , 0.94258374,\n",
              "        0.9389874 , 0.92105645, 0.916983  , 0.8853556 , 0.85249406,\n",
              "        0.84176844, 0.836384  , 0.8228576 , 0.7948613 , 0.7651777 ,\n",
              "        0.7458195 , 0.6937541 , 0.6712092 , 0.6335867 , 0.6231661 ,\n",
              "        0.5907792 , 0.55343837, 0.5393082 , 0.52035236, 0.5122632 ,\n",
              "        0.5007001 , 0.3869602 , 0.3839841 , 0.34660023, 0.31152597,\n",
              "        0.30798504, 0.28977618, 0.28290242, 0.2656458 , 0.26335025,\n",
              "        0.25753808], dtype=float32),\n",
              " 'image_height': 800,\n",
              " 'image_id': '01243',\n",
              " 'image_width': 525,\n",
              " 'num_boxes': 36,\n",
              " 'objects': array([ 117,  117,  391,  198,  117,  117,  786,  117,  117,  117,  117,\n",
              "         117,  274,  117,  198,  117,  117,  117,  117,  117,  327,  117,\n",
              "         117,  117,  117,  117,  222,  222,  222,  117,  222,  381,  222,\n",
              "        1038,  117,  117])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4apQdfQzaQG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3d00d686-9ec1-4ace-fc26-54e9c85868a2"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(36, 2048)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOsKxSfZbCIX"
      },
      "source": [
        "### <font color='Gold'><b> Number of 'unique' (based on file names) images </b></font>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_ipIAgOa_kd"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "annotation_dir = \"/root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/annotations\"\n",
        "img_dir = \"/root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/images/img/\"\n",
        "\n",
        "# Collect all the annotations (from Phase-2)\n",
        "train       = pd.read_json(f\"{annotation_dir}/train.jsonl\", lines=True)\n",
        "dev_seen    = pd.read_json(f\"{annotation_dir}/dev_seen.jsonl\", lines=True)\n",
        "dev_unseen  = pd.read_json(f\"{annotation_dir}/dev_unseen.jsonl\", lines=True)\n",
        "test_seen   = pd.read_json(f\"{annotation_dir}/test_seen.jsonl\", lines=True)\n",
        "test_unseen = pd.read_json(f\"{annotation_dir}/test_unseen.jsonl\", lines=True)\n",
        "\n",
        "# Create 2 sets: \n",
        "#   A set of strings, 'a': for all the image names,\n",
        "#   A set of lists, 'b': for all the image names in dataset, e.g. train, dev_seen, etc.\n",
        "a = os.listdir(f\"{img_dir}\")\n",
        "b = []\n",
        "for i in [train, dev_seen, dev_unseen, test_seen, test_unseen]:\n",
        "    b.append(list(i[\"img\"].str.split(\"/\").str.get(1)))\n",
        "\n",
        "set_mapping = ['train', 'dev_seen', 'dev_unseen', 'test_seen', 'test_unseen']\n",
        "total_size = 0\n",
        "print(\"#of images in: \")\n",
        "for idx, i in enumerate(b):\n",
        "    total_size += len(set(i))\n",
        "    print(f\"\\t'{set_mapping[idx]}'  \\t:\", len(set(i)))\n",
        "else:\n",
        "    print(f\"\\nIn total there are {total_size} images,\",\n",
        "          \"\\nBut the # of images in /img/ directory is: \", len(a))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-SFAVsnmqcw"
      },
      "source": [
        "# First, let's check if all the images are within jsonl files, in other words: \n",
        "# 'do we have an image in /img folder that's not in one of the .jsonl files?'\n",
        "# 0 means every image in /img directory is in a jsonl file\n",
        "print(\"#of images that are not in one of the .jsonl files: \", \n",
        "      len(set(a).symmetric_difference(set(b[0] + b[1] + b[2] + b[3] + b[4]))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPuWl4ZCq28s"
      },
      "source": [
        "print(\"#of same images in between: \")\n",
        "for i in range(0, 5):\n",
        "    print(\"\\n\")\n",
        "    for j in range(0, 5):\n",
        "        if i != j:\n",
        "            print(f\"{set_mapping[i], set_mapping[j]}   \\t: {len(set(b[i]) & set(b[j]))}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-004uK7hv3p"
      },
      "source": [
        "As seen, `dev_seen.jsonl` and `dev_unseen.jsonl` have `400` same images. Let's double check that:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_W3G8kkjw0S",
        "outputId": "317f80d1-84ce-4bc0-accd-aa6b25fe45a8"
      },
      "source": [
        "print(f\"#of same images in {set_mapping[1], set_mapping[2]}: {len(set(b[1]) & set(b[2]))}\",\n",
        "      f\"\\n#of different images: {len(set(b[1]).symmetric_difference(set(b[2])))}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#of same images in ('dev_seen', 'dev_unseen'): 400 \n",
            "#of different images: 240\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZXyn01tvhN_"
      },
      "source": [
        "That means in Phase-2, `100` images were removed from `dev_seen.jsonl` and `140` new images are added to the validation set.\\\n",
        "Hence;  `|dev_unseen.jsonl|=500-100+140=540`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooL6a8rV0s9H"
      },
      "source": [
        "### <font color='Gold'> <b> Image Feature Extraction </b> </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJ_dAfRLt9hI"
      },
      "source": [
        "#### <font color='PaleGoldenrod'> <b> Discovering default image features from MMF </b> </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPLWtTofNUf_"
      },
      "source": [
        "Download the features for Phase-2 which was published by MMF on 01.10.2020 [[source of link]](https://github.com/facebookresearch/mmf/blob/518a5a675586e4dc1b415a52a8a80c75edfc2960/mmf/configs/zoo/datasets.yaml#L232)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtBCSXYHKzod"
      },
      "source": [
        "# Download the features for Phase-2 from the following link\n",
        "!wget https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/features/features_2020_10_01.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SF6cDF4MQ6qU"
      },
      "source": [
        "Extract `features_2020_10_01.tar.gz`:\n",
        "> this file actually extracts a folder `detectron.lmdb/` which\n",
        "stores the `data.mdb` file, which is where all the image features are compressed into."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKxRLf3QO27b"
      },
      "source": [
        "!tar -xzf /content/features_2020_10_01.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLUqOVApWsZq"
      },
      "source": [
        "Extract image features from `detectron.lmdb/` folder to `/features/`.\n",
        "\n",
        "**Note**\n",
        "Interrupt the execution as we only want to have a sneek peek into a few features. So, no need to extract the whole image features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkj0TfSkq6zi"
      },
      "source": [
        "!python /content/mmf/tools/scripts/features/lmdb_conversion.py \\\n",
        "        --mode \"extract\" \\\n",
        "        --lmdb_path \"/content/detectron.lmdb\" \\\n",
        "        --features_folder \"/content/features/\" \\"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_JWVeOYvJsM"
      },
      "source": [
        "# Load only one image feature\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "img_list = os.listdir(\"/content/features/\")\n",
        "feat_dir = \"/content/features/\"\n",
        "\n",
        "\n",
        "if len(img_list[0].split('_'))==2:\n",
        "    img_id = img_list[0].split('_')[0]\n",
        "else:\n",
        "    img_id = img_list[0].split('_')[0].split('.')[0]\n",
        "\n",
        "# There are 2 .npy files for each image, \n",
        "#   e.g. : for the image with 'image_id=75349':\n",
        "#       - '75349.npy' : actual feature embedding\n",
        "#       - '75349_info.npy' : meta-data about the image\n",
        "data      = np.load(f\"{feat_dir + img_id}.npy\", allow_pickle=True)\n",
        "data_info = np.load(f\"{feat_dir + img_id}_info.npy\", allow_pickle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJIyYjD_yeIM"
      },
      "source": [
        "# Images are embedded to 2048 dimension!\n",
        "# There are 100 bbox's\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SIf3EuIy1U8"
      },
      "source": [
        "# The meta-data about the image\n",
        "data_info.item().keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y648iYbLqgfj"
      },
      "source": [
        "print(f\"image_id\\t: {data_info.item()['image_id']}\",\n",
        "      f\"\\nnum_boxes\\t: {data_info.item()['num_boxes']}\",\n",
        "      f\"\\nimage_height\\t: {data_info.item()['image_height']}\",\n",
        "      f\"\\nimage_width\\t: {data_info.item()['image_width']}\",\n",
        "      f\"\\nshape(bbox)\\t: {data_info.item()['bbox'].shape}\",\n",
        "      f\"\\nshape(objects)\\t: {data_info.item()['objects'].shape}\",\n",
        "      f\"\\nshape(cls_prob)\\t: {data_info.item()['cls_prob'].shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmIygFhyz1u2"
      },
      "source": [
        "data_info.item()[\"objects\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2wJZAFZ2EEB"
      },
      "source": [
        "for i in [0, 1, 2]:\n",
        "  print(f\"max cls_prob of box #{[i]}: {data_info.item()['cls_prob'][i].max()}\",\n",
        "        f\"\\nindex of that class\\t: {data_info.item()['cls_prob'][i].argmax()}\\n\",\n",
        "        \"-\"*10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36-yi-0G0gTI"
      },
      "source": [
        "data_info.item()[\"bbox\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGrBm27j8Qcr"
      },
      "source": [
        "#### <font color='PaleGoldenrod'> <b> Different techniques for image feature extraction </b> </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V_Q1g7cZDuL"
      },
      "source": [
        "##### <font color='PeachPuff'> <b> Extract image features using `Detectron2` & `ResNet-152` </b> </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2Lb6CYtG5Ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "f18f72f2-056a-41f1-9d07-4dce6765df33"
      },
      "source": [
        "!python extract_region_feature.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0% 0/3 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/detectron2/modeling/poolers.py:231: UserWarning: This overload of nonzero is deprecated:\n",
            "\tnonzero(Tensor input, *, Tensor out)\n",
            "Consider using one of the following signatures instead:\n",
            "\tnonzero(Tensor input, *, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  inds = torch.nonzero(level_assignments == level).squeeze(1)\n",
            "100% 3/3 [00:20<00:00,  6.84s/it]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amSAhPpfNVSZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "7ff74da9-c790-4163-ca89-56f9f02a2147"
      },
      "source": [
        "data = np.load(\"/content/features/hateful_memes/image_1.npy\", allow_pickle=True)\n",
        "data.item(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bbox': array([[207.1685 , 102.26065, 653.63135, 386.67548],\n",
              "        [116.85211,  46.02007, 470.8916 , 412.14105],\n",
              "        [388.9102 , 190.30983, 491.19983, 226.42296]], dtype=float32),\n",
              " 'cls_prob': array([0.9472457 , 0.79215443, 0.5706494 ], dtype=float32),\n",
              " 'features': array([[0.5375976 , 0.41097498, 0.49014562, ..., 0.41597521, 0.27225962,\n",
              "         0.42191768],\n",
              "        [0.48594958, 0.323035  , 0.50595093, ..., 0.33366024, 0.319493  ,\n",
              "         0.40102425],\n",
              "        [0.34906894, 1.17614079, 0.90047979, ..., 0.23805985, 0.04871404,\n",
              "         0.25833166],\n",
              "        ...,\n",
              "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "         0.        ],\n",
              "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "         0.        ],\n",
              "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "         0.        ]]),\n",
              " 'image_height': 500,\n",
              " 'image_id': 'image_1',\n",
              " 'image_width': 735,\n",
              " 'num_boxes': 3,\n",
              " 'objects': array([ 0,  0, 79])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGneRiZ-NiUR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a4fa1fe1-f709-404c-eef5-dc87a1efb8d6"
      },
      "source": [
        "data.item(0)[\"features\"].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 2048)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nid0HIoXru5Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e6b09cb8-31cb-4278-f95b-bf21ae82a00c"
      },
      "source": [
        "data.item().keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['image_id', 'bbox', 'num_boxes', 'image_height', 'image_width', 'objects', 'cls_prob', 'features'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJeiXIYMpsf_"
      },
      "source": [
        "##### <font color='PeachPuff'> <b> Extract image features using [`facebookresearch/grid-feats-vqa`](https://github.com/facebookresearch/grid-feats-vqa) </b> </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAqqZtLEpZr7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "19e2edae-8067-41e3-bd57-f9e7bfa80795"
      },
      "source": [
        "# Install required packages\n",
        "!pip install -U git+https://github.com/facebookresearch/fvcore\n",
        "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git@ffff8ac'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/facebookresearch/fvcore\n",
            "  Cloning https://github.com/facebookresearch/fvcore to /tmp/pip-req-build-3dp0yqz1\n",
            "  Running command git clone -q https://github.com/facebookresearch/fvcore /tmp/pip-req-build-3dp0yqz1\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1.2) (1.18.5)\n",
            "Collecting yacs>=0.1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/38/4f/fe9a4d472aa867878ce3bb7efb16654c5d63672b86dc0e6e953a67018433/yacs-0.1.8-py3-none-any.whl\n",
            "Collecting pyyaml>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 4.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1.2) (4.41.1)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1 in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1.2) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: Pillow in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1.2) (7.0.0)\n",
            "Requirement already satisfied, skipping upgrade: tabulate in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1.2) (0.8.7)\n",
            "Building wheels for collected packages: fvcore, pyyaml\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.2-cp36-none-any.whl size=47852 sha256=0b1b671d78973e197bbce5891bb734d016a9c82d815333949dacb40838a80049\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-30p49r35/wheels/04/a4/85/e50340018c00ae6e07e891fed78895891da33700e90a68aa05\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44619 sha256=73fae1d1d3f1c5de3dd0351b62cd38a973d054364ecceb62a39f6a8457e891db\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "Successfully built fvcore pyyaml\n",
            "Installing collected packages: pyyaml, yacs, portalocker, fvcore\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed fvcore-0.1.2 portalocker-2.0.0 pyyaml-5.3.1 yacs-0.1.8\n",
            "Collecting git+https://github.com/facebookresearch/detectron2.git@ffff8ac\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git (to revision ffff8ac) to /tmp/pip-req-build-7pdcjcgh\n",
            "  Running command git clone -q https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-7pdcjcgh\n",
            "\u001b[33m  WARNING: Did not find branch or tag 'ffff8ac', assuming revision or ref.\u001b[0m\n",
            "  Running command git checkout -q ffff8ac\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1.1) (1.1.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1.1) (7.0.0)\n",
            "Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1.1) (0.1.8)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1.1) (0.8.7)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1.1) (1.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1.1) (3.2.2)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1.1) (4.41.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1.1) (2.3.0)\n",
            "Requirement already satisfied: fvcore in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1.1) (0.1.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1.1) (0.16.0)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1.1) (1.3.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from yacs>=0.1.6->detectron2==0.1.1) (5.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2==0.1.1) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2==0.1.1) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2==0.1.1) (1.18.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2==0.1.1) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2==0.1.1) (2.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1.1) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1.1) (3.2.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1.1) (1.15.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1.1) (2.23.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1.1) (1.32.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1.1) (1.7.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1.1) (50.3.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1.1) (0.35.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1.1) (0.4.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1.1) (3.12.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1.1) (0.10.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1.1) (1.17.2)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from fvcore->detectron2==0.1.1) (2.0.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard->detectron2==0.1.1) (2.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.1.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.1.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.1.1) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.1.1) (2.10)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.1.1) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2==0.1.1) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2==0.1.1) (4.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2==0.1.1) (4.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard->detectron2==0.1.1) (3.2.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.1.1) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->detectron2==0.1.1) (0.4.8)\n",
            "Building wheels for collected packages: detectron2\n",
            "  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for detectron2: filename=detectron2-0.1.1-cp36-cp36m-linux_x86_64.whl size=4470504 sha256=15499fcb784cde0a1023dbc6b398979398c7f17e1e09414f244e4502553a941c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-36uanryh/wheels/be/80/d6/579d4f9812a2d9f182de96969a905af9a2ff2f0f82611f8422\n",
            "Successfully built detectron2\n",
            "Installing collected packages: detectron2\n",
            "Successfully installed detectron2-0.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pvr-E4zQjBmr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "64ad8509-72d3-4193-96d5-1a6408d9ddd6"
      },
      "source": [
        "!git clone https://github.com/vedanuj/grid-feats-vqa.git --branch region_features"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'grid-feats-vqa'...\n",
            "remote: Enumerating objects: 102, done.\u001b[K\n",
            "remote: Counting objects: 100% (102/102), done.\u001b[K\n",
            "remote: Compressing objects: 100% (67/67), done.\u001b[K\n",
            "remote: Total 102 (delta 61), reused 75 (delta 35), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (102/102), 36.21 KiB | 210.00 KiB/s, done.\n",
            "Resolving deltas: 100% (61/61), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nQadBYTlFq1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "590bb320-fa21-4c3c-aab7-21010f7acb0d"
      },
      "source": [
        "cd grid-feats-vqa/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/grid-feats-vqa\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_pUuWxokuNO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f3a31893-8b00-4d52-db00-9717da14db25"
      },
      "source": [
        "!python extract_region_feature.py \\\n",
        "              --config-file configs/X-152-region-c4.yaml \\\n",
        "              --dataset \"hateful_memes\" \\\n",
        "              --dataset-path \"/root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/images/img/\" \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Command Line Args: Namespace(config_file='configs/X-152-region-c4.yaml', dataset='hateful_memes', dataset_path='/root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/images/img/', feature_name='fc7', opts=[])\n",
            "\u001b[32m[09/30 16:01:05 detectron2]: \u001b[0mRank of current process: 0. World size: 1\n",
            "\u001b[32m[09/30 16:01:06 detectron2]: \u001b[0mEnvironment info:\n",
            "------------------------  ---------------------------------------------------------------\n",
            "sys.platform              linux\n",
            "Python                    3.6.9 (default, Jul 17 2020, 12:50:27) [GCC 8.4.0]\n",
            "numpy                     1.18.5\n",
            "detectron2                0.1.1 @/usr/local/lib/python3.6/dist-packages/detectron2\n",
            "detectron2 compiler       GCC 7.5\n",
            "detectron2 CUDA compiler  10.1\n",
            "detectron2 arch flags     sm_75\n",
            "DETECTRON2_ENV_MODULE     <not set>\n",
            "PyTorch                   1.6.0+cu101 @/usr/local/lib/python3.6/dist-packages/torch\n",
            "PyTorch debug build       False\n",
            "CUDA available            True\n",
            "GPU 0                     Tesla T4\n",
            "CUDA_HOME                 /usr/local/cuda\n",
            "NVCC                      Cuda compilation tools, release 10.1, V10.1.243\n",
            "Pillow                    7.0.0\n",
            "torchvision               0.7.0+cu101 @/usr/local/lib/python3.6/dist-packages/torchvision\n",
            "torchvision arch flags    sm_35, sm_50, sm_60, sm_70, sm_75\n",
            "cv2                       4.1.2\n",
            "------------------------  ---------------------------------------------------------------\n",
            "PyTorch built with:\n",
            "  - GCC 7.3\n",
            "  - C++ Version: 201402\n",
            "  - Intel(R) Math Kernel Library Version 2019.0.5 Product Build 20190808 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v1.5.0 (Git Hash e2ac1fac44c5078ca927cb9b90e1b3066a0b2ed0)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 10.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75\n",
            "  - CuDNN 7.6.3\n",
            "  - Magma 2.5.2\n",
            "  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, \n",
            "\n",
            "\u001b[32m[09/30 16:01:06 detectron2]: \u001b[0mCommand line arguments: Namespace(config_file='configs/X-152-region-c4.yaml', dataset='hateful_memes', dataset_path='/root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/images/img/', feature_name='fc7', opts=[])\n",
            "\u001b[32m[09/30 16:01:06 detectron2]: \u001b[0mContents of args.config_file=configs/X-152-region-c4.yaml:\n",
            "_BASE_: \"Base-RCNN-region-c4.yaml\"\n",
            "MODEL:\n",
            "  WEIGHTS: \"catalog://ImageNetPretrained/FAIR/X-152-32x8d-IN5k\"\n",
            "  RESNETS:\n",
            "    STRIDE_IN_1X1: False  # this is a C2 model\n",
            "    NUM_GROUPS: 32\n",
            "    WIDTH_PER_GROUP: 8\n",
            "    DEPTH: 152\n",
            "\u001b[32m[09/30 16:01:06 detectron2]: \u001b[0mRunning with full config:\n",
            "CUDNN_BENCHMARK: False\n",
            "DATALOADER:\n",
            "  ASPECT_RATIO_GROUPING: True\n",
            "  FILTER_EMPTY_ANNOTATIONS: True\n",
            "  NUM_WORKERS: 4\n",
            "  REPEAT_THRESHOLD: 0.0\n",
            "  SAMPLER_TRAIN: TrainingSampler\n",
            "DATASETS:\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000\n",
            "  PROPOSAL_FILES_TEST: ()\n",
            "  PROPOSAL_FILES_TRAIN: ()\n",
            "  TEST: ('visual_genome_test',)\n",
            "  TRAIN: ('visual_genome_train', 'visual_genome_val')\n",
            "GLOBAL:\n",
            "  HACK: 1.0\n",
            "INPUT:\n",
            "  CROP:\n",
            "    ENABLED: False\n",
            "    SIZE: [0.9, 0.9]\n",
            "    TYPE: relative_range\n",
            "  FORMAT: BGR\n",
            "  MASK_FORMAT: polygon\n",
            "  MAX_ATTR_PER_INS: 16\n",
            "  MAX_SIZE_TEST: 1000\n",
            "  MAX_SIZE_TRAIN: 1000\n",
            "  MIN_SIZE_TEST: 600\n",
            "  MIN_SIZE_TRAIN: (600,)\n",
            "  MIN_SIZE_TRAIN_SAMPLING: choice\n",
            "MODEL:\n",
            "  ANCHOR_GENERATOR:\n",
            "    ANGLES: [[-90, 0, 90]]\n",
            "    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]\n",
            "    NAME: DefaultAnchorGenerator\n",
            "    OFFSET: 0.0\n",
            "    SIZES: [[32, 64, 128, 256, 512]]\n",
            "  ATTRIBUTE_ON: True\n",
            "  BACKBONE:\n",
            "    FREEZE_AT: 2\n",
            "    NAME: build_resnet_backbone\n",
            "  DEVICE: cuda\n",
            "  FPN:\n",
            "    FUSE_TYPE: sum\n",
            "    IN_FEATURES: []\n",
            "    NORM: \n",
            "    OUT_CHANNELS: 256\n",
            "  KEYPOINT_ON: False\n",
            "  LOAD_PROPOSALS: False\n",
            "  MASK_ON: False\n",
            "  META_ARCHITECTURE: GeneralizedRCNN\n",
            "  PANOPTIC_FPN:\n",
            "    COMBINE:\n",
            "      ENABLED: True\n",
            "      INSTANCES_CONFIDENCE_THRESH: 0.5\n",
            "      OVERLAP_THRESH: 0.5\n",
            "      STUFF_AREA_LIMIT: 4096\n",
            "    INSTANCE_LOSS_WEIGHT: 1.0\n",
            "  PIXEL_MEAN: [103.53, 116.28, 123.675]\n",
            "  PIXEL_STD: [1.0, 1.0, 1.0]\n",
            "  PROPOSAL_GENERATOR:\n",
            "    MIN_SIZE: 0\n",
            "    NAME: RPN\n",
            "  RESNETS:\n",
            "    DEFORM_MODULATED: False\n",
            "    DEFORM_NUM_GROUPS: 1\n",
            "    DEFORM_ON_PER_STAGE: [False, False, False, False]\n",
            "    DEPTH: 152\n",
            "    NORM: FrozenBN\n",
            "    NUM_GROUPS: 32\n",
            "    OUT_FEATURES: ['res4']\n",
            "    RES2_OUT_CHANNELS: 256\n",
            "    RES5_DILATION: 1\n",
            "    STEM_OUT_CHANNELS: 64\n",
            "    STRIDE_IN_1X1: False\n",
            "    WIDTH_PER_GROUP: 8\n",
            "  RETINANET:\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    FOCAL_LOSS_ALPHA: 0.25\n",
            "    FOCAL_LOSS_GAMMA: 2.0\n",
            "    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.4, 0.5]\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NUM_CLASSES: 80\n",
            "    NUM_CONVS: 4\n",
            "    PRIOR_PROB: 0.01\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "    SMOOTH_L1_LOSS_BETA: 0.1\n",
            "    TOPK_CANDIDATES_TEST: 1000\n",
            "  ROI_ATTRIBUTE_HEAD:\n",
            "    FC_DIM: 512\n",
            "    LOSS_WEIGHT: 0.2\n",
            "    NUM_CLASSES: 400\n",
            "    OBJ_EMBED_DIM: 256\n",
            "  ROI_BOX_CASCADE_HEAD:\n",
            "    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))\n",
            "    IOUS: (0.5, 0.6, 0.7)\n",
            "  ROI_BOX_HEAD:\n",
            "    BBOX_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)\n",
            "    CLS_AGNOSTIC_BBOX_REG: False\n",
            "    CONV_DIM: 256\n",
            "    FC_DIM: 1024\n",
            "    NAME: FastRCNNConvFCHead\n",
            "    NORM: \n",
            "    NUM_CONV: 0\n",
            "    NUM_FC: 2\n",
            "    POOLER_RESOLUTION: 7\n",
            "    POOLER_SAMPLING_RATIO: 2\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "    SMOOTH_L1_BETA: 1.0\n",
            "    TRAIN_ON_PRED_BOXES: False\n",
            "  ROI_HEADS:\n",
            "    BATCH_SIZE_PER_IMAGE: 512\n",
            "    IN_FEATURES: ['res4']\n",
            "    IOU_LABELS: [0, 1]\n",
            "    IOU_THRESHOLDS: [0.5]\n",
            "    NAME: AttributeRes5ROIHeads\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NUM_CLASSES: 1600\n",
            "    POSITIVE_FRACTION: 0.25\n",
            "    PROPOSAL_APPEND_GT: True\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "  ROI_KEYPOINT_HEAD:\n",
            "    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    MIN_KEYPOINTS_PER_IMAGE: 1\n",
            "    NAME: KRCNNConvDeconvUpsampleHead\n",
            "    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True\n",
            "    NUM_KEYPOINTS: 17\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  ROI_MASK_HEAD:\n",
            "    CLS_AGNOSTIC_MASK: False\n",
            "    CONV_DIM: 256\n",
            "    NAME: MaskRCNNConvUpsampleHead\n",
            "    NORM: \n",
            "    NUM_CONV: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  RPN:\n",
            "    BATCH_SIZE_PER_IMAGE: 256\n",
            "    BBOX_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    BOUNDARY_THRESH: 0\n",
            "    HEAD_NAME: StandardRPNHead\n",
            "    IN_FEATURES: ['res4']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.3, 0.7]\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NMS_THRESH: 0.7\n",
            "    POSITIVE_FRACTION: 0.5\n",
            "    POST_NMS_TOPK_TEST: 1000\n",
            "    POST_NMS_TOPK_TRAIN: 2000\n",
            "    PRE_NMS_TOPK_TEST: 6000\n",
            "    PRE_NMS_TOPK_TRAIN: 12000\n",
            "    SMOOTH_L1_BETA: 0.1111\n",
            "  SEM_SEG_HEAD:\n",
            "    COMMON_STRIDE: 4\n",
            "    CONVS_DIM: 128\n",
            "    IGNORE_VALUE: 255\n",
            "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NAME: SemSegFPNHead\n",
            "    NORM: GN\n",
            "    NUM_CLASSES: 54\n",
            "  WEIGHTS: catalog://ImageNetPretrained/FAIR/X-152-32x8d-IN5k\n",
            "OUTPUT_DIR: ./output\n",
            "SEED: -1\n",
            "SOLVER:\n",
            "  BASE_LR: 0.02\n",
            "  BIAS_LR_FACTOR: 1.0\n",
            "  CHECKPOINT_PERIOD: 5000\n",
            "  CLIP_GRADIENTS:\n",
            "    CLIP_TYPE: value\n",
            "    CLIP_VALUE: 1.0\n",
            "    ENABLED: False\n",
            "    NORM_TYPE: 2.0\n",
            "  GAMMA: 0.1\n",
            "  IMS_PER_BATCH: 16\n",
            "  LR_SCHEDULER_NAME: WarmupMultiStepLR\n",
            "  MAX_ITER: 90000\n",
            "  MOMENTUM: 0.9\n",
            "  STEPS: (60000, 80000)\n",
            "  WARMUP_FACTOR: 0.001\n",
            "  WARMUP_ITERS: 1000\n",
            "  WARMUP_METHOD: linear\n",
            "  WEIGHT_DECAY: 0.0001\n",
            "  WEIGHT_DECAY_BIAS: 0.0001\n",
            "  WEIGHT_DECAY_NORM: 0.0\n",
            "TEST:\n",
            "  AUG:\n",
            "    ENABLED: False\n",
            "    FLIP: True\n",
            "    MAX_SIZE: 4000\n",
            "    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)\n",
            "  DETECTIONS_PER_IMAGE: 100\n",
            "  EVAL_PERIOD: 0\n",
            "  EXPECTED_RESULTS: []\n",
            "  KEYPOINT_OKS_SIGMAS: []\n",
            "  PRECISE_BN:\n",
            "    ENABLED: False\n",
            "    NUM_ITER: 200\n",
            "VERSION: 2\n",
            "VIS_PERIOD: 0\n",
            "\u001b[32m[09/30 16:01:06 detectron2]: \u001b[0mFull config saved to ./output/config.yaml\n",
            "\u001b[32m[09/30 16:01:06 d2.utils.env]: \u001b[0mUsing a generated random seed 6491821\n",
            "\u001b[32m[09/30 16:01:13 fvcore.common.checkpoint]: \u001b[0mLoading checkpoint from catalog://ImageNetPretrained/FAIR/X-152-32x8d-IN5k\n",
            "\u001b[32m[09/30 16:01:13 d2.checkpoint.catalog]: \u001b[0mCatalog entry catalog://ImageNetPretrained/FAIR/X-152-32x8d-IN5k points to https://dl.fbaipublicfiles.com/detectron/ImageNetPretrained/25093814/X-152-32x8d-IN5k.pkl\n",
            "\u001b[32m[09/30 16:01:13 fvcore.common.file_io]: \u001b[0mURL https://dl.fbaipublicfiles.com/detectron/ImageNetPretrained/25093814/X-152-32x8d-IN5k.pkl cached in /root/.torch/fvcore_cache/detectron/ImageNetPretrained/25093814/X-152-32x8d-IN5k.pkl\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mRemapping C2 weights ......\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res2.0.conv1.norm.bias                      loaded from res2_0_branch2a_bn_b        of shape (256,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res2.0.conv1.norm.weight                    loaded from res2_0_branch2a_bn_s        of shape (256,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res2.0.conv1.weight                         loaded from res2_0_branch2a_w           of shape (256, 64, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res2.0.conv2.norm.bias                      loaded from res2_0_branch2b_bn_b        of shape (256,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res2.0.conv2.norm.weight                    loaded from res2_0_branch2b_bn_s        of shape (256,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res2.0.conv2.weight                         loaded from res2_0_branch2b_w           of shape (256, 8, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res2.0.conv3.norm.bias                      loaded from res2_0_branch2c_bn_b        of shape (256,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res2.0.conv3.norm.weight                    loaded from res2_0_branch2c_bn_s        of shape (256,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res2.0.conv3.weight                         loaded from res2_0_branch2c_w           of shape (256, 256, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res2.0.shortcut.norm.bias                   loaded from res2_0_branch1_bn_b         of shape (256,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res2.0.shortcut.norm.weight                 loaded from res2_0_branch1_bn_s         of shape (256,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res2.0.shortcut.weight                      loaded from res2_0_branch1_w            of shape (256, 64, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res2.1.conv1.norm.bias                      loaded from res2_1_branch2a_bn_b        of shape (256,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res2.1.conv1.norm.weight                    loaded from res2_1_branch2a_bn_s        of shape (256,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res2.1.conv1.weight                         loaded from res2_1_branch2a_w           of shape (256, 256, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res2.1.conv2.norm.bias                      loaded from res2_1_branch2b_bn_b        of shape (256,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res2.1.conv2.norm.weight                    loaded from res2_1_branch2b_bn_s        of shape (256,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res2.1.conv2.weight                         loaded from res2_1_branch2b_w           of shape (256, 8, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res2.1.conv3.norm.bias                      loaded from res2_1_branch2c_bn_b        of shape (256,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res2.1.conv3.norm.weight                    loaded from res2_1_branch2c_bn_s        of shape (256,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res2.1.conv3.weight                         loaded from res2_1_branch2c_w           of shape (256, 256, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res2.2.conv1.norm.bias                      loaded from res2_2_branch2a_bn_b        of shape (256,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res2.2.conv1.norm.weight                    loaded from res2_2_branch2a_bn_s        of shape (256,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res2.2.conv1.weight                         loaded from res2_2_branch2a_w           of shape (256, 256, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res2.2.conv2.norm.bias                      loaded from res2_2_branch2b_bn_b        of shape (256,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res2.2.conv2.norm.weight                    loaded from res2_2_branch2b_bn_s        of shape (256,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res2.2.conv2.weight                         loaded from res2_2_branch2b_w           of shape (256, 8, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res2.2.conv3.norm.bias                      loaded from res2_2_branch2c_bn_b        of shape (256,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res2.2.conv3.norm.weight                    loaded from res2_2_branch2c_bn_s        of shape (256,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res2.2.conv3.weight                         loaded from res2_2_branch2c_w           of shape (256, 256, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.0.conv1.norm.bias                      loaded from res3_0_branch2a_bn_b        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.0.conv1.norm.weight                    loaded from res3_0_branch2a_bn_s        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.0.conv1.weight                         loaded from res3_0_branch2a_w           of shape (512, 256, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.0.conv2.norm.bias                      loaded from res3_0_branch2b_bn_b        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.0.conv2.norm.weight                    loaded from res3_0_branch2b_bn_s        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.0.conv2.weight                         loaded from res3_0_branch2b_w           of shape (512, 16, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.0.conv3.norm.bias                      loaded from res3_0_branch2c_bn_b        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.0.conv3.norm.weight                    loaded from res3_0_branch2c_bn_s        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.0.conv3.weight                         loaded from res3_0_branch2c_w           of shape (512, 512, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.0.shortcut.norm.bias                   loaded from res3_0_branch1_bn_b         of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.0.shortcut.norm.weight                 loaded from res3_0_branch1_bn_s         of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.0.shortcut.weight                      loaded from res3_0_branch1_w            of shape (512, 256, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.1.conv1.norm.bias                      loaded from res3_1_branch2a_bn_b        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.1.conv1.norm.weight                    loaded from res3_1_branch2a_bn_s        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.1.conv1.weight                         loaded from res3_1_branch2a_w           of shape (512, 512, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.1.conv2.norm.bias                      loaded from res3_1_branch2b_bn_b        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.1.conv2.norm.weight                    loaded from res3_1_branch2b_bn_s        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.1.conv2.weight                         loaded from res3_1_branch2b_w           of shape (512, 16, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.1.conv3.norm.bias                      loaded from res3_1_branch2c_bn_b        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.1.conv3.norm.weight                    loaded from res3_1_branch2c_bn_s        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.1.conv3.weight                         loaded from res3_1_branch2c_w           of shape (512, 512, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.2.conv1.norm.bias                      loaded from res3_2_branch2a_bn_b        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.2.conv1.norm.weight                    loaded from res3_2_branch2a_bn_s        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.2.conv1.weight                         loaded from res3_2_branch2a_w           of shape (512, 512, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.2.conv2.norm.bias                      loaded from res3_2_branch2b_bn_b        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.2.conv2.norm.weight                    loaded from res3_2_branch2b_bn_s        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.2.conv2.weight                         loaded from res3_2_branch2b_w           of shape (512, 16, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.2.conv3.norm.bias                      loaded from res3_2_branch2c_bn_b        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.2.conv3.norm.weight                    loaded from res3_2_branch2c_bn_s        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.2.conv3.weight                         loaded from res3_2_branch2c_w           of shape (512, 512, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.3.conv1.norm.bias                      loaded from res3_3_branch2a_bn_b        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.3.conv1.norm.weight                    loaded from res3_3_branch2a_bn_s        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.3.conv1.weight                         loaded from res3_3_branch2a_w           of shape (512, 512, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.3.conv2.norm.bias                      loaded from res3_3_branch2b_bn_b        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.3.conv2.norm.weight                    loaded from res3_3_branch2b_bn_s        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.3.conv2.weight                         loaded from res3_3_branch2b_w           of shape (512, 16, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.3.conv3.norm.bias                      loaded from res3_3_branch2c_bn_b        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.3.conv3.norm.weight                    loaded from res3_3_branch2c_bn_s        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.3.conv3.weight                         loaded from res3_3_branch2c_w           of shape (512, 512, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.4.conv1.norm.bias                      loaded from res3_4_branch2a_bn_b        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.4.conv1.norm.weight                    loaded from res3_4_branch2a_bn_s        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.4.conv1.weight                         loaded from res3_4_branch2a_w           of shape (512, 512, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.4.conv2.norm.bias                      loaded from res3_4_branch2b_bn_b        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.4.conv2.norm.weight                    loaded from res3_4_branch2b_bn_s        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.4.conv2.weight                         loaded from res3_4_branch2b_w           of shape (512, 16, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.4.conv3.norm.bias                      loaded from res3_4_branch2c_bn_b        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.4.conv3.norm.weight                    loaded from res3_4_branch2c_bn_s        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.4.conv3.weight                         loaded from res3_4_branch2c_w           of shape (512, 512, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.5.conv1.norm.bias                      loaded from res3_5_branch2a_bn_b        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.5.conv1.norm.weight                    loaded from res3_5_branch2a_bn_s        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.5.conv1.weight                         loaded from res3_5_branch2a_w           of shape (512, 512, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.5.conv2.norm.bias                      loaded from res3_5_branch2b_bn_b        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.5.conv2.norm.weight                    loaded from res3_5_branch2b_bn_s        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.5.conv2.weight                         loaded from res3_5_branch2b_w           of shape (512, 16, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.5.conv3.norm.bias                      loaded from res3_5_branch2c_bn_b        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.5.conv3.norm.weight                    loaded from res3_5_branch2c_bn_s        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.5.conv3.weight                         loaded from res3_5_branch2c_w           of shape (512, 512, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.6.conv1.norm.bias                      loaded from res3_6_branch2a_bn_b        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.6.conv1.norm.weight                    loaded from res3_6_branch2a_bn_s        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.6.conv1.weight                         loaded from res3_6_branch2a_w           of shape (512, 512, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.6.conv2.norm.bias                      loaded from res3_6_branch2b_bn_b        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.6.conv2.norm.weight                    loaded from res3_6_branch2b_bn_s        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.6.conv2.weight                         loaded from res3_6_branch2b_w           of shape (512, 16, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.6.conv3.norm.bias                      loaded from res3_6_branch2c_bn_b        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.6.conv3.norm.weight                    loaded from res3_6_branch2c_bn_s        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.6.conv3.weight                         loaded from res3_6_branch2c_w           of shape (512, 512, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.7.conv1.norm.bias                      loaded from res3_7_branch2a_bn_b        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.7.conv1.norm.weight                    loaded from res3_7_branch2a_bn_s        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.7.conv1.weight                         loaded from res3_7_branch2a_w           of shape (512, 512, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.7.conv2.norm.bias                      loaded from res3_7_branch2b_bn_b        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.7.conv2.norm.weight                    loaded from res3_7_branch2b_bn_s        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.7.conv2.weight                         loaded from res3_7_branch2b_w           of shape (512, 16, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.7.conv3.norm.bias                      loaded from res3_7_branch2c_bn_b        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.7.conv3.norm.weight                    loaded from res3_7_branch2c_bn_s        of shape (512,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res3.7.conv3.weight                         loaded from res3_7_branch2c_w           of shape (512, 512, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.0.conv1.norm.bias                      loaded from res4_0_branch2a_bn_b        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.0.conv1.norm.weight                    loaded from res4_0_branch2a_bn_s        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.0.conv1.weight                         loaded from res4_0_branch2a_w           of shape (1024, 512, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.0.conv2.norm.bias                      loaded from res4_0_branch2b_bn_b        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.0.conv2.norm.weight                    loaded from res4_0_branch2b_bn_s        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.0.conv2.weight                         loaded from res4_0_branch2b_w           of shape (1024, 32, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.0.conv3.norm.bias                      loaded from res4_0_branch2c_bn_b        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.0.conv3.norm.weight                    loaded from res4_0_branch2c_bn_s        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.0.conv3.weight                         loaded from res4_0_branch2c_w           of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.0.shortcut.norm.bias                   loaded from res4_0_branch1_bn_b         of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.0.shortcut.norm.weight                 loaded from res4_0_branch1_bn_s         of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.0.shortcut.weight                      loaded from res4_0_branch1_w            of shape (1024, 512, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.1.conv1.norm.bias                      loaded from res4_1_branch2a_bn_b        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.1.conv1.norm.weight                    loaded from res4_1_branch2a_bn_s        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.1.conv1.weight                         loaded from res4_1_branch2a_w           of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.1.conv2.norm.bias                      loaded from res4_1_branch2b_bn_b        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.1.conv2.norm.weight                    loaded from res4_1_branch2b_bn_s        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.1.conv2.weight                         loaded from res4_1_branch2b_w           of shape (1024, 32, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.1.conv3.norm.bias                      loaded from res4_1_branch2c_bn_b        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.1.conv3.norm.weight                    loaded from res4_1_branch2c_bn_s        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.1.conv3.weight                         loaded from res4_1_branch2c_w           of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.10.conv1.norm.bias                     loaded from res4_10_branch2a_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.10.conv1.norm.weight                   loaded from res4_10_branch2a_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.10.conv1.weight                        loaded from res4_10_branch2a_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.10.conv2.norm.bias                     loaded from res4_10_branch2b_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.10.conv2.norm.weight                   loaded from res4_10_branch2b_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.10.conv2.weight                        loaded from res4_10_branch2b_w          of shape (1024, 32, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.10.conv3.norm.bias                     loaded from res4_10_branch2c_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.10.conv3.norm.weight                   loaded from res4_10_branch2c_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.10.conv3.weight                        loaded from res4_10_branch2c_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.11.conv1.norm.bias                     loaded from res4_11_branch2a_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.11.conv1.norm.weight                   loaded from res4_11_branch2a_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.11.conv1.weight                        loaded from res4_11_branch2a_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.11.conv2.norm.bias                     loaded from res4_11_branch2b_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.11.conv2.norm.weight                   loaded from res4_11_branch2b_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.11.conv2.weight                        loaded from res4_11_branch2b_w          of shape (1024, 32, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.11.conv3.norm.bias                     loaded from res4_11_branch2c_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.11.conv3.norm.weight                   loaded from res4_11_branch2c_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.11.conv3.weight                        loaded from res4_11_branch2c_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.12.conv1.norm.bias                     loaded from res4_12_branch2a_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.12.conv1.norm.weight                   loaded from res4_12_branch2a_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.12.conv1.weight                        loaded from res4_12_branch2a_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.12.conv2.norm.bias                     loaded from res4_12_branch2b_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.12.conv2.norm.weight                   loaded from res4_12_branch2b_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.12.conv2.weight                        loaded from res4_12_branch2b_w          of shape (1024, 32, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.12.conv3.norm.bias                     loaded from res4_12_branch2c_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.12.conv3.norm.weight                   loaded from res4_12_branch2c_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.12.conv3.weight                        loaded from res4_12_branch2c_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.13.conv1.norm.bias                     loaded from res4_13_branch2a_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.13.conv1.norm.weight                   loaded from res4_13_branch2a_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.13.conv1.weight                        loaded from res4_13_branch2a_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.13.conv2.norm.bias                     loaded from res4_13_branch2b_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.13.conv2.norm.weight                   loaded from res4_13_branch2b_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.13.conv2.weight                        loaded from res4_13_branch2b_w          of shape (1024, 32, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.13.conv3.norm.bias                     loaded from res4_13_branch2c_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.13.conv3.norm.weight                   loaded from res4_13_branch2c_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.13.conv3.weight                        loaded from res4_13_branch2c_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.14.conv1.norm.bias                     loaded from res4_14_branch2a_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.14.conv1.norm.weight                   loaded from res4_14_branch2a_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.14.conv1.weight                        loaded from res4_14_branch2a_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.14.conv2.norm.bias                     loaded from res4_14_branch2b_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.14.conv2.norm.weight                   loaded from res4_14_branch2b_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.14.conv2.weight                        loaded from res4_14_branch2b_w          of shape (1024, 32, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.14.conv3.norm.bias                     loaded from res4_14_branch2c_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.14.conv3.norm.weight                   loaded from res4_14_branch2c_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.14.conv3.weight                        loaded from res4_14_branch2c_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.15.conv1.norm.bias                     loaded from res4_15_branch2a_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.15.conv1.norm.weight                   loaded from res4_15_branch2a_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.15.conv1.weight                        loaded from res4_15_branch2a_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.15.conv2.norm.bias                     loaded from res4_15_branch2b_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.15.conv2.norm.weight                   loaded from res4_15_branch2b_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.15.conv2.weight                        loaded from res4_15_branch2b_w          of shape (1024, 32, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.15.conv3.norm.bias                     loaded from res4_15_branch2c_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.15.conv3.norm.weight                   loaded from res4_15_branch2c_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.15.conv3.weight                        loaded from res4_15_branch2c_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.16.conv1.norm.bias                     loaded from res4_16_branch2a_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.16.conv1.norm.weight                   loaded from res4_16_branch2a_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.16.conv1.weight                        loaded from res4_16_branch2a_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.16.conv2.norm.bias                     loaded from res4_16_branch2b_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.16.conv2.norm.weight                   loaded from res4_16_branch2b_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.16.conv2.weight                        loaded from res4_16_branch2b_w          of shape (1024, 32, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.16.conv3.norm.bias                     loaded from res4_16_branch2c_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.16.conv3.norm.weight                   loaded from res4_16_branch2c_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.16.conv3.weight                        loaded from res4_16_branch2c_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.17.conv1.norm.bias                     loaded from res4_17_branch2a_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.17.conv1.norm.weight                   loaded from res4_17_branch2a_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.17.conv1.weight                        loaded from res4_17_branch2a_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.17.conv2.norm.bias                     loaded from res4_17_branch2b_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.17.conv2.norm.weight                   loaded from res4_17_branch2b_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.17.conv2.weight                        loaded from res4_17_branch2b_w          of shape (1024, 32, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.17.conv3.norm.bias                     loaded from res4_17_branch2c_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.17.conv3.norm.weight                   loaded from res4_17_branch2c_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.17.conv3.weight                        loaded from res4_17_branch2c_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.18.conv1.norm.bias                     loaded from res4_18_branch2a_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.18.conv1.norm.weight                   loaded from res4_18_branch2a_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.18.conv1.weight                        loaded from res4_18_branch2a_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.18.conv2.norm.bias                     loaded from res4_18_branch2b_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.18.conv2.norm.weight                   loaded from res4_18_branch2b_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.18.conv2.weight                        loaded from res4_18_branch2b_w          of shape (1024, 32, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.18.conv3.norm.bias                     loaded from res4_18_branch2c_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.18.conv3.norm.weight                   loaded from res4_18_branch2c_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.18.conv3.weight                        loaded from res4_18_branch2c_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.19.conv1.norm.bias                     loaded from res4_19_branch2a_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.19.conv1.norm.weight                   loaded from res4_19_branch2a_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.19.conv1.weight                        loaded from res4_19_branch2a_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.19.conv2.norm.bias                     loaded from res4_19_branch2b_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.19.conv2.norm.weight                   loaded from res4_19_branch2b_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.19.conv2.weight                        loaded from res4_19_branch2b_w          of shape (1024, 32, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.19.conv3.norm.bias                     loaded from res4_19_branch2c_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.19.conv3.norm.weight                   loaded from res4_19_branch2c_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.19.conv3.weight                        loaded from res4_19_branch2c_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.2.conv1.norm.bias                      loaded from res4_2_branch2a_bn_b        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.2.conv1.norm.weight                    loaded from res4_2_branch2a_bn_s        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.2.conv1.weight                         loaded from res4_2_branch2a_w           of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.2.conv2.norm.bias                      loaded from res4_2_branch2b_bn_b        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.2.conv2.norm.weight                    loaded from res4_2_branch2b_bn_s        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.2.conv2.weight                         loaded from res4_2_branch2b_w           of shape (1024, 32, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.2.conv3.norm.bias                      loaded from res4_2_branch2c_bn_b        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.2.conv3.norm.weight                    loaded from res4_2_branch2c_bn_s        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.2.conv3.weight                         loaded from res4_2_branch2c_w           of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.20.conv1.norm.bias                     loaded from res4_20_branch2a_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.20.conv1.norm.weight                   loaded from res4_20_branch2a_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.20.conv1.weight                        loaded from res4_20_branch2a_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.20.conv2.norm.bias                     loaded from res4_20_branch2b_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.20.conv2.norm.weight                   loaded from res4_20_branch2b_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.20.conv2.weight                        loaded from res4_20_branch2b_w          of shape (1024, 32, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.20.conv3.norm.bias                     loaded from res4_20_branch2c_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.20.conv3.norm.weight                   loaded from res4_20_branch2c_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.20.conv3.weight                        loaded from res4_20_branch2c_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.21.conv1.norm.bias                     loaded from res4_21_branch2a_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.21.conv1.norm.weight                   loaded from res4_21_branch2a_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.21.conv1.weight                        loaded from res4_21_branch2a_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.21.conv2.norm.bias                     loaded from res4_21_branch2b_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.21.conv2.norm.weight                   loaded from res4_21_branch2b_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.21.conv2.weight                        loaded from res4_21_branch2b_w          of shape (1024, 32, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.21.conv3.norm.bias                     loaded from res4_21_branch2c_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.21.conv3.norm.weight                   loaded from res4_21_branch2c_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.21.conv3.weight                        loaded from res4_21_branch2c_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.22.conv1.norm.bias                     loaded from res4_22_branch2a_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.22.conv1.norm.weight                   loaded from res4_22_branch2a_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.22.conv1.weight                        loaded from res4_22_branch2a_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.22.conv2.norm.bias                     loaded from res4_22_branch2b_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.22.conv2.norm.weight                   loaded from res4_22_branch2b_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.22.conv2.weight                        loaded from res4_22_branch2b_w          of shape (1024, 32, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.22.conv3.norm.bias                     loaded from res4_22_branch2c_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.22.conv3.norm.weight                   loaded from res4_22_branch2c_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.22.conv3.weight                        loaded from res4_22_branch2c_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.23.conv1.norm.bias                     loaded from res4_23_branch2a_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.23.conv1.norm.weight                   loaded from res4_23_branch2a_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.23.conv1.weight                        loaded from res4_23_branch2a_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.23.conv2.norm.bias                     loaded from res4_23_branch2b_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.23.conv2.norm.weight                   loaded from res4_23_branch2b_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.23.conv2.weight                        loaded from res4_23_branch2b_w          of shape (1024, 32, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.23.conv3.norm.bias                     loaded from res4_23_branch2c_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.23.conv3.norm.weight                   loaded from res4_23_branch2c_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.23.conv3.weight                        loaded from res4_23_branch2c_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.24.conv1.norm.bias                     loaded from res4_24_branch2a_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.24.conv1.norm.weight                   loaded from res4_24_branch2a_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.24.conv1.weight                        loaded from res4_24_branch2a_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.24.conv2.norm.bias                     loaded from res4_24_branch2b_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.24.conv2.norm.weight                   loaded from res4_24_branch2b_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.24.conv2.weight                        loaded from res4_24_branch2b_w          of shape (1024, 32, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.24.conv3.norm.bias                     loaded from res4_24_branch2c_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.24.conv3.norm.weight                   loaded from res4_24_branch2c_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.24.conv3.weight                        loaded from res4_24_branch2c_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.25.conv1.norm.bias                     loaded from res4_25_branch2a_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.25.conv1.norm.weight                   loaded from res4_25_branch2a_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.25.conv1.weight                        loaded from res4_25_branch2a_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.25.conv2.norm.bias                     loaded from res4_25_branch2b_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.25.conv2.norm.weight                   loaded from res4_25_branch2b_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.25.conv2.weight                        loaded from res4_25_branch2b_w          of shape (1024, 32, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.25.conv3.norm.bias                     loaded from res4_25_branch2c_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.25.conv3.norm.weight                   loaded from res4_25_branch2c_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.25.conv3.weight                        loaded from res4_25_branch2c_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.26.conv1.norm.bias                     loaded from res4_26_branch2a_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.26.conv1.norm.weight                   loaded from res4_26_branch2a_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.26.conv1.weight                        loaded from res4_26_branch2a_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.26.conv2.norm.bias                     loaded from res4_26_branch2b_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.26.conv2.norm.weight                   loaded from res4_26_branch2b_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.26.conv2.weight                        loaded from res4_26_branch2b_w          of shape (1024, 32, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.26.conv3.norm.bias                     loaded from res4_26_branch2c_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.26.conv3.norm.weight                   loaded from res4_26_branch2c_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.26.conv3.weight                        loaded from res4_26_branch2c_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.27.conv1.norm.bias                     loaded from res4_27_branch2a_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.27.conv1.norm.weight                   loaded from res4_27_branch2a_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.27.conv1.weight                        loaded from res4_27_branch2a_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.27.conv2.norm.bias                     loaded from res4_27_branch2b_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.27.conv2.norm.weight                   loaded from res4_27_branch2b_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.27.conv2.weight                        loaded from res4_27_branch2b_w          of shape (1024, 32, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.27.conv3.norm.bias                     loaded from res4_27_branch2c_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.27.conv3.norm.weight                   loaded from res4_27_branch2c_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.27.conv3.weight                        loaded from res4_27_branch2c_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.28.conv1.norm.bias                     loaded from res4_28_branch2a_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.28.conv1.norm.weight                   loaded from res4_28_branch2a_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.28.conv1.weight                        loaded from res4_28_branch2a_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.28.conv2.norm.bias                     loaded from res4_28_branch2b_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.28.conv2.norm.weight                   loaded from res4_28_branch2b_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.28.conv2.weight                        loaded from res4_28_branch2b_w          of shape (1024, 32, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.28.conv3.norm.bias                     loaded from res4_28_branch2c_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.28.conv3.norm.weight                   loaded from res4_28_branch2c_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.28.conv3.weight                        loaded from res4_28_branch2c_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.29.conv1.norm.bias                     loaded from res4_29_branch2a_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.29.conv1.norm.weight                   loaded from res4_29_branch2a_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.29.conv1.weight                        loaded from res4_29_branch2a_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.29.conv2.norm.bias                     loaded from res4_29_branch2b_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.29.conv2.norm.weight                   loaded from res4_29_branch2b_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.29.conv2.weight                        loaded from res4_29_branch2b_w          of shape (1024, 32, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.29.conv3.norm.bias                     loaded from res4_29_branch2c_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.29.conv3.norm.weight                   loaded from res4_29_branch2c_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.29.conv3.weight                        loaded from res4_29_branch2c_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.3.conv1.norm.bias                      loaded from res4_3_branch2a_bn_b        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.3.conv1.norm.weight                    loaded from res4_3_branch2a_bn_s        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.3.conv1.weight                         loaded from res4_3_branch2a_w           of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.3.conv2.norm.bias                      loaded from res4_3_branch2b_bn_b        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.3.conv2.norm.weight                    loaded from res4_3_branch2b_bn_s        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.3.conv2.weight                         loaded from res4_3_branch2b_w           of shape (1024, 32, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.3.conv3.norm.bias                      loaded from res4_3_branch2c_bn_b        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.3.conv3.norm.weight                    loaded from res4_3_branch2c_bn_s        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.3.conv3.weight                         loaded from res4_3_branch2c_w           of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.30.conv1.norm.bias                     loaded from res4_30_branch2a_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.30.conv1.norm.weight                   loaded from res4_30_branch2a_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.30.conv1.weight                        loaded from res4_30_branch2a_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.30.conv2.norm.bias                     loaded from res4_30_branch2b_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.30.conv2.norm.weight                   loaded from res4_30_branch2b_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.30.conv2.weight                        loaded from res4_30_branch2b_w          of shape (1024, 32, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.30.conv3.norm.bias                     loaded from res4_30_branch2c_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.30.conv3.norm.weight                   loaded from res4_30_branch2c_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.30.conv3.weight                        loaded from res4_30_branch2c_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.31.conv1.norm.bias                     loaded from res4_31_branch2a_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.31.conv1.norm.weight                   loaded from res4_31_branch2a_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.31.conv1.weight                        loaded from res4_31_branch2a_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.31.conv2.norm.bias                     loaded from res4_31_branch2b_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.31.conv2.norm.weight                   loaded from res4_31_branch2b_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.31.conv2.weight                        loaded from res4_31_branch2b_w          of shape (1024, 32, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.31.conv3.norm.bias                     loaded from res4_31_branch2c_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.31.conv3.norm.weight                   loaded from res4_31_branch2c_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.31.conv3.weight                        loaded from res4_31_branch2c_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.32.conv1.norm.bias                     loaded from res4_32_branch2a_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.32.conv1.norm.weight                   loaded from res4_32_branch2a_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.32.conv1.weight                        loaded from res4_32_branch2a_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.32.conv2.norm.bias                     loaded from res4_32_branch2b_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.32.conv2.norm.weight                   loaded from res4_32_branch2b_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.32.conv2.weight                        loaded from res4_32_branch2b_w          of shape (1024, 32, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.32.conv3.norm.bias                     loaded from res4_32_branch2c_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.32.conv3.norm.weight                   loaded from res4_32_branch2c_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.32.conv3.weight                        loaded from res4_32_branch2c_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.33.conv1.norm.bias                     loaded from res4_33_branch2a_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.33.conv1.norm.weight                   loaded from res4_33_branch2a_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.33.conv1.weight                        loaded from res4_33_branch2a_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.33.conv2.norm.bias                     loaded from res4_33_branch2b_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.33.conv2.norm.weight                   loaded from res4_33_branch2b_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.33.conv2.weight                        loaded from res4_33_branch2b_w          of shape (1024, 32, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.33.conv3.norm.bias                     loaded from res4_33_branch2c_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.33.conv3.norm.weight                   loaded from res4_33_branch2c_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.33.conv3.weight                        loaded from res4_33_branch2c_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.34.conv1.norm.bias                     loaded from res4_34_branch2a_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.34.conv1.norm.weight                   loaded from res4_34_branch2a_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.34.conv1.weight                        loaded from res4_34_branch2a_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.34.conv2.norm.bias                     loaded from res4_34_branch2b_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.34.conv2.norm.weight                   loaded from res4_34_branch2b_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.34.conv2.weight                        loaded from res4_34_branch2b_w          of shape (1024, 32, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.34.conv3.norm.bias                     loaded from res4_34_branch2c_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.34.conv3.norm.weight                   loaded from res4_34_branch2c_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.34.conv3.weight                        loaded from res4_34_branch2c_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.35.conv1.norm.bias                     loaded from res4_35_branch2a_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.35.conv1.norm.weight                   loaded from res4_35_branch2a_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.35.conv1.weight                        loaded from res4_35_branch2a_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.35.conv2.norm.bias                     loaded from res4_35_branch2b_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.35.conv2.norm.weight                   loaded from res4_35_branch2b_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.35.conv2.weight                        loaded from res4_35_branch2b_w          of shape (1024, 32, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.35.conv3.norm.bias                     loaded from res4_35_branch2c_bn_b       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.35.conv3.norm.weight                   loaded from res4_35_branch2c_bn_s       of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.35.conv3.weight                        loaded from res4_35_branch2c_w          of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.4.conv1.norm.bias                      loaded from res4_4_branch2a_bn_b        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.4.conv1.norm.weight                    loaded from res4_4_branch2a_bn_s        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.4.conv1.weight                         loaded from res4_4_branch2a_w           of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.4.conv2.norm.bias                      loaded from res4_4_branch2b_bn_b        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.4.conv2.norm.weight                    loaded from res4_4_branch2b_bn_s        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.4.conv2.weight                         loaded from res4_4_branch2b_w           of shape (1024, 32, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.4.conv3.norm.bias                      loaded from res4_4_branch2c_bn_b        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.4.conv3.norm.weight                    loaded from res4_4_branch2c_bn_s        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.4.conv3.weight                         loaded from res4_4_branch2c_w           of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.5.conv1.norm.bias                      loaded from res4_5_branch2a_bn_b        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.5.conv1.norm.weight                    loaded from res4_5_branch2a_bn_s        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.5.conv1.weight                         loaded from res4_5_branch2a_w           of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.5.conv2.norm.bias                      loaded from res4_5_branch2b_bn_b        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.5.conv2.norm.weight                    loaded from res4_5_branch2b_bn_s        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.5.conv2.weight                         loaded from res4_5_branch2b_w           of shape (1024, 32, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.5.conv3.norm.bias                      loaded from res4_5_branch2c_bn_b        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.5.conv3.norm.weight                    loaded from res4_5_branch2c_bn_s        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.5.conv3.weight                         loaded from res4_5_branch2c_w           of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.6.conv1.norm.bias                      loaded from res4_6_branch2a_bn_b        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.6.conv1.norm.weight                    loaded from res4_6_branch2a_bn_s        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.6.conv1.weight                         loaded from res4_6_branch2a_w           of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.6.conv2.norm.bias                      loaded from res4_6_branch2b_bn_b        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.6.conv2.norm.weight                    loaded from res4_6_branch2b_bn_s        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.6.conv2.weight                         loaded from res4_6_branch2b_w           of shape (1024, 32, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.6.conv3.norm.bias                      loaded from res4_6_branch2c_bn_b        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.6.conv3.norm.weight                    loaded from res4_6_branch2c_bn_s        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.6.conv3.weight                         loaded from res4_6_branch2c_w           of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.7.conv1.norm.bias                      loaded from res4_7_branch2a_bn_b        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.7.conv1.norm.weight                    loaded from res4_7_branch2a_bn_s        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.7.conv1.weight                         loaded from res4_7_branch2a_w           of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.7.conv2.norm.bias                      loaded from res4_7_branch2b_bn_b        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.7.conv2.norm.weight                    loaded from res4_7_branch2b_bn_s        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.7.conv2.weight                         loaded from res4_7_branch2b_w           of shape (1024, 32, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.7.conv3.norm.bias                      loaded from res4_7_branch2c_bn_b        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.7.conv3.norm.weight                    loaded from res4_7_branch2c_bn_s        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.7.conv3.weight                         loaded from res4_7_branch2c_w           of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.8.conv1.norm.bias                      loaded from res4_8_branch2a_bn_b        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.8.conv1.norm.weight                    loaded from res4_8_branch2a_bn_s        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.8.conv1.weight                         loaded from res4_8_branch2a_w           of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.8.conv2.norm.bias                      loaded from res4_8_branch2b_bn_b        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.8.conv2.norm.weight                    loaded from res4_8_branch2b_bn_s        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.8.conv2.weight                         loaded from res4_8_branch2b_w           of shape (1024, 32, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.8.conv3.norm.bias                      loaded from res4_8_branch2c_bn_b        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.8.conv3.norm.weight                    loaded from res4_8_branch2c_bn_s        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.8.conv3.weight                         loaded from res4_8_branch2c_w           of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.9.conv1.norm.bias                      loaded from res4_9_branch2a_bn_b        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.9.conv1.norm.weight                    loaded from res4_9_branch2a_bn_s        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.9.conv1.weight                         loaded from res4_9_branch2a_w           of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.9.conv2.norm.bias                      loaded from res4_9_branch2b_bn_b        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.9.conv2.norm.weight                    loaded from res4_9_branch2b_bn_s        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.9.conv2.weight                         loaded from res4_9_branch2b_w           of shape (1024, 32, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.9.conv3.norm.bias                      loaded from res4_9_branch2c_bn_b        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.9.conv3.norm.weight                    loaded from res4_9_branch2c_bn_s        of shape (1024,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.res4.9.conv3.weight                         loaded from res4_9_branch2c_w           of shape (1024, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.stem.conv1.norm.bias                        loaded from res_conv1_bn_b              of shape (64,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.stem.conv1.norm.weight                      loaded from res_conv1_bn_s              of shape (64,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mbackbone.stem.conv1.weight                           loaded from conv1_w                     of shape (64, 3, 7, 7)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mroi_heads.res5.0.conv1.norm.bias                     loaded from res5_0_branch2a_bn_b        of shape (2048,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mroi_heads.res5.0.conv1.norm.weight                   loaded from res5_0_branch2a_bn_s        of shape (2048,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mroi_heads.res5.0.conv1.weight                        loaded from res5_0_branch2a_w           of shape (2048, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mroi_heads.res5.0.conv2.norm.bias                     loaded from res5_0_branch2b_bn_b        of shape (2048,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mroi_heads.res5.0.conv2.norm.weight                   loaded from res5_0_branch2b_bn_s        of shape (2048,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mroi_heads.res5.0.conv2.weight                        loaded from res5_0_branch2b_w           of shape (2048, 64, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mroi_heads.res5.0.conv3.norm.bias                     loaded from res5_0_branch2c_bn_b        of shape (2048,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mroi_heads.res5.0.conv3.norm.weight                   loaded from res5_0_branch2c_bn_s        of shape (2048,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mroi_heads.res5.0.conv3.weight                        loaded from res5_0_branch2c_w           of shape (2048, 2048, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mroi_heads.res5.0.shortcut.norm.bias                  loaded from res5_0_branch1_bn_b         of shape (2048,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mroi_heads.res5.0.shortcut.norm.weight                loaded from res5_0_branch1_bn_s         of shape (2048,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mroi_heads.res5.0.shortcut.weight                     loaded from res5_0_branch1_w            of shape (2048, 1024, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mroi_heads.res5.1.conv1.norm.bias                     loaded from res5_1_branch2a_bn_b        of shape (2048,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mroi_heads.res5.1.conv1.norm.weight                   loaded from res5_1_branch2a_bn_s        of shape (2048,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mroi_heads.res5.1.conv1.weight                        loaded from res5_1_branch2a_w           of shape (2048, 2048, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mroi_heads.res5.1.conv2.norm.bias                     loaded from res5_1_branch2b_bn_b        of shape (2048,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mroi_heads.res5.1.conv2.norm.weight                   loaded from res5_1_branch2b_bn_s        of shape (2048,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mroi_heads.res5.1.conv2.weight                        loaded from res5_1_branch2b_w           of shape (2048, 64, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mroi_heads.res5.1.conv3.norm.bias                     loaded from res5_1_branch2c_bn_b        of shape (2048,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mroi_heads.res5.1.conv3.norm.weight                   loaded from res5_1_branch2c_bn_s        of shape (2048,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mroi_heads.res5.1.conv3.weight                        loaded from res5_1_branch2c_w           of shape (2048, 2048, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mroi_heads.res5.2.conv1.norm.bias                     loaded from res5_2_branch2a_bn_b        of shape (2048,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mroi_heads.res5.2.conv1.norm.weight                   loaded from res5_2_branch2a_bn_s        of shape (2048,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mroi_heads.res5.2.conv1.weight                        loaded from res5_2_branch2a_w           of shape (2048, 2048, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mroi_heads.res5.2.conv2.norm.bias                     loaded from res5_2_branch2b_bn_b        of shape (2048,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mroi_heads.res5.2.conv2.norm.weight                   loaded from res5_2_branch2b_bn_s        of shape (2048,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mroi_heads.res5.2.conv2.weight                        loaded from res5_2_branch2b_w           of shape (2048, 64, 3, 3)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mroi_heads.res5.2.conv3.norm.bias                     loaded from res5_2_branch2c_bn_b        of shape (2048,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mroi_heads.res5.2.conv3.norm.weight                   loaded from res5_2_branch2c_bn_s        of shape (2048,)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mroi_heads.res5.2.conv3.weight                        loaded from res5_2_branch2c_w           of shape (2048, 2048, 1, 1)\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mSome model parameters or buffers are not found in the checkpoint:\n",
            "  \u001b[34mbackbone.res2.0.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res2.0.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res2.0.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res2.0.shortcut.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res2.1.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res2.1.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res2.1.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res2.2.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res2.2.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res2.2.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res3.0.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res3.0.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res3.0.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res3.0.shortcut.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res3.1.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res3.1.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res3.1.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res3.2.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res3.2.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res3.2.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res3.3.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res3.3.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res3.3.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res3.4.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res3.4.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res3.4.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res3.5.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res3.5.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res3.5.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res3.6.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res3.6.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res3.6.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res3.7.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res3.7.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res3.7.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.0.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.0.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.0.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.0.shortcut.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.1.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.1.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.1.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.10.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.10.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.10.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.11.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.11.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.11.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.12.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.12.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.12.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.13.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.13.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.13.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.14.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.14.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.14.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.15.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.15.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.15.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.16.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.16.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.16.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.17.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.17.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.17.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.18.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.18.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.18.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.19.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.19.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.19.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.2.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.2.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.2.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.20.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.20.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.20.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.21.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.21.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.21.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.22.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.22.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.22.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.23.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.23.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.23.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.24.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.24.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.24.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.25.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.25.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.25.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.26.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.26.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.26.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.27.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.27.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.27.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.28.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.28.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.28.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.29.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.29.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.29.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.3.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.3.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.3.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.30.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.30.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.30.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.31.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.31.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.31.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.32.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.32.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.32.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.33.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.33.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.33.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.34.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.34.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.34.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.35.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.35.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.35.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.4.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.4.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.4.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.5.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.5.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.5.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.6.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.6.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.6.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.7.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.7.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.7.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.8.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.8.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.8.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.9.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.9.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.res4.9.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mbackbone.stem.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mproposal_generator.anchor_generator.cell_anchors.0\u001b[0m\n",
            "  \u001b[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}\u001b[0m\n",
            "  \u001b[34mproposal_generator.rpn_head.conv.{bias, weight}\u001b[0m\n",
            "  \u001b[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}\u001b[0m\n",
            "  \u001b[34mroi_heads.attribute_predictor.attr_score.{bias, weight}\u001b[0m\n",
            "  \u001b[34mroi_heads.attribute_predictor.fc.0.{bias, weight}\u001b[0m\n",
            "  \u001b[34mroi_heads.attribute_predictor.obj_embed.weight\u001b[0m\n",
            "  \u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
            "  \u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n",
            "  \u001b[34mroi_heads.res5.0.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mroi_heads.res5.0.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mroi_heads.res5.0.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mroi_heads.res5.0.shortcut.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mroi_heads.res5.1.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mroi_heads.res5.1.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mroi_heads.res5.1.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mroi_heads.res5.2.conv1.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mroi_heads.res5.2.conv2.norm.{running_mean, running_var}\u001b[0m\n",
            "  \u001b[34mroi_heads.res5.2.conv3.norm.{running_mean, running_var}\u001b[0m\n",
            "\u001b[32m[09/30 16:01:14 d2.checkpoint.c2_model_loading]: \u001b[0mThe checkpoint state_dict contains keys that are not used by the model:\n",
            "  \u001b[35mpred_b\u001b[0m\n",
            "  \u001b[35mpred_w\u001b[0m\n",
            "\u001b[32m[09/30 16:01:15 d2.data.common]: \u001b[0mSerializing 10000 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[09/30 16:01:15 d2.data.common]: \u001b[0mSerialized dataset takes 1.24 MiB\n",
            "  0% 0/10000 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/detectron2/modeling/roi_heads/fast_rcnn.py:107: UserWarning: This overload of nonzero is deprecated:\n",
            "\tnonzero()\n",
            "Consider using one of the following signatures instead:\n",
            "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  filter_inds = filter_mask.nonzero()\n",
            "100% 10000/10000 [1:31:36<00:00,  1.82it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVRUiTnJo8hc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "696b59a9-d318-4921-d7cc-535dd4517cd5"
      },
      "source": [
        "!ls /content/grid-feats-vqa/output/features/hateful_memes/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "01235.npy  14728.npy  28764.npy  42650.npy  57183.npy  71390.npy  85209.npy\n",
            "01236.npy  14765.npy  28765.npy  42653.npy  57189.npy  71392.npy  85213.npy\n",
            "01243.npy  14769.npy  28790.npy  42658.npy  57193.npy  71396.npy  85237.npy\n",
            "01245.npy  14782.npy  28793.npy  42673.npy  57198.npy  71398.npy  85239.npy\n",
            "01247.npy  14783.npy  28905.npy  42675.npy  57203.npy  71403.npy  85243.npy\n",
            "01256.npy  14789.npy  28930.npy  42681.npy  57208.npy  71428.npy  85261.npy\n",
            "01258.npy  14793.npy  28935.npy  42685.npy  57209.npy  71429.npy  85269.npy\n",
            "01264.npy  14802.npy  28936.npy  42687.npy  57236.npy  71430.npy  85271.npy\n",
            "01268.npy  14823.npy  28945.npy  42690.npy  57248.npy  71432.npy  85290.npy\n",
            "01269.npy  14829.npy  28951.npy  42691.npy  57249.npy  71436.npy  85291.npy\n",
            "01274.npy  14830.npy  28954.npy  42693.npy  57260.npy  71450.npy  85307.npy\n",
            "01275.npy  14836.npy  28957.npy  42705.npy  57261.npy  71452.npy  85310.npy\n",
            "01276.npy  14837.npy  28964.npy  42706.npy  57268.npy  71453.npy  85314.npy\n",
            "01284.npy  14852.npy  28970.npy  42715.npy  57280.npy  71459.npy  85316.npy\n",
            "01293.npy  14856.npy  28971.npy  42736.npy  57281.npy  71462.npy  85317.npy\n",
            "01295.npy  14859.npy  28973.npy  42739.npy  57284.npy  71463.npy  85324.npy\n",
            "01324.npy  14865.npy  28974.npy  42751.npy  57286.npy  71480.npy  85327.npy\n",
            "01325.npy  14869.npy  28976.npy  42759.npy  57298.npy  71482.npy  85329.npy\n",
            "01327.npy  14870.npy  29013.npy  42763.npy  57302.npy  71483.npy  85346.npy\n",
            "01329.npy  14873.npy  29036.npy  42783.npy  57312.npy  71485.npy  85362.npy\n",
            "01348.npy  14879.npy  29041.npy  42786.npy  57319.npy  71486.npy  85369.npy\n",
            "01349.npy  14892.npy  29046.npy  42801.npy  57346.npy  71492.npy  85371.npy\n",
            "01359.npy  14893.npy  29051.npy  42806.npy  57348.npy  71502.npy  85379.npy\n",
            "01364.npy  14897.npy  29054.npy  42810.npy  57349.npy  71503.npy  85392.npy\n",
            "01379.npy  14906.npy  29056.npy  42813.npy  57369.npy  71506.npy  85394.npy\n",
            "01382.npy  14908.npy  29063.npy  42816.npy  57386.npy  71509.npy  85409.npy\n",
            "01389.npy  14920.npy  29075.npy  42830.npy  57389.npy  71524.npy  85410.npy\n",
            "01392.npy  14950.npy  29083.npy  42836.npy  57394.npy  71528.npy  85413.npy\n",
            "01395.npy  14953.npy  29084.npy  42850.npy  57401.npy  71529.npy  85417.npy\n",
            "01423.npy  14958.npy  29104.npy  42851.npy  57412.npy  71530.npy  85420.npy\n",
            "01436.npy  14965.npy  29130.npy  42853.npy  57418.npy  71532.npy  85421.npy\n",
            "01439.npy  14968.npy  29137.npy  42856.npy  57421.npy  71539.npy  85426.npy\n",
            "01452.npy  14975.npy  29138.npy  42860.npy  57423.npy  71562.npy  85429.npy\n",
            "01456.npy  14976.npy  29140.npy  42861.npy  57426.npy  71568.npy  85439.npy\n",
            "01459.npy  15029.npy  29148.npy  42865.npy  57430.npy  71583.npy  85461.npy\n",
            "01465.npy  15037.npy  29150.npy  42871.npy  57431.npy  71584.npy  85471.npy\n",
            "01467.npy  15043.npy  29158.npy  42876.npy  57438.npy  71586.npy  85472.npy\n",
            "01468.npy  15049.npy  29163.npy  42896.npy  57463.npy  71593.npy  85476.npy\n",
            "01469.npy  15062.npy  29164.npy  42897.npy  57469.npy  71596.npy  85492.npy\n",
            "01472.npy  15064.npy  29173.npy  42903.npy  57481.npy  71602.npy  85496.npy\n",
            "01475.npy  15067.npy  29174.npy  42931.npy  57482.npy  71620.npy  85612.npy\n",
            "01476.npy  15072.npy  29178.npy  42936.npy  57490.npy  71624.npy  85614.npy\n",
            "01483.npy  15076.npy  29304.npy  42953.npy  57496.npy  71625.npy  85621.npy\n",
            "01487.npy  15078.npy  29308.npy  42958.npy  57602.npy  71628.npy  85630.npy\n",
            "01492.npy  15079.npy  29351.npy  42961.npy  57608.npy  71630.npy  85670.npy\n",
            "01497.npy  15083.npy  29354.npy  42967.npy  57618.npy  71634.npy  85679.npy\n",
            "01498.npy  15093.npy  29357.npy  42970.npy  57619.npy  71635.npy  85690.npy\n",
            "01524.npy  15097.npy  29376.npy  42975.npy  57621.npy  71645.npy  85704.npy\n",
            "01526.npy  15203.npy  29380.npy  42983.npy  57630.npy  71649.npy  85709.npy\n",
            "01527.npy  15208.npy  29384.npy  42986.npy  57631.npy  71658.npy  85710.npy\n",
            "01529.npy  15209.npy  29385.npy  42987.npy  57638.npy  71659.npy  85714.npy\n",
            "01546.npy  15236.npy  29387.npy  43015.npy  57649.npy  71680.npy  85716.npy\n",
            "01547.npy  15238.npy  29401.npy  43025.npy  57681.npy  71682.npy  85721.npy\n",
            "01548.npy  15243.npy  29406.npy  43026.npy  57693.npy  71689.npy  85729.npy\n",
            "01564.npy  15260.npy  29408.npy  43051.npy  57802.npy  71692.npy  85730.npy\n",
            "01568.npy  15267.npy  29416.npy  43052.npy  57812.npy  71823.npy  85741.npy\n",
            "01569.npy  15269.npy  29430.npy  43065.npy  57814.npy  71825.npy  85743.npy\n",
            "01576.npy  15270.npy  29437.npy  43078.npy  57821.npy  71826.npy  85746.npy\n",
            "01578.npy  15278.npy  29438.npy  43082.npy  57823.npy  71830.npy  85749.npy\n",
            "01579.npy  15280.npy  29460.npy  43085.npy  57826.npy  71832.npy  85761.npy\n",
            "01589.npy  15283.npy  29463.npy  43087.npy  57831.npy  71834.npy  85764.npy\n",
            "01598.npy  15290.npy  29467.npy  43089.npy  57836.npy  71836.npy  85793.npy\n",
            "01627.npy  15298.npy  29470.npy  43092.npy  57840.npy  71839.npy  85901.npy\n",
            "01634.npy  15306.npy  29475.npy  43095.npy  57849.npy  71845.npy  85902.npy\n",
            "01637.npy  15308.npy  29476.npy  43096.npy  57861.npy  71846.npy  85912.npy\n",
            "01642.npy  15320.npy  29478.npy  43109.npy  57869.npy  71849.npy  85916.npy\n",
            "01643.npy  15328.npy  29483.npy  43127.npy  57890.npy  71863.npy  85920.npy\n",
            "01649.npy  15349.npy  29485.npy  43128.npy  57893.npy  71869.npy  85923.npy\n",
            "01653.npy  15368.npy  29503.npy  43152.npy  57908.npy  71892.npy  85927.npy\n",
            "01658.npy  15374.npy  29506.npy  43162.npy  57912.npy  71902.npy  85930.npy\n",
            "01672.npy  15379.npy  29507.npy  43170.npy  57913.npy  71903.npy  85936.npy\n",
            "01682.npy  15394.npy  29508.npy  43175.npy  57914.npy  71905.npy  85941.npy\n",
            "01694.npy  15407.npy  29513.npy  43178.npy  57918.npy  71906.npy  85947.npy\n",
            "01698.npy  15409.npy  29517.npy  43180.npy  57921.npy  71920.npy  85960.npy\n",
            "01726.npy  15420.npy  29547.npy  43185.npy  57923.npy  71925.npy  85964.npy\n",
            "01734.npy  15427.npy  29574.npy  43190.npy  57926.npy  71930.npy  85967.npy\n",
            "01736.npy  15432.npy  29576.npy  43192.npy  57932.npy  71938.npy  85970.npy\n",
            "01742.npy  15438.npy  29581.npy  43197.npy  57938.npy  71942.npy  85976.npy\n",
            "01743.npy  15439.npy  29587.npy  43198.npy  57962.npy  71943.npy  86013.npy\n",
            "01746.npy  15473.npy  29608.npy  43201.npy  57968.npy  71950.npy  86017.npy\n",
            "01749.npy  15478.npy  29614.npy  43206.npy  57982.npy  71954.npy  86021.npy\n",
            "01756.npy  15479.npy  29630.npy  43207.npy  57984.npy  71963.npy  86023.npy\n",
            "01763.npy  15480.npy  29635.npy  43216.npy  57986.npy  71986.npy  86024.npy\n",
            "01765.npy  15489.npy  29643.npy  43217.npy  58014.npy  72014.npy  86025.npy\n",
            "01793.npy  15490.npy  29671.npy  43218.npy  58016.npy  72018.npy  86031.npy\n",
            "01794.npy  15493.npy  29675.npy  43258.npy  58019.npy  72019.npy  86032.npy\n",
            "01796.npy  15603.npy  29678.npy  43259.npy  58023.npy  72035.npy  86039.npy\n",
            "01823.npy  15609.npy  29684.npy  43265.npy  58026.npy  72046.npy  86041.npy\n",
            "01827.npy  15630.npy  29703.npy  43269.npy  58027.npy  72048.npy  86043.npy\n",
            "01829.npy  15638.npy  29706.npy  43271.npy  58029.npy  72058.npy  86045.npy\n",
            "01835.npy  15642.npy  29708.npy  43275.npy  58036.npy  72059.npy  86051.npy\n",
            "01836.npy  15647.npy  29710.npy  43278.npy  58046.npy  72061.npy  86054.npy\n",
            "01842.npy  15648.npy  29714.npy  43279.npy  58061.npy  72064.npy  86059.npy\n",
            "01845.npy  15649.npy  29731.npy  43295.npy  58069.npy  72084.npy  86071.npy\n",
            "01854.npy  15674.npy  29735.npy  43506.npy  58071.npy  72093.npy  86072.npy\n",
            "01865.npy  15690.npy  29740.npy  43517.npy  58079.npy  72094.npy  86075.npy\n",
            "01875.npy  15694.npy  29750.npy  43519.npy  58091.npy  72095.npy  86103.npy\n",
            "01892.npy  15720.npy  29756.npy  43520.npy  58093.npy  72130.npy  86104.npy\n",
            "01894.npy  15723.npy  29760.npy  43521.npy  58096.npy  72136.npy  86123.npy\n",
            "01896.npy  15726.npy  29761.npy  43527.npy  58109.npy  72145.npy  86125.npy\n",
            "01924.npy  15728.npy  29764.npy  43529.npy  58124.npy  72146.npy  86127.npy\n",
            "01925.npy  15734.npy  29785.npy  43560.npy  58127.npy  72160.npy  86129.npy\n",
            "01936.npy  15738.npy  29786.npy  43569.npy  58129.npy  72164.npy  86135.npy\n",
            "01937.npy  15743.npy  29813.npy  43570.npy  58130.npy  72168.npy  86139.npy\n",
            "01943.npy  15746.npy  29814.npy  43571.npy  58134.npy  72194.npy  86142.npy\n",
            "01953.npy  15763.npy  29816.npy  43579.npy  58136.npy  72195.npy  86143.npy\n",
            "01954.npy  15768.npy  29835.npy  43581.npy  58160.npy  72198.npy  86145.npy\n",
            "01956.npy  15796.npy  29841.npy  43608.npy  58164.npy  72301.npy  86159.npy\n",
            "01962.npy  15798.npy  29843.npy  43609.npy  58170.npy  72305.npy  86170.npy\n",
            "01967.npy  15803.npy  29850.npy  43610.npy  58176.npy  72310.npy  86172.npy\n",
            "01972.npy  15804.npy  29851.npy  43612.npy  58190.npy  72318.npy  86173.npy\n",
            "01974.npy  15806.npy  29863.npy  43615.npy  58194.npy  72340.npy  86175.npy\n",
            "01975.npy  15809.npy  29873.npy  43617.npy  58196.npy  72345.npy  86179.npy\n",
            "02139.npy  15820.npy  30124.npy  43650.npy  58197.npy  72354.npy  86193.npy\n",
            "02143.npy  15824.npy  30128.npy  43651.npy  58204.npy  72356.npy  86194.npy\n",
            "02145.npy  15839.npy  30142.npy  43652.npy  58209.npy  72358.npy  86195.npy\n",
            "02146.npy  15846.npy  30145.npy  43657.npy  58210.npy  72361.npy  86201.npy\n",
            "02153.npy  15847.npy  30148.npy  43658.npy  58217.npy  72364.npy  86203.npy\n",
            "02156.npy  15872.npy  30154.npy  43675.npy  58246.npy  72369.npy  86205.npy\n",
            "02157.npy  15892.npy  30156.npy  43679.npy  58247.npy  72380.npy  86207.npy\n",
            "02158.npy  15893.npy  30157.npy  43680.npy  58264.npy  72386.npy  86215.npy\n",
            "02165.npy  15902.npy  30162.npy  43690.npy  58276.npy  72396.npy  86217.npy\n",
            "02168.npy  15906.npy  30165.npy  43691.npy  58301.npy  72406.npy  86231.npy\n",
            "02169.npy  15907.npy  30168.npy  43695.npy  58304.npy  72413.npy  86237.npy\n",
            "02185.npy  15920.npy  30174.npy  43697.npy  58306.npy  72415.npy  86249.npy\n",
            "02194.npy  15926.npy  30175.npy  43698.npy  58309.npy  72418.npy  86250.npy\n",
            "02315.npy  15936.npy  30182.npy  43701.npy  58310.npy  72419.npy  86251.npy\n",
            "02316.npy  15937.npy  30185.npy  43702.npy  58312.npy  72430.npy  86253.npy\n",
            "02317.npy  15938.npy  30192.npy  43716.npy  58321.npy  72431.npy  86254.npy\n",
            "02351.npy  15942.npy  30196.npy  43721.npy  58326.npy  72450.npy  86257.npy\n",
            "02356.npy  15948.npy  30241.npy  43725.npy  58327.npy  72451.npy  86271.npy\n",
            "02358.npy  15964.npy  30245.npy  43728.npy  58329.npy  72456.npy  86273.npy\n",
            "02364.npy  15983.npy  30247.npy  43758.npy  58340.npy  72459.npy  86293.npy\n",
            "02365.npy  16029.npy  30248.npy  43759.npy  58349.npy  72461.npy  86307.npy\n",
            "02367.npy  16032.npy  30249.npy  43780.npy  58361.npy  72469.npy  86314.npy\n",
            "02371.npy  16035.npy  30251.npy  43782.npy  58367.npy  72486.npy  86325.npy\n",
            "02374.npy  16039.npy  30256.npy  43791.npy  58370.npy  72489.npy  86329.npy\n",
            "02381.npy  16042.npy  30257.npy  43792.npy  58371.npy  72490.npy  86351.npy\n",
            "02384.npy  16045.npy  30258.npy  43798.npy  58372.npy  72504.npy  86354.npy\n",
            "02385.npy  16047.npy  30259.npy  43805.npy  58374.npy  72506.npy  86357.npy\n",
            "02389.npy  16048.npy  30267.npy  43810.npy  58417.npy  72508.npy  86372.npy\n",
            "02413.npy  16049.npy  30276.npy  43812.npy  58419.npy  72509.npy  86391.npy\n",
            "02416.npy  16053.npy  30286.npy  43815.npy  58421.npy  72510.npy  86394.npy\n",
            "02431.npy  16054.npy  30289.npy  43816.npy  58426.npy  72514.npy  86401.npy\n",
            "02435.npy  16058.npy  30291.npy  43817.npy  58462.npy  72531.npy  86413.npy\n",
            "02439.npy  16059.npy  30296.npy  43819.npy  58463.npy  72536.npy  86417.npy\n",
            "02456.npy  16072.npy  30412.npy  43826.npy  58467.npy  72541.npy  86425.npy\n",
            "02457.npy  16075.npy  30452.npy  43852.npy  58471.npy  72561.npy  86431.npy\n",
            "02459.npy  16082.npy  30456.npy  43856.npy  58473.npy  72580.npy  86437.npy\n",
            "02461.npy  16084.npy  30462.npy  43857.npy  58476.npy  72584.npy  86451.npy\n",
            "02467.npy  16089.npy  30465.npy  43859.npy  58479.npy  72591.npy  86452.npy\n",
            "02471.npy  16093.npy  30481.npy  43895.npy  58490.npy  72594.npy  86453.npy\n",
            "02475.npy  16097.npy  30486.npy  43905.npy  58491.npy  72598.npy  86457.npy\n",
            "02476.npy  16098.npy  30487.npy  43906.npy  58497.npy  72604.npy  86479.npy\n",
            "02478.npy  16207.npy  30489.npy  43907.npy  58602.npy  72605.npy  86491.npy\n",
            "02481.npy  16208.npy  30492.npy  43910.npy  58604.npy  72608.npy  86504.npy\n",
            "02483.npy  16234.npy  30518.npy  43920.npy  58607.npy  72609.npy  86509.npy\n",
            "02485.npy  16240.npy  30526.npy  43928.npy  58609.npy  72610.npy  86512.npy\n",
            "02497.npy  16248.npy  30527.npy  43952.npy  58612.npy  72634.npy  86514.npy\n",
            "02514.npy  16253.npy  30546.npy  43956.npy  58631.npy  72638.npy  86529.npy\n",
            "02518.npy  16254.npy  30548.npy  43958.npy  58637.npy  72640.npy  86530.npy\n",
            "02519.npy  16259.npy  30549.npy  43961.npy  58640.npy  72641.npy  86540.npy\n",
            "02536.npy  16275.npy  30561.npy  43971.npy  58642.npy  72654.npy  86543.npy\n",
            "02537.npy  16278.npy  30567.npy  45016.npy  58649.npy  72658.npy  86593.npy\n",
            "02538.npy  16280.npy  30569.npy  45017.npy  58672.npy  72680.npy  86594.npy\n",
            "02543.npy  16283.npy  30571.npy  45023.npy  58674.npy  72685.npy  86597.npy\n",
            "02548.npy  16297.npy  30576.npy  45029.npy  58679.npy  72689.npy  86701.npy\n",
            "02561.npy  16298.npy  30579.npy  45031.npy  58690.npy  72698.npy  86705.npy\n",
            "02568.npy  16302.npy  30582.npy  45036.npy  58703.npy  72801.npy  86713.npy\n",
            "02571.npy  16320.npy  30586.npy  45037.npy  58706.npy  72805.npy  86714.npy\n",
            "02576.npy  16340.npy  30591.npy  45062.npy  58716.npy  72810.npy  86725.npy\n",
            "02581.npy  16348.npy  30597.npy  45069.npy  58730.npy  72814.npy  86730.npy\n",
            "02584.npy  16354.npy  30598.npy  45072.npy  58732.npy  72816.npy  86741.npy\n",
            "02594.npy  16357.npy  30615.npy  45093.npy  58739.npy  72819.npy  86750.npy\n",
            "02613.npy  16359.npy  30618.npy  45103.npy  58740.npy  72830.npy  86751.npy\n",
            "02614.npy  16372.npy  30628.npy  45108.npy  58743.npy  72834.npy  86753.npy\n",
            "02631.npy  16374.npy  30629.npy  45109.npy  58764.npy  72839.npy  86795.npy\n",
            "02634.npy  16380.npy  30642.npy  45120.npy  58769.npy  72850.npy  86907.npy\n",
            "02647.npy  16389.npy  30648.npy  45126.npy  58793.npy  72851.npy  86910.npy\n",
            "02649.npy  16395.npy  30652.npy  45128.npy  58901.npy  72864.npy  86912.npy\n",
            "02653.npy  16407.npy  30675.npy  45136.npy  58904.npy  72891.npy  86914.npy\n",
            "02654.npy  16409.npy  30682.npy  45139.npy  58906.npy  72893.npy  86917.npy\n",
            "02657.npy  16420.npy  30684.npy  45167.npy  58914.npy  72901.npy  86925.npy\n",
            "02674.npy  16423.npy  30687.npy  45172.npy  58916.npy  72903.npy  86934.npy\n",
            "02687.npy  16435.npy  30692.npy  45176.npy  58917.npy  72904.npy  86947.npy\n",
            "02691.npy  16437.npy  30695.npy  45179.npy  58924.npy  72910.npy  86954.npy\n",
            "02716.npy  16438.npy  30697.npy  45180.npy  58947.npy  72914.npy  86970.npy\n",
            "02718.npy  16439.npy  30714.npy  45182.npy  58971.npy  72915.npy  86974.npy\n",
            "02719.npy  16452.npy  30721.npy  45189.npy  58974.npy  72936.npy  87013.npy\n",
            "02735.npy  16458.npy  30726.npy  45197.npy  59012.npy  72940.npy  87016.npy\n",
            "02751.npy  16473.npy  30742.npy  45198.npy  59021.npy  72941.npy  87023.npy\n",
            "02761.npy  16478.npy  30745.npy  45201.npy  59024.npy  72945.npy  87024.npy\n",
            "02763.npy  16480.npy  30756.npy  45203.npy  59026.npy  72946.npy  87031.npy\n",
            "02764.npy  16487.npy  30761.npy  45206.npy  59028.npy  72951.npy  87034.npy\n",
            "02768.npy  16490.npy  30762.npy  45207.npy  59031.npy  72956.npy  87039.npy\n",
            "02769.npy  16492.npy  30764.npy  45208.npy  59034.npy  72958.npy  87043.npy\n",
            "02783.npy  16502.npy  30782.npy  45209.npy  59041.npy  72964.npy  87049.npy\n",
            "02789.npy  16509.npy  30785.npy  45213.npy  59043.npy  72965.npy  87052.npy\n",
            "02793.npy  16520.npy  30792.npy  45216.npy  59046.npy  72968.npy  87053.npy\n",
            "02795.npy  16529.npy  30796.npy  45219.npy  59047.npy  72981.npy  87054.npy\n",
            "02814.npy  16532.npy  30815.npy  45231.npy  59048.npy  72984.npy  87059.npy\n",
            "02815.npy  16537.npy  30816.npy  45263.npy  59061.npy  73021.npy  87063.npy\n",
            "02816.npy  16538.npy  30819.npy  45267.npy  59062.npy  73026.npy  87065.npy\n",
            "02831.npy  16539.npy  30824.npy  45269.npy  59064.npy  73028.npy  87094.npy\n",
            "02841.npy  16540.npy  30826.npy  45281.npy  59072.npy  73045.npy  87096.npy\n",
            "02845.npy  16543.npy  30829.npy  45283.npy  59073.npy  73049.npy  87120.npy\n",
            "02846.npy  16548.npy  30841.npy  45286.npy  59107.npy  73056.npy  87125.npy\n",
            "02849.npy  16573.npy  30847.npy  45289.npy  59123.npy  73059.npy  87126.npy\n",
            "02853.npy  16579.npy  30849.npy  45291.npy  59124.npy  73069.npy  87130.npy\n",
            "02854.npy  16587.npy  30851.npy  45297.npy  59128.npy  73082.npy  87140.npy\n",
            "02857.npy  16592.npy  30854.npy  45301.npy  59130.npy  73091.npy  87146.npy\n",
            "02863.npy  16593.npy  30859.npy  45307.npy  59140.npy  73104.npy  87149.npy\n",
            "02867.npy  16598.npy  30862.npy  45316.npy  59142.npy  73105.npy  87160.npy\n",
            "02876.npy  16702.npy  30864.npy  45317.npy  59143.npy  73125.npy  87165.npy\n",
            "02891.npy  16704.npy  30871.npy  45318.npy  59148.npy  73128.npy  87169.npy\n",
            "02894.npy  16720.npy  30872.npy  45320.npy  59163.npy  73146.npy  87190.npy\n",
            "02914.npy  16723.npy  30896.npy  45326.npy  59167.npy  73152.npy  87196.npy\n",
            "02917.npy  16734.npy  30914.npy  45368.npy  59170.npy  73154.npy  87203.npy\n",
            "02918.npy  16749.npy  30918.npy  45370.npy  59178.npy  73159.npy  87209.npy\n",
            "02935.npy  16758.npy  30927.npy  45371.npy  59186.npy  73160.npy  87215.npy\n",
            "02943.npy  16759.npy  30928.npy  45379.npy  59203.npy  73162.npy  87219.npy\n",
            "02945.npy  16780.npy  30942.npy  45382.npy  59206.npy  73164.npy  87230.npy\n",
            "02946.npy  16783.npy  30948.npy  45389.npy  59217.npy  73168.npy  87234.npy\n",
            "02947.npy  16784.npy  30954.npy  45396.npy  59230.npy  73180.npy  87235.npy\n",
            "02951.npy  16785.npy  30957.npy  45601.npy  59231.npy  73192.npy  87239.npy\n",
            "02956.npy  16820.npy  30958.npy  45603.npy  59237.npy  73198.npy  87241.npy\n",
            "02958.npy  16824.npy  30961.npy  45608.npy  59238.npy  73204.npy  87246.npy\n",
            "02965.npy  16827.npy  30982.npy  45609.npy  59240.npy  73205.npy  87251.npy\n",
            "02967.npy  16830.npy  30986.npy  45610.npy  59248.npy  73206.npy  87253.npy\n",
            "02971.npy  16832.npy  31024.npy  45612.npy  59260.npy  73218.npy  87260.npy\n",
            "02973.npy  16837.npy  31025.npy  45630.npy  59261.npy  73219.npy  87261.npy\n",
            "02974.npy  16842.npy  31029.npy  45671.npy  59264.npy  73241.npy  87263.npy\n",
            "02975.npy  16845.npy  31042.npy  45672.npy  59278.npy  73248.npy  87290.npy\n",
            "02983.npy  16849.npy  31048.npy  45687.npy  59287.npy  73250.npy  87296.npy\n",
            "02984.npy  16850.npy  31057.npy  45691.npy  59301.npy  73251.npy  87304.npy\n",
            "02987.npy  16857.npy  31058.npy  45698.npy  59304.npy  73256.npy  87309.npy\n",
            "03124.npy  16870.npy  31059.npy  45702.npy  59307.npy  73258.npy  87315.npy\n",
            "03128.npy  16873.npy  31068.npy  45706.npy  59316.npy  73259.npy  87324.npy\n",
            "03145.npy  16874.npy  31074.npy  45708.npy  59317.npy  73406.npy  87326.npy\n",
            "03146.npy  16892.npy  31076.npy  45723.npy  59321.npy  73416.npy  87340.npy\n",
            "03148.npy  16894.npy  31082.npy  45730.npy  59327.npy  73421.npy  87341.npy\n",
            "03162.npy  16895.npy  31085.npy  45732.npy  59328.npy  73426.npy  87345.npy\n",
            "03164.npy  16903.npy  31092.npy  45739.npy  59340.npy  73465.npy  87351.npy\n",
            "03172.npy  16904.npy  31094.npy  45780.npy  59342.npy  73480.npy  87352.npy\n",
            "03178.npy  16920.npy  31095.npy  45792.npy  59346.npy  73482.npy  87364.npy\n",
            "03185.npy  16923.npy  31096.npy  45802.npy  59362.npy  73485.npy  87365.npy\n",
            "03186.npy  16927.npy  31097.npy  45806.npy  59364.npy  73496.npy  87390.npy\n",
            "03187.npy  16934.npy  31204.npy  45810.npy  59370.npy  73498.npy  87391.npy\n",
            "03189.npy  16935.npy  31205.npy  45817.npy  59376.npy  73506.npy  87392.npy\n",
            "03197.npy  16947.npy  31208.npy  45819.npy  59380.npy  73508.npy  87401.npy\n",
            "03214.npy  16948.npy  31209.npy  45827.npy  59384.npy  73514.npy  87403.npy\n",
            "03217.npy  16952.npy  31256.npy  45829.npy  59401.npy  73516.npy  87406.npy\n",
            "03241.npy  16954.npy  31267.npy  45831.npy  59402.npy  73518.npy  87409.npy\n",
            "03246.npy  16973.npy  31268.npy  45832.npy  59408.npy  73520.npy  87412.npy\n",
            "03248.npy  17023.npy  31270.npy  45836.npy  59410.npy  73526.npy  87413.npy\n",
            "03251.npy  17025.npy  31278.npy  45837.npy  59423.npy  73549.npy  87416.npy\n",
            "03254.npy  17028.npy  31280.npy  45871.npy  59436.npy  73561.npy  87420.npy\n",
            "03256.npy  17034.npy  31284.npy  45891.npy  59460.npy  73562.npy  87425.npy\n",
            "03257.npy  17045.npy  31285.npy  45912.npy  59462.npy  73568.npy  87426.npy\n",
            "03258.npy  17052.npy  31287.npy  45923.npy  59463.npy  73581.npy  87429.npy\n",
            "03267.npy  17054.npy  31290.npy  45927.npy  59467.npy  73596.npy  87430.npy\n",
            "03268.npy  17058.npy  31294.npy  45930.npy  59468.npy  73598.npy  87436.npy\n",
            "03271.npy  17062.npy  31295.npy  45931.npy  59470.npy  73601.npy  87451.npy\n",
            "03275.npy  17082.npy  31405.npy  45938.npy  59471.npy  73604.npy  87453.npy\n",
            "03276.npy  17086.npy  31406.npy  45960.npy  59473.npy  73605.npy  87461.npy\n",
            "03279.npy  17089.npy  31408.npy  45976.npy  59478.npy  73609.npy  87501.npy\n",
            "03281.npy  17093.npy  31409.npy  45978.npy  59487.npy  73615.npy  87504.npy\n",
            "03285.npy  17096.npy  31426.npy  45987.npy  59601.npy  73619.npy  87510.npy\n",
            "03289.npy  17205.npy  31429.npy  46013.npy  59602.npy  73620.npy  87513.npy\n",
            "03291.npy  17208.npy  31450.npy  46017.npy  59613.npy  73652.npy  87519.npy\n",
            "03296.npy  17209.npy  31465.npy  46021.npy  59617.npy  73659.npy  87520.npy\n",
            "03298.npy  17230.npy  31467.npy  46027.npy  59621.npy  73680.npy  87521.npy\n",
            "03418.npy  17234.npy  31468.npy  46051.npy  59623.npy  73681.npy  87523.npy\n",
            "03421.npy  17235.npy  31470.npy  46053.npy  59627.npy  73690.npy  87526.npy\n",
            "03429.npy  17236.npy  31472.npy  46058.npy  59641.npy  73806.npy  87530.npy\n",
            "03468.npy  17238.npy  31479.npy  46075.npy  59671.npy  73810.npy  87532.npy\n",
            "03472.npy  17248.npy  31485.npy  46078.npy  59672.npy  73814.npy  87539.npy\n",
            "03479.npy  17253.npy  31487.npy  46081.npy  59678.npy  73816.npy  87541.npy\n",
            "03482.npy  17265.npy  31495.npy  46082.npy  59682.npy  73819.npy  87543.npy\n",
            "03495.npy  17268.npy  31496.npy  46085.npy  59684.npy  73841.npy  87564.npy\n",
            "03519.npy  17280.npy  31497.npy  46087.npy  59701.npy  73842.npy  87592.npy\n",
            "03524.npy  17285.npy  31508.npy  46095.npy  59708.npy  73846.npy  87594.npy\n",
            "03528.npy  17289.npy  31527.npy  46097.npy  59714.npy  73849.npy  87596.npy\n",
            "03547.npy  17290.npy  31547.npy  46123.npy  59716.npy  73910.npy  87604.npy\n",
            "03567.npy  17294.npy  31549.npy  46127.npy  59718.npy  73914.npy  87610.npy\n",
            "03568.npy  17296.npy  31560.npy  46130.npy  59726.npy  73924.npy  87615.npy\n",
            "03574.npy  17305.npy  31568.npy  46137.npy  59731.npy  73926.npy  87619.npy\n",
            "03591.npy  17306.npy  31569.npy  46138.npy  59732.npy  73945.npy  87620.npy\n",
            "03615.npy  17308.npy  31570.npy  46150.npy  59738.npy  73956.npy  87621.npy\n",
            "03624.npy  17326.npy  31586.npy  46152.npy  59743.npy  73962.npy  87625.npy\n",
            "03629.npy  17345.npy  31590.npy  46172.npy  59761.npy  73965.npy  87634.npy\n",
            "03642.npy  17348.npy  31604.npy  46173.npy  59784.npy  73980.npy  87639.npy\n",
            "03659.npy  17352.npy  31605.npy  46175.npy  59801.npy  73981.npy  87645.npy\n",
            "03675.npy  17354.npy  31609.npy  46178.npy  59806.npy  73982.npy  87649.npy\n",
            "03678.npy  17356.npy  31625.npy  46179.npy  59812.npy  73984.npy  87691.npy\n",
            "03681.npy  17358.npy  31627.npy  46182.npy  59817.npy  73986.npy  87692.npy\n",
            "03685.npy  17359.npy  31629.npy  46183.npy  59824.npy  74013.npy  87693.npy\n",
            "03691.npy  17369.npy  31640.npy  46193.npy  59832.npy  74016.npy  87695.npy\n",
            "03715.npy  17385.npy  31645.npy  46197.npy  59837.npy  74019.npy  87902.npy\n",
            "03718.npy  17386.npy  31649.npy  46198.npy  59840.npy  74021.npy  87903.npy\n",
            "03728.npy  17390.npy  31652.npy  46205.npy  59862.npy  74025.npy  87905.npy\n",
            "03745.npy  17392.npy  31657.npy  46208.npy  59863.npy  74029.npy  87913.npy\n",
            "03751.npy  17398.npy  31678.npy  46213.npy  59864.npy  74031.npy  87914.npy\n",
            "03756.npy  17405.npy  31684.npy  46215.npy  59871.npy  74038.npy  87923.npy\n",
            "03759.npy  17420.npy  31687.npy  46217.npy  59876.npy  74053.npy  87924.npy\n",
            "03764.npy  17425.npy  31690.npy  46218.npy  60128.npy  74058.npy  87925.npy\n",
            "03765.npy  17430.npy  31694.npy  46231.npy  60132.npy  74059.npy  87934.npy\n",
            "03789.npy  17438.npy  31706.npy  46237.npy  60134.npy  74061.npy  87936.npy\n",
            "03794.npy  17439.npy  31708.npy  46238.npy  60142.npy  74062.npy  87946.npy\n",
            "03795.npy  17450.npy  31709.npy  46239.npy  60143.npy  74083.npy  87953.npy\n",
            "03798.npy  17453.npy  31720.npy  46251.npy  60147.npy  74091.npy  87956.npy\n",
            "03841.npy  17458.npy  31728.npy  46270.npy  60173.npy  74096.npy  89014.npy\n",
            "03845.npy  17459.npy  31729.npy  46271.npy  60175.npy  74098.npy  89025.npy\n",
            "03847.npy  17469.npy  31742.npy  46279.npy  60183.npy  74126.npy  89026.npy\n",
            "03849.npy  17483.npy  31748.npy  46283.npy  60185.npy  74132.npy  89034.npy\n",
            "03854.npy  17493.npy  31752.npy  46293.npy  60193.npy  74150.npy  89035.npy\n",
            "03861.npy  17495.npy  31758.npy  46295.npy  60197.npy  74152.npy  89042.npy\n",
            "03864.npy  17504.npy  31760.npy  46301.npy  60213.npy  74158.npy  89043.npy\n",
            "03865.npy  17508.npy  31764.npy  46302.npy  60214.npy  74183.npy  89056.npy\n",
            "03869.npy  17523.npy  31765.npy  46310.npy  60215.npy  74190.npy  89061.npy\n",
            "03871.npy  17524.npy  31769.npy  46312.npy  60234.npy  74192.npy  89063.npy\n",
            "03874.npy  17528.npy  31782.npy  46315.npy  60235.npy  74198.npy  89067.npy\n",
            "03875.npy  17530.npy  31786.npy  46318.npy  60238.npy  74203.npy  89071.npy\n",
            "03896.npy  17532.npy  31794.npy  46352.npy  60249.npy  74206.npy  89072.npy\n",
            "03917.npy  17536.npy  31805.npy  46357.npy  60254.npy  74215.npy  89073.npy\n",
            "03924.npy  17539.npy  31809.npy  46358.npy  60257.npy  74218.npy  89076.npy\n",
            "03927.npy  17546.npy  31824.npy  46359.npy  60289.npy  74230.npy  89102.npy\n",
            "03942.npy  17548.npy  31847.npy  46375.npy  60314.npy  74235.npy  89105.npy\n",
            "03947.npy  17564.npy  31849.npy  46380.npy  60317.npy  74236.npy  89106.npy\n",
            "03957.npy  17582.npy  31852.npy  46387.npy  60329.npy  74238.npy  89107.npy\n",
            "03962.npy  17589.npy  31857.npy  46501.npy  60345.npy  74250.npy  89123.npy\n",
            "03968.npy  17594.npy  31875.npy  46503.npy  60348.npy  74259.npy  89126.npy\n",
            "03971.npy  17596.npy  31876.npy  46507.npy  60357.npy  74261.npy  89136.npy\n",
            "03976.npy  17598.npy  31892.npy  46509.npy  60371.npy  74265.npy  89137.npy\n",
            "03981.npy  17603.npy  31894.npy  46510.npy  60374.npy  74268.npy  89140.npy\n",
            "03984.npy  17609.npy  31896.npy  46518.npy  60379.npy  74281.npy  89142.npy\n",
            "03987.npy  17623.npy  31897.npy  46520.npy  60384.npy  74286.npy  89146.npy\n",
            "04125.npy  17624.npy  31902.npy  46527.npy  60389.npy  74298.npy  89152.npy\n",
            "04126.npy  17630.npy  31907.npy  46528.npy  60398.npy  74302.npy  89153.npy\n",
            "04127.npy  17634.npy  31920.npy  46529.npy  60421.npy  74310.npy  89154.npy\n",
            "04132.npy  17642.npy  31924.npy  46532.npy  60427.npy  74312.npy  89156.npy\n",
            "04135.npy  17643.npy  31925.npy  46537.npy  60439.npy  74315.npy  89160.npy\n",
            "04138.npy  17645.npy  31927.npy  46538.npy  60451.npy  74318.npy  89162.npy\n",
            "04152.npy  17649.npy  31948.npy  46578.npy  60478.npy  74326.npy  89165.npy\n",
            "04153.npy  17652.npy  31960.npy  46580.npy  60483.npy  74350.npy  89170.npy\n",
            "04156.npy  17653.npy  31975.npy  46582.npy  60487.npy  74351.npy  89206.npy\n",
            "04157.npy  17659.npy  31985.npy  46701.npy  60513.npy  74352.npy  89236.npy\n",
            "04162.npy  17682.npy  32015.npy  46710.npy  60532.npy  74356.npy  89245.npy\n",
            "04163.npy  17698.npy  32017.npy  46712.npy  60538.npy  74359.npy  89250.npy\n",
            "04175.npy  17802.npy  32046.npy  46715.npy  60539.npy  74361.npy  89251.npy\n",
            "04183.npy  17805.npy  32049.npy  46720.npy  60541.npy  74362.npy  89263.npy\n",
            "04185.npy  17809.npy  32051.npy  46721.npy  60549.npy  74368.npy  89264.npy\n",
            "04187.npy  17829.npy  32067.npy  46730.npy  60571.npy  74380.npy  89267.npy\n",
            "04217.npy  17830.npy  32069.npy  46732.npy  60573.npy  74385.npy  89275.npy\n",
            "04239.npy  17832.npy  32076.npy  46735.npy  60578.npy  74386.npy  89302.npy\n",
            "04253.npy  17834.npy  32081.npy  46739.npy  60592.npy  74390.npy  89306.npy\n",
            "04256.npy  17835.npy  32089.npy  46753.npy  60598.npy  74391.npy  89307.npy\n",
            "04257.npy  17842.npy  32106.npy  46783.npy  60718.npy  74502.npy  89314.npy\n",
            "04263.npy  17843.npy  32140.npy  46785.npy  60721.npy  74508.npy  89320.npy\n",
            "04265.npy  17845.npy  32147.npy  46790.npy  60729.npy  74509.npy  89324.npy\n",
            "04267.npy  17849.npy  32150.npy  46792.npy  60732.npy  74513.npy  89325.npy\n",
            "04268.npy  17850.npy  32160.npy  46793.npy  60739.npy  74520.npy  89326.npy\n",
            "04273.npy  17852.npy  32164.npy  46807.npy  60741.npy  74530.npy  89342.npy\n",
            "04278.npy  17853.npy  32168.npy  46810.npy  60758.npy  74531.npy  89350.npy\n",
            "04281.npy  17860.npy  32174.npy  46812.npy  60759.npy  74536.npy  89356.npy\n",
            "04287.npy  17864.npy  32189.npy  46813.npy  60785.npy  74562.npy  89360.npy\n",
            "04295.npy  17869.npy  32401.npy  46815.npy  60789.npy  74563.npy  89362.npy\n",
            "04296.npy  17892.npy  32405.npy  46817.npy  60794.npy  74583.npy  89367.npy\n",
            "04316.npy  17894.npy  32409.npy  46823.npy  60798.npy  74589.npy  89372.npy\n",
            "04319.npy  17904.npy  32410.npy  46827.npy  60812.npy  74598.npy  89375.npy\n",
            "04321.npy  17905.npy  32415.npy  46829.npy  60823.npy  74602.npy  89406.npy\n",
            "04325.npy  17908.npy  32416.npy  46830.npy  60827.npy  74608.npy  89407.npy\n",
            "04326.npy  17928.npy  32451.npy  46831.npy  60829.npy  74609.npy  89412.npy\n",
            "04328.npy  17932.npy  32459.npy  46832.npy  60839.npy  74612.npy  89417.npy\n",
            "04356.npy  17935.npy  32468.npy  46837.npy  60841.npy  74615.npy  89421.npy\n",
            "04361.npy  17938.npy  32470.npy  46850.npy  60892.npy  74621.npy  89423.npy\n",
            "04372.npy  17948.npy  32480.npy  46852.npy  60893.npy  74625.npy  89425.npy\n",
            "04379.npy  17950.npy  32481.npy  46853.npy  60895.npy  74630.npy  89430.npy\n",
            "04387.npy  17953.npy  32485.npy  46857.npy  60913.npy  74631.npy  89432.npy\n",
            "04397.npy  17956.npy  32490.npy  46870.npy  60917.npy  74650.npy  89436.npy\n",
            "04529.npy  17962.npy  32491.npy  46872.npy  60927.npy  74652.npy  89437.npy\n",
            "04536.npy  17963.npy  32507.npy  46895.npy  60931.npy  74658.npy  89450.npy\n",
            "04538.npy  17965.npy  32514.npy  46902.npy  60937.npy  74801.npy  89461.npy\n",
            "04563.npy  18029.npy  32516.npy  46907.npy  60938.npy  74802.npy  89465.npy\n",
            "04568.npy  18034.npy  32540.npy  46910.npy  60971.npy  74805.npy  89471.npy\n",
            "04569.npy  18045.npy  32549.npy  46918.npy  60973.npy  74813.npy  89475.npy\n",
            "04579.npy  18052.npy  32560.npy  46920.npy  60981.npy  74825.npy  89503.npy\n",
            "04582.npy  18059.npy  32564.npy  46921.npy  60982.npy  74832.npy  89504.npy\n",
            "04583.npy  18074.npy  32568.npy  46925.npy  60985.npy  74839.npy  89507.npy\n",
            "04591.npy  18094.npy  32570.npy  46928.npy  61028.npy  74861.npy  89512.npy\n",
            "04597.npy  18203.npy  32576.npy  46938.npy  61032.npy  74862.npy  89517.npy\n",
            "04615.npy  18204.npy  32579.npy  46952.npy  61037.npy  74890.npy  89521.npy\n",
            "04619.npy  18207.npy  32586.npy  46953.npy  61038.npy  74892.npy  89523.npy\n",
            "04621.npy  18234.npy  32587.npy  46970.npy  61054.npy  74901.npy  89526.npy\n",
            "04623.npy  18235.npy  32591.npy  46971.npy  61058.npy  74906.npy  89527.npy\n",
            "04629.npy  18236.npy  32605.npy  46973.npy  61082.npy  74908.npy  89532.npy\n",
            "04637.npy  18239.npy  32608.npy  46978.npy  61085.npy  74913.npy  89534.npy\n",
            "04639.npy  18246.npy  32615.npy  46980.npy  61089.npy  74915.npy  89536.npy\n",
            "04651.npy  18254.npy  32617.npy  46983.npy  61092.npy  74920.npy  89540.npy\n",
            "04658.npy  18257.npy  32640.npy  47012.npy  61093.npy  74923.npy  89541.npy\n",
            "04675.npy  18267.npy  32647.npy  47015.npy  61094.npy  74930.npy  89542.npy\n",
            "04682.npy  18273.npy  32649.npy  47016.npy  61097.npy  74951.npy  89543.npy\n",
            "04683.npy  18274.npy  32650.npy  47029.npy  61204.npy  74953.npy  89561.npy\n",
            "04689.npy  18275.npy  32658.npy  47031.npy  61205.npy  74956.npy  89567.npy\n",
            "04695.npy  18290.npy  32670.npy  47032.npy  61207.npy  74963.npy  89571.npy\n",
            "04712.npy  18294.npy  32674.npy  47051.npy  61208.npy  74965.npy  89573.npy\n",
            "04716.npy  18296.npy  32691.npy  47053.npy  61234.npy  74968.npy  89574.npy\n",
            "04718.npy  18306.npy  32695.npy  47056.npy  61250.npy  75016.npy  89602.npy\n",
            "04719.npy  18307.npy  32704.npy  47095.npy  61257.npy  75018.npy  89603.npy\n",
            "04726.npy  18325.npy  32706.npy  47096.npy  61258.npy  75021.npy  89607.npy\n",
            "04729.npy  18350.npy  32714.npy  47098.npy  61259.npy  75023.npy  89610.npy\n",
            "04732.npy  18356.npy  32716.npy  47103.npy  61270.npy  75026.npy  89612.npy\n",
            "04735.npy  18362.npy  32746.npy  47123.npy  61274.npy  75031.npy  89613.npy\n",
            "04758.npy  18364.npy  32750.npy  47125.npy  61278.npy  75036.npy  89615.npy\n",
            "04759.npy  18367.npy  32751.npy  47128.npy  61280.npy  75039.npy  89617.npy\n",
            "04762.npy  18379.npy  32765.npy  47136.npy  61285.npy  75041.npy  89621.npy\n",
            "04765.npy  18394.npy  32768.npy  47159.npy  61289.npy  75046.npy  89632.npy\n",
            "04768.npy  18397.npy  32781.npy  47162.npy  61297.npy  75063.npy  89634.npy\n",
            "04769.npy  18406.npy  32789.npy  47180.npy  61298.npy  75068.npy  89640.npy\n",
            "04782.npy  18407.npy  32794.npy  47183.npy  61304.npy  75081.npy  89642.npy\n",
            "04783.npy  18420.npy  32798.npy  47189.npy  61342.npy  75082.npy  89675.npy\n",
            "04786.npy  18427.npy  32806.npy  47192.npy  61345.npy  75086.npy  89723.npy\n",
            "04791.npy  18430.npy  32841.npy  47196.npy  61347.npy  75089.npy  89726.npy\n",
            "04798.npy  18432.npy  32849.npy  47203.npy  61349.npy  75092.npy  89732.npy\n",
            "04813.npy  18450.npy  32854.npy  47205.npy  61378.npy  75096.npy  89740.npy\n",
            "04819.npy  18453.npy  32867.npy  47208.npy  61379.npy  75098.npy  89741.npy\n",
            "04823.npy  18462.npy  32875.npy  47209.npy  61380.npy  75102.npy  89743.npy\n",
            "04827.npy  18465.npy  32876.npy  47215.npy  61382.npy  75104.npy  89745.npy\n",
            "04857.npy  18469.npy  32891.npy  47239.npy  61395.npy  75106.npy  89750.npy\n",
            "04859.npy  18490.npy  32896.npy  47250.npy  61407.npy  75123.npy  89754.npy\n",
            "04861.npy  18497.npy  32897.npy  47251.npy  61408.npy  75130.npy  89756.npy\n",
            "04863.npy  18504.npy  32906.npy  47259.npy  61420.npy  75132.npy  89761.npy\n",
            "04873.npy  18507.npy  32907.npy  47263.npy  61423.npy  75134.npy  89762.npy\n",
            "04876.npy  18524.npy  32915.npy  47265.npy  61425.npy  75136.npy  89764.npy\n",
            "04879.npy  18526.npy  32941.npy  47286.npy  61438.npy  75138.npy  90125.npy\n",
            "04891.npy  18529.npy  32947.npy  47289.npy  61450.npy  75139.npy  90126.npy\n",
            "04892.npy  18532.npy  32956.npy  47290.npy  61453.npy  75142.npy  90127.npy\n",
            "04912.npy  18542.npy  32957.npy  47308.npy  61458.npy  75146.npy  90128.npy\n",
            "04915.npy  18546.npy  32960.npy  47309.npy  61459.npy  75160.npy  90136.npy\n",
            "04917.npy  18547.npy  32965.npy  47320.npy  61480.npy  75163.npy  90138.npy\n",
            "04918.npy  18593.npy  32971.npy  47326.npy  61482.npy  75182.npy  90143.npy\n",
            "04923.npy  18594.npy  32974.npy  47350.npy  61483.npy  75192.npy  90145.npy\n",
            "04926.npy  18597.npy  32980.npy  47358.npy  61485.npy  75193.npy  90146.npy\n",
            "04928.npy  18607.npy  32981.npy  47359.npy  61492.npy  75198.npy  90148.npy\n",
            "04938.npy  18632.npy  32985.npy  47368.npy  61495.npy  75203.npy  90162.npy\n",
            "04958.npy  18640.npy  34015.npy  47369.npy  61497.npy  75204.npy  90165.npy\n",
            "04971.npy  18650.npy  34018.npy  47385.npy  61502.npy  75209.npy  90175.npy\n",
            "04975.npy  18673.npy  34021.npy  47386.npy  61503.npy  75210.npy  90178.npy\n",
            "04976.npy  18674.npy  34028.npy  47390.npy  61527.npy  75216.npy  90183.npy\n",
            "04986.npy  18675.npy  34051.npy  47391.npy  61532.npy  75230.npy  90186.npy\n",
            "05123.npy  18679.npy  34056.npy  47506.npy  61537.npy  75231.npy  90214.npy\n",
            "05126.npy  18693.npy  34057.npy  47510.npy  61538.npy  75234.npy  90236.npy\n",
            "05127.npy  18724.npy  34059.npy  47516.npy  61549.npy  75239.npy  90243.npy\n",
            "05129.npy  18726.npy  34067.npy  47518.npy  61570.npy  75241.npy  90247.npy\n",
            "05134.npy  18730.npy  34068.npy  47521.npy  61578.npy  75260.npy  90251.npy\n",
            "05138.npy  18739.npy  34072.npy  47528.npy  61580.npy  75269.npy  90256.npy\n",
            "05148.npy  18742.npy  34075.npy  47529.npy  61589.npy  75280.npy  90257.npy\n",
            "05162.npy  18750.npy  34076.npy  47530.npy  61590.npy  75286.npy  90263.npy\n",
            "05164.npy  18759.npy  34078.npy  47531.npy  61597.npy  75290.npy  90267.npy\n",
            "05172.npy  18764.npy  34079.npy  47532.npy  61702.npy  75291.npy  90274.npy\n",
            "05174.npy  18790.npy  34081.npy  47561.npy  61723.npy  75294.npy  90278.npy\n",
            "05183.npy  18926.npy  34089.npy  47569.npy  61725.npy  75298.npy  90285.npy\n",
            "05189.npy  18935.npy  34091.npy  47581.npy  61730.npy  75308.npy  90286.npy\n",
            "05198.npy  18937.npy  34092.npy  47582.npy  61732.npy  75319.npy  90315.npy\n",
            "05213.npy  18940.npy  34095.npy  47589.npy  61742.npy  75320.npy  90318.npy\n",
            "05214.npy  18942.npy  34096.npy  47591.npy  61743.npy  75321.npy  90321.npy\n",
            "05218.npy  18943.npy  34097.npy  47592.npy  61749.npy  75340.npy  90342.npy\n",
            "05219.npy  18945.npy  34106.npy  47596.npy  61752.npy  75349.npy  90345.npy\n",
            "05231.npy  18946.npy  34107.npy  47598.npy  61753.npy  75360.npy  90351.npy\n",
            "05241.npy  18960.npy  34109.npy  47601.npy  61754.npy  75362.npy  90365.npy\n",
            "05249.npy  18963.npy  34127.npy  47605.npy  61780.npy  75380.npy  90378.npy\n",
            "05261.npy  19026.npy  34129.npy  47608.npy  61792.npy  75381.npy  90382.npy\n",
            "05264.npy  19027.npy  34152.npy  47609.npy  61793.npy  75382.npy  90386.npy\n",
            "05269.npy  19032.npy  34157.npy  47612.npy  61798.npy  75402.npy  90413.npy\n",
            "05273.npy  19036.npy  34162.npy  47615.npy  61802.npy  75403.npy  90417.npy\n",
            "05276.npy  19047.npy  34165.npy  47618.npy  61820.npy  75408.npy  90427.npy\n",
            "05279.npy  19062.npy  34170.npy  47620.npy  61823.npy  75410.npy  90461.npy\n",
            "05283.npy  19065.npy  34178.npy  47625.npy  61825.npy  75420.npy  90463.npy\n",
            "05294.npy  19072.npy  34179.npy  47628.npy  61827.npy  75421.npy  90468.npy\n",
            "05297.npy  19074.npy  34180.npy  47629.npy  61834.npy  75428.npy  90471.npy\n",
            "05316.npy  19075.npy  34186.npy  47680.npy  61840.npy  75430.npy  90481.npy\n",
            "05327.npy  19085.npy  34189.npy  47692.npy  61859.npy  75432.npy  90483.npy\n",
            "05329.npy  19203.npy  34190.npy  47693.npy  61870.npy  75438.npy  90513.npy\n",
            "05349.npy  19205.npy  34196.npy  47698.npy  61872.npy  75461.npy  90516.npy\n",
            "05362.npy  19206.npy  34206.npy  47801.npy  61873.npy  75462.npy  90531.npy\n",
            "05369.npy  19230.npy  34207.npy  47809.npy  61890.npy  75468.npy  90534.npy\n",
            "05372.npy  19234.npy  34209.npy  47819.npy  61892.npy  75481.npy  90538.npy\n",
            "05376.npy  19238.npy  34215.npy  47825.npy  61894.npy  75496.npy  90541.npy\n",
            "05379.npy  19240.npy  34216.npy  47826.npy  61904.npy  75498.npy  90564.npy\n",
            "05384.npy  19243.npy  34217.npy  47829.npy  61905.npy  75601.npy  90568.npy\n",
            "05387.npy  19247.npy  34219.npy  47830.npy  61908.npy  75602.npy  90572.npy\n",
            "05389.npy  19248.npy  34250.npy  47831.npy  61927.npy  75603.npy  90573.npy\n",
            "05398.npy  19256.npy  34258.npy  47836.npy  61928.npy  75613.npy  90574.npy\n",
            "05421.npy  19258.npy  34275.npy  47856.npy  61932.npy  75614.npy  90576.npy\n",
            "05429.npy  19268.npy  34291.npy  47859.npy  61935.npy  75618.npy  90582.npy\n",
            "05436.npy  19275.npy  34295.npy  47862.npy  61937.npy  75624.npy  90583.npy\n",
            "05437.npy  19302.npy  34297.npy  47863.npy  61940.npy  75629.npy  90586.npy\n",
            "05438.npy  19304.npy  34298.npy  47865.npy  61948.npy  75639.npy  90587.npy\n",
            "05439.npy  19308.npy  34506.npy  47896.npy  61950.npy  75642.npy  90612.npy\n",
            "05461.npy  19320.npy  34508.npy  47901.npy  61952.npy  75648.npy  90614.npy\n",
            "05462.npy  19324.npy  34510.npy  47903.npy  61953.npy  75649.npy  90618.npy\n",
            "05463.npy  19326.npy  34518.npy  47905.npy  61973.npy  75680.npy  90625.npy\n",
            "05468.npy  19342.npy  34520.npy  47913.npy  61980.npy  75682.npy  90632.npy\n",
            "05471.npy  19345.npy  34521.npy  47918.npy  62017.npy  75693.npy  90638.npy\n",
            "05476.npy  19346.npy  34526.npy  47920.npy  62018.npy  75698.npy  90643.npy\n",
            "05479.npy  19352.npy  34528.npy  47926.npy  62019.npy  75801.npy  90648.npy\n",
            "05482.npy  19358.npy  34567.npy  47931.npy  62031.npy  75806.npy  90654.npy\n",
            "05483.npy  19362.npy  34570.npy  47938.npy  62035.npy  75810.npy  90657.npy\n",
            "05489.npy  19364.npy  34576.npy  47950.npy  62043.npy  75816.npy  90673.npy\n",
            "05498.npy  19367.npy  34581.npy  47960.npy  62047.npy  75820.npy  90681.npy\n",
            "05612.npy  19370.npy  34582.npy  47981.npy  62057.npy  75821.npy  90718.npy\n",
            "05614.npy  19372.npy  34586.npy  47982.npy  62081.npy  75823.npy  90723.npy\n",
            "05617.npy  19375.npy  34587.npy  47985.npy  62083.npy  75829.npy  90725.npy\n",
            "05619.npy  19380.npy  34591.npy  48012.npy  62084.npy  75832.npy  90728.npy\n",
            "05642.npy  19384.npy  34592.npy  48015.npy  62085.npy  75839.npy  90734.npy\n",
            "05643.npy  19385.npy  34596.npy  48021.npy  62097.npy  75840.npy  90742.npy\n",
            "05649.npy  19406.npy  34598.npy  48031.npy  62105.npy  75841.npy  90748.npy\n",
            "05672.npy  19408.npy  34602.npy  48039.npy  62108.npy  75846.npy  90752.npy\n",
            "05689.npy  19420.npy  34609.npy  48051.npy  62134.npy  75892.npy  90753.npy\n",
            "05712.npy  19426.npy  34618.npy  48052.npy  62135.npy  75894.npy  90756.npy\n",
            "05716.npy  19427.npy  34619.npy  48059.npy  62147.npy  75908.npy  90761.npy\n",
            "05719.npy  19430.npy  34620.npy  48062.npy  62149.npy  75910.npy  90768.npy\n",
            "05726.npy  19438.npy  34628.npy  48069.npy  62158.npy  75918.npy  90786.npy\n",
            "05734.npy  19452.npy  34629.npy  48079.npy  62175.npy  75920.npy  90814.npy\n",
            "05736.npy  19457.npy  34652.npy  48091.npy  62178.npy  75921.npy  90817.npy\n",
            "05741.npy  19458.npy  34658.npy  48096.npy  62184.npy  75923.npy  90826.npy\n",
            "05743.npy  19472.npy  34659.npy  48103.npy  62197.npy  75928.npy  90827.npy\n",
            "05749.npy  19487.npy  34670.npy  48106.npy  62198.npy  75931.npy  90834.npy\n",
            "05761.npy  19502.npy  34671.npy  48120.npy  62301.npy  75934.npy  90836.npy\n",
            "05762.npy  19503.npy  34675.npy  48123.npy  62305.npy  75940.npy  90842.npy\n",
            "05781.npy  19504.npy  34678.npy  48125.npy  62307.npy  75941.npy  90843.npy\n",
            "05782.npy  19507.npy  34680.npy  48132.npy  62308.npy  75943.npy  90845.npy\n",
            "05784.npy  19508.npy  34687.npy  48136.npy  62314.npy  75960.npy  90846.npy\n",
            "05789.npy  19523.npy  34695.npy  48153.npy  62315.npy  75984.npy  90847.npy\n",
            "05791.npy  19526.npy  34698.npy  48162.npy  62319.npy  76015.npy  90856.npy\n",
            "05792.npy  19530.npy  34708.npy  48170.npy  62340.npy  76021.npy  90861.npy\n",
            "05793.npy  19532.npy  34710.npy  48173.npy  62351.npy  76023.npy  90864.npy\n",
            "05798.npy  19536.npy  34715.npy  48175.npy  62354.npy  76032.npy  90875.npy\n",
            "05813.npy  19537.npy  34721.npy  48192.npy  62357.npy  76034.npy  91024.npy\n",
            "05824.npy  19540.npy  34726.npy  48196.npy  62358.npy  76035.npy  91026.npy\n",
            "05826.npy  19542.npy  34728.npy  48203.npy  62374.npy  76038.npy  91027.npy\n",
            "05832.npy  19543.npy  34751.npy  48205.npy  62375.npy  76045.npy  91034.npy\n",
            "05841.npy  19547.npy  34756.npy  48213.npy  62381.npy  76083.npy  91036.npy\n",
            "05846.npy  19563.npy  34765.npy  48216.npy  62384.npy  76091.npy  91037.npy\n",
            "05847.npy  19567.npy  34768.npy  48235.npy  62391.npy  76092.npy  91042.npy\n",
            "05863.npy  19573.npy  34780.npy  48236.npy  62398.npy  76093.npy  91045.npy\n",
            "05864.npy  19582.npy  34782.npy  48250.npy  62405.npy  76094.npy  91052.npy\n",
            "05869.npy  19586.npy  34785.npy  48257.npy  62408.npy  76095.npy  91056.npy\n",
            "05872.npy  19587.npy  34786.npy  48260.npy  62409.npy  76098.npy  91058.npy\n",
            "05874.npy  19602.npy  34791.npy  48263.npy  62413.npy  76103.npy  91063.npy\n",
            "05879.npy  19604.npy  34795.npy  48269.npy  62417.npy  76104.npy  91064.npy\n",
            "05912.npy  19630.npy  34796.npy  48270.npy  62418.npy  76108.npy  91065.npy\n",
            "05914.npy  19634.npy  34798.npy  48271.npy  62435.npy  76134.npy  91072.npy\n",
            "05916.npy  19635.npy  34805.npy  48276.npy  62439.npy  76138.npy  91075.npy\n",
            "05917.npy  19637.npy  34806.npy  48279.npy  62457.npy  76139.npy  91082.npy\n",
            "05918.npy  19643.npy  34815.npy  48291.npy  62458.npy  76140.npy  91083.npy\n",
            "05926.npy  19647.npy  34816.npy  48296.npy  62471.npy  76142.npy  91208.npy\n",
            "05928.npy  19650.npy  34821.npy  48306.npy  62483.npy  76143.npy  91234.npy\n",
            "05931.npy  19653.npy  34825.npy  48307.npy  62485.npy  76145.npy  91236.npy\n",
            "05936.npy  19670.npy  34852.npy  48309.npy  62490.npy  76148.npy  91240.npy\n",
            "05938.npy  19672.npy  34857.npy  48310.npy  62501.npy  76152.npy  91245.npy\n",
            "05941.npy  19678.npy  34862.npy  48315.npy  62504.npy  76158.npy  91260.npy\n",
            "05942.npy  19684.npy  34870.npy  48316.npy  62509.npy  76180.npy  91268.npy\n",
            "05943.npy  19702.npy  34871.npy  48317.npy  62510.npy  76183.npy  91270.npy\n",
            "05947.npy  19705.npy  34872.npy  48320.npy  62514.npy  76208.npy  91273.npy\n",
            "05948.npy  19726.npy  34875.npy  48326.npy  62531.npy  76213.npy  91275.npy\n",
            "05968.npy  19730.npy  34876.npy  48356.npy  62537.npy  76214.npy  91276.npy\n",
            "05972.npy  19736.npy  34897.npy  48357.npy  62541.npy  76219.npy  91283.npy\n",
            "05973.npy  19740.npy  34901.npy  48361.npy  62571.npy  76239.npy  91284.npy\n",
            "05976.npy  19742.npy  34910.npy  48370.npy  62573.npy  76243.npy  91286.npy\n",
            "05978.npy  19752.npy  34915.npy  48376.npy  62578.npy  76250.npy  91308.npy\n",
            "05984.npy  19753.npy  34920.npy  48379.npy  62580.npy  76253.npy  91320.npy\n",
            "05986.npy  19758.npy  34925.npy  48391.npy  62589.npy  76254.npy  91340.npy\n",
            "05987.npy  19763.npy  34927.npy  48396.npy  62590.npy  76258.npy  91345.npy\n",
            "06123.npy  19768.npy  34950.npy  48507.npy  62597.npy  76259.npy  91348.npy\n",
            "06125.npy  19783.npy  34952.npy  48512.npy  62703.npy  76283.npy  91356.npy\n",
            "06127.npy  19823.npy  34960.npy  48513.npy  62705.npy  76285.npy  91358.npy\n",
            "06135.npy  19824.npy  34961.npy  48516.npy  62710.npy  76293.npy  91367.npy\n",
            "06147.npy  19825.npy  34970.npy  48517.npy  62713.npy  76295.npy  91372.npy\n",
            "06153.npy  19834.npy  34971.npy  48520.npy  62719.npy  76298.npy  91374.npy\n",
            "06157.npy  19836.npy  34972.npy  48523.npy  62731.npy  76305.npy  91375.npy\n",
            "06158.npy  19837.npy  34975.npy  48539.npy  62739.npy  76320.npy  91385.npy\n",
            "06175.npy  19845.npy  34978.npy  48567.npy  62745.npy  76321.npy  91402.npy\n",
            "06179.npy  19846.npy  34981.npy  48570.npy  62751.npy  76328.npy  91405.npy\n",
            "06194.npy  19860.npy  34982.npy  48573.npy  62758.npy  76341.npy  91423.npy\n",
            "06195.npy  19875.npy  34985.npy  48579.npy  62789.npy  76342.npy  91428.npy\n",
            "06197.npy  20134.npy  35016.npy  48605.npy  62790.npy  76348.npy  91430.npy\n",
            "06198.npy  20143.npy  35017.npy  48612.npy  62791.npy  76351.npy  91432.npy\n",
            "06213.npy  20145.npy  35041.npy  48617.npy  62804.npy  76354.npy  91436.npy\n",
            "06218.npy  20146.npy  35048.npy  48623.npy  62805.npy  76358.npy  91453.npy\n",
            "06231.npy  20149.npy  35062.npy  48625.npy  62814.npy  76359.npy  91457.npy\n",
            "06237.npy  20154.npy  35064.npy  48630.npy  62815.npy  76381.npy  91462.npy\n",
            "06239.npy  20159.npy  35068.npy  48635.npy  62831.npy  76402.npy  91468.npy\n",
            "06243.npy  20163.npy  35074.npy  48639.npy  62840.npy  76405.npy  91476.npy\n",
            "06248.npy  20165.npy  35084.npy  48651.npy  62843.npy  76408.npy  91482.npy\n",
            "06273.npy  20174.npy  35086.npy  48652.npy  62849.npy  76415.npy  91483.npy\n",
            "06275.npy  20183.npy  35087.npy  48670.npy  62850.npy  76419.npy  91486.npy\n",
            "06291.npy  20184.npy  35091.npy  48673.npy  62851.npy  76421.npy  91526.npy\n",
            "06293.npy  20187.npy  35096.npy  48679.npy  62854.npy  76423.npy  91527.npy\n",
            "06295.npy  20194.npy  35097.npy  48697.npy  62859.npy  76429.npy  91530.npy\n",
            "06315.npy  20195.npy  35102.npy  48703.npy  62891.npy  76431.npy  91536.npy\n",
            "06319.npy  20196.npy  35104.npy  48705.npy  62904.npy  76435.npy  91537.npy\n",
            "06325.npy  20317.npy  35107.npy  48706.npy  62913.npy  76450.npy  91540.npy\n",
            "06329.npy  20318.npy  35146.npy  48710.npy  62931.npy  76458.npy  91547.npy\n",
            "06345.npy  20348.npy  35160.npy  48715.npy  62934.npy  76459.npy  91562.npy\n",
            "06348.npy  20356.npy  35167.npy  48725.npy  62941.npy  76483.npy  91563.npy\n",
            "06349.npy  20371.npy  35168.npy  48731.npy  62947.npy  76489.npy  91574.npy\n",
            "06352.npy  20374.npy  35170.npy  48735.npy  62948.npy  76491.npy  91584.npy\n",
            "06357.npy  20375.npy  35178.npy  48739.npy  62970.npy  76495.npy  91586.npy\n",
            "06359.npy  20379.npy  35182.npy  48751.npy  62971.npy  76498.npy  91602.npy\n",
            "06374.npy  20381.npy  35186.npy  48756.npy  62981.npy  76510.npy  91627.npy\n",
            "06375.npy  20386.npy  35189.npy  48760.npy  62985.npy  76519.npy  91637.npy\n",
            "06378.npy  20395.npy  35198.npy  48762.npy  63012.npy  76521.npy  91647.npy\n",
            "06387.npy  20413.npy  35210.npy  48790.npy  63025.npy  76524.npy  91654.npy\n",
            "06392.npy  20435.npy  35216.npy  48792.npy  63028.npy  76528.npy  91674.npy\n",
            "06412.npy  20437.npy  35217.npy  48905.npy  63029.npy  76532.npy  91675.npy\n",
            "06415.npy  20438.npy  35218.npy  48916.npy  63041.npy  76534.npy  91683.npy\n",
            "06418.npy  20459.npy  35219.npy  48917.npy  63042.npy  76538.npy  91704.npy\n",
            "06425.npy  20473.npy  35247.npy  48920.npy  63051.npy  76539.npy  91708.npy\n",
            "06427.npy  20485.npy  35249.npy  48921.npy  63052.npy  76542.npy  91724.npy\n",
            "06431.npy  20491.npy  35264.npy  48923.npy  63057.npy  76549.npy  91730.npy\n",
            "06435.npy  20497.npy  35271.npy  48925.npy  63074.npy  76581.npy  91736.npy\n",
            "06439.npy  20498.npy  35276.npy  48926.npy  63078.npy  76582.npy  91738.npy\n",
            "06458.npy  20513.npy  35279.npy  48927.npy  63087.npy  76583.npy  91740.npy\n",
            "06471.npy  20538.npy  35287.npy  48930.npy  63092.npy  76584.npy  91750.npy\n",
            "06479.npy  20539.npy  35289.npy  48932.npy  63095.npy  76592.npy  91753.npy\n",
            "06481.npy  20543.npy  35298.npy  48936.npy  63097.npy  76810.npy  91754.npy\n",
            "06482.npy  20567.npy  35402.npy  48956.npy  63104.npy  76814.npy  91756.npy\n",
            "06483.npy  20568.npy  35408.npy  48970.npy  63105.npy  76815.npy  91763.npy\n",
            "06489.npy  20578.npy  35412.npy  48971.npy  63124.npy  76819.npy  91764.npy\n",
            "06491.npy  20593.npy  35417.npy  48976.npy  63129.npy  76821.npy  91765.npy\n",
            "06513.npy  20598.npy  35419.npy  49017.npy  63140.npy  76824.npy  91768.npy\n",
            "06514.npy  20615.npy  35429.npy  49021.npy  63150.npy  76825.npy  91786.npy\n",
            "06519.npy  20617.npy  35460.npy  49023.npy  63159.npy  76840.npy  91802.npy\n",
            "06524.npy  20619.npy  35470.npy  49028.npy  63174.npy  76842.npy  91803.npy\n",
            "06531.npy  20643.npy  35472.npy  49032.npy  63175.npy  76843.npy  91806.npy\n",
            "06534.npy  20654.npy  35480.npy  49038.npy  63182.npy  76850.npy  91827.npy\n",
            "06537.npy  20657.npy  35482.npy  49061.npy  63185.npy  76851.npy  91836.npy\n",
            "06539.npy  20671.npy  35487.npy  49063.npy  63187.npy  76854.npy  91840.npy\n",
            "06541.npy  20673.npy  35497.npy  49067.npy  63192.npy  76890.npy  91842.npy\n",
            "06543.npy  20681.npy  35601.npy  49075.npy  63201.npy  76892.npy  91845.npy\n",
            "06547.npy  20684.npy  35602.npy  49076.npy  63207.npy  76894.npy  91853.npy\n",
            "06579.npy  20685.npy  35604.npy  49083.npy  63210.npy  76910.npy  91856.npy\n",
            "06582.npy  20691.npy  35607.npy  49087.npy  63217.npy  76913.npy  91862.npy\n",
            "06584.npy  20693.npy  35608.npy  49105.npy  63250.npy  76914.npy  91863.npy\n",
            "06589.npy  20715.npy  35614.npy  49106.npy  63251.npy  76920.npy  91872.npy\n",
            "06593.npy  20718.npy  35618.npy  49120.npy  63275.npy  76921.npy  92013.npy\n",
            "06597.npy  20736.npy  35620.npy  49126.npy  63280.npy  76923.npy  92014.npy\n",
            "06712.npy  20738.npy  35628.npy  49128.npy  63281.npy  76924.npy  92015.npy\n",
            "06714.npy  20739.npy  35640.npy  49136.npy  63285.npy  76931.npy  92016.npy\n",
            "06715.npy  20745.npy  35642.npy  49150.npy  63289.npy  76932.npy  92038.npy\n",
            "06723.npy  20749.npy  35647.npy  49152.npy  63290.npy  76938.npy  92041.npy\n",
            "06725.npy  20751.npy  35670.npy  49153.npy  63295.npy  76950.npy  92046.npy\n",
            "06728.npy  20759.npy  35671.npy  49156.npy  63410.npy  76951.npy  92051.npy\n",
            "06739.npy  20781.npy  35674.npy  49157.npy  63412.npy  76952.npy  92056.npy\n",
            "06749.npy  20786.npy  35678.npy  49165.npy  63415.npy  76953.npy  92058.npy\n",
            "06752.npy  20791.npy  35680.npy  49168.npy  63417.npy  76980.npy  92064.npy\n",
            "06759.npy  20795.npy  35684.npy  49170.npy  63418.npy  78014.npy  92067.npy\n",
            "06781.npy  20814.npy  35687.npy  49173.npy  63451.npy  78015.npy  92068.npy\n",
            "06791.npy  20815.npy  35689.npy  49176.npy  63452.npy  78031.npy  92075.npy\n",
            "06793.npy  20819.npy  35691.npy  49178.npy  63472.npy  78034.npy  92078.npy\n",
            "06794.npy  20835.npy  35692.npy  49185.npy  63490.npy  78035.npy  92081.npy\n",
            "06795.npy  20837.npy  35701.npy  49201.npy  63491.npy  78039.npy  92085.npy\n",
            "06798.npy  20851.npy  35708.npy  49208.npy  63492.npy  78052.npy  92086.npy\n",
            "06823.npy  20854.npy  35709.npy  49213.npy  63501.npy  78054.npy  92104.npy\n",
            "06824.npy  20857.npy  35716.npy  49215.npy  63502.npy  78062.npy  92136.npy\n",
            "06825.npy  20861.npy  35719.npy  49260.npy  63507.npy  78064.npy  92140.npy\n",
            "06832.npy  20867.npy  35724.npy  49267.npy  63510.npy  78069.npy  92143.npy\n",
            "06842.npy  20869.npy  35729.npy  49270.npy  63517.npy  78091.npy  92146.npy\n",
            "06845.npy  20873.npy  35740.npy  49271.npy  63520.npy  78092.npy  92148.npy\n",
            "06847.npy  20876.npy  35746.npy  49278.npy  63524.npy  78094.npy  92154.npy\n",
            "06852.npy  20879.npy  35764.npy  49280.npy  63547.npy  78095.npy  92157.npy\n",
            "06859.npy  20891.npy  35780.npy  49285.npy  63548.npy  78096.npy  92158.npy\n",
            "06871.npy  20913.npy  35781.npy  49316.npy  63572.npy  78104.npy  92160.npy\n",
            "06873.npy  20915.npy  35784.npy  49327.npy  63578.npy  78124.npy  92167.npy\n",
            "06874.npy  20918.npy  35786.npy  49328.npy  63579.npy  78125.npy  92168.npy\n",
            "06875.npy  20931.npy  35789.npy  49351.npy  63580.npy  78132.npy  92170.npy\n",
            "06892.npy  20936.npy  35794.npy  49360.npy  63581.npy  78134.npy  92180.npy\n",
            "06893.npy  20943.npy  35796.npy  49372.npy  63587.npy  78139.npy  92183.npy\n",
            "06897.npy  20945.npy  35801.npy  49385.npy  63589.npy  78152.npy  92185.npy\n",
            "06914.npy  20948.npy  35802.npy  49387.npy  63594.npy  78156.npy  92187.npy\n",
            "06927.npy  20951.npy  35814.npy  49527.npy  63702.npy  78163.npy  92308.npy\n",
            "06931.npy  20956.npy  35840.npy  49531.npy  63709.npy  78164.npy  92314.npy\n",
            "06934.npy  20957.npy  35860.npy  49536.npy  63710.npy  78190.npy  92317.npy\n",
            "06937.npy  20968.npy  35861.npy  49581.npy  63712.npy  78192.npy  92341.npy\n",
            "06945.npy  20971.npy  35869.npy  49583.npy  63714.npy  78205.npy  92354.npy\n",
            "06948.npy  20974.npy  35870.npy  49601.npy  63715.npy  78206.npy  92357.npy\n",
            "06951.npy  20978.npy  35890.npy  49607.npy  63718.npy  78214.npy  92364.npy\n",
            "06952.npy  20984.npy  35894.npy  49608.npy  63719.npy  78215.npy  92365.npy\n",
            "06973.npy  21035.npy  35896.npy  49613.npy  63720.npy  78219.npy  92368.npy\n",
            "06985.npy  21038.npy  35902.npy  49615.npy  63741.npy  78231.npy  92370.npy\n",
            "06987.npy  21043.npy  35907.npy  49618.npy  63745.npy  78236.npy  92375.npy\n",
            "07124.npy  21047.npy  35910.npy  49621.npy  63749.npy  78239.npy  92381.npy\n",
            "07125.npy  21048.npy  35912.npy  49630.npy  63758.npy  78243.npy  92385.npy\n",
            "07126.npy  21053.npy  35917.npy  49635.npy  63784.npy  78251.npy  92405.npy\n",
            "07134.npy  21056.npy  35924.npy  49650.npy  63795.npy  78260.npy  92407.npy\n",
            "07135.npy  21059.npy  35942.npy  49652.npy  63798.npy  78293.npy  92408.npy\n",
            "07159.npy  21063.npy  35948.npy  49658.npy  63804.npy  78294.npy  92413.npy\n",
            "07164.npy  21064.npy  35964.npy  49670.npy  63805.npy  78296.npy  92417.npy\n",
            "07192.npy  21065.npy  35967.npy  49671.npy  63812.npy  78302.npy  92430.npy\n",
            "07193.npy  21067.npy  35970.npy  49673.npy  63814.npy  78304.npy  92436.npy\n",
            "07194.npy  21073.npy  35971.npy  49675.npy  63827.npy  78306.npy  92437.npy\n",
            "07198.npy  21075.npy  36014.npy  49680.npy  63845.npy  78314.npy  92450.npy\n",
            "07215.npy  21078.npy  36021.npy  49682.npy  63850.npy  78315.npy  92453.npy\n",
            "07218.npy  21083.npy  36025.npy  49685.npy  63859.npy  78321.npy  92456.npy\n",
            "07219.npy  21086.npy  36027.npy  49687.npy  63871.npy  78325.npy  92457.npy\n",
            "07234.npy  21089.npy  36028.npy  49701.npy  63890.npy  78345.npy  92467.npy\n",
            "07236.npy  21096.npy  36029.npy  49703.npy  63894.npy  78351.npy  92473.npy\n",
            "07238.npy  21305.npy  36048.npy  49705.npy  63902.npy  78365.npy  92478.npy\n",
            "07239.npy  21309.npy  36049.npy  49723.npy  63905.npy  78394.npy  92481.npy\n",
            "07241.npy  21345.npy  36054.npy  49725.npy  63908.npy  78395.npy  92483.npy\n",
            "07248.npy  21346.npy  36058.npy  49726.npy  63910.npy  78401.npy  92501.npy\n",
            "07249.npy  21347.npy  36072.npy  49728.npy  63915.npy  78402.npy  92513.npy\n",
            "07254.npy  21348.npy  36075.npy  49750.npy  63917.npy  78405.npy  92514.npy\n",
            "07258.npy  21354.npy  36079.npy  49752.npy  63920.npy  78406.npy  92534.npy\n",
            "07259.npy  21364.npy  36081.npy  49758.npy  63921.npy  78409.npy  92541.npy\n",
            "07261.npy  21385.npy  36089.npy  49762.npy  63924.npy  78420.npy  92546.npy\n",
            "07265.npy  21386.npy  36092.npy  49785.npy  63941.npy  78429.npy  92547.npy\n",
            "07268.npy  21387.npy  36095.npy  49786.npy  63952.npy  78450.npy  92567.npy\n",
            "07269.npy  21394.npy  36097.npy  49802.npy  63954.npy  78452.npy  92568.npy\n",
            "07285.npy  21403.npy  36098.npy  49805.npy  63957.npy  78459.npy  92571.npy\n",
            "07286.npy  21405.npy  36102.npy  49806.npy  63981.npy  78462.npy  92573.npy\n",
            "07291.npy  21408.npy  36107.npy  49807.npy  63985.npy  78465.npy  92578.npy\n",
            "07293.npy  21435.npy  36109.npy  49810.npy  63987.npy  78469.npy  92581.npy\n",
            "07294.npy  21436.npy  36120.npy  49826.npy  64015.npy  78492.npy  92601.npy\n",
            "07312.npy  21439.npy  36128.npy  49831.npy  64017.npy  78493.npy  92603.npy\n",
            "07314.npy  21450.npy  36152.npy  49832.npy  64025.npy  78509.npy  92608.npy\n",
            "07318.npy  21453.npy  36158.npy  49836.npy  64027.npy  78520.npy  92610.npy\n",
            "07319.npy  21468.npy  36174.npy  49850.npy  64029.npy  78532.npy  92613.npy\n",
            "07325.npy  21479.npy  36175.npy  49856.npy  64038.npy  78534.npy  92614.npy\n",
            "07345.npy  21486.npy  36178.npy  49861.npy  64051.npy  78539.npy  92617.npy\n",
            "07351.npy  21497.npy  36179.npy  49863.npy  64059.npy  78541.npy  92637.npy\n",
            "07354.npy  21504.npy  36184.npy  49867.npy  64071.npy  78560.npy  92641.npy\n",
            "07356.npy  21509.npy  36185.npy  49870.npy  64072.npy  78563.npy  92643.npy\n",
            "07382.npy  21530.npy  36187.npy  50124.npy  64078.npy  78564.npy  92645.npy\n",
            "07385.npy  21534.npy  36189.npy  50126.npy  64087.npy  78591.npy  92654.npy\n",
            "07389.npy  21547.npy  36190.npy  50129.npy  64089.npy  78592.npy  92658.npy\n",
            "07391.npy  21548.npy  36197.npy  50137.npy  64093.npy  78610.npy  92670.npy\n",
            "07392.npy  21560.npy  36201.npy  50142.npy  64097.npy  78612.npy  92671.npy\n",
            "07396.npy  21567.npy  36204.npy  50146.npy  64125.npy  78613.npy  92683.npy\n",
            "07412.npy  21569.npy  36207.npy  50148.npy  64127.npy  78619.npy  92705.npy\n",
            "07413.npy  21583.npy  36210.npy  50149.npy  64128.npy  78623.npy  92708.npy\n",
            "07419.npy  21589.npy  36214.npy  50162.npy  64130.npy  78624.npy  92710.npy\n",
            "07429.npy  21593.npy  36240.npy  50163.npy  64137.npy  78631.npy  92718.npy\n",
            "07431.npy  21598.npy  36248.npy  50167.npy  64138.npy  78634.npy  92734.npy\n",
            "07436.npy  21604.npy  36254.npy  50176.npy  64153.npy  78639.npy  92735.npy\n",
            "07438.npy  21605.npy  36281.npy  50184.npy  64157.npy  78642.npy  92738.npy\n",
            "07451.npy  21607.npy  36294.npy  50186.npy  64172.npy  78650.npy  92741.npy\n",
            "07452.npy  21609.npy  36401.npy  50187.npy  64175.npy  78659.npy  92743.npy\n",
            "07456.npy  21637.npy  36415.npy  50193.npy  64190.npy  78693.npy  92750.npy\n",
            "07458.npy  21640.npy  36417.npy  50198.npy  64193.npy  78902.npy  92768.npy\n",
            "07463.npy  21643.npy  36418.npy  50236.npy  64197.npy  78903.npy  92780.npy\n",
            "07465.npy  21647.npy  36420.npy  50237.npy  64198.npy  78904.npy  92783.npy\n",
            "07469.npy  21648.npy  36421.npy  50239.npy  64207.npy  78906.npy  92785.npy\n",
            "07481.npy  21650.npy  36429.npy  50241.npy  64215.npy  78913.npy  92806.npy\n",
            "07486.npy  21653.npy  36450.npy  50246.npy  64273.npy  78914.npy  92814.npy\n",
            "07496.npy  21659.npy  36452.npy  50248.npy  64278.npy  78915.npy  92817.npy\n",
            "07516.npy  21674.npy  36458.npy  50261.npy  64279.npy  78921.npy  92831.npy\n",
            "07521.npy  21693.npy  36459.npy  50263.npy  64280.npy  78923.npy  92835.npy\n",
            "07523.npy  21698.npy  36470.npy  50271.npy  64283.npy  78924.npy  92836.npy\n",
            "07528.npy  21704.npy  36472.npy  50273.npy  64285.npy  78925.npy  92837.npy\n",
            "07539.npy  21706.npy  36478.npy  50278.npy  64287.npy  78930.npy  92845.npy\n",
            "07561.npy  21730.npy  36480.npy  50286.npy  64291.npy  78931.npy  92846.npy\n",
            "07562.npy  21739.npy  36481.npy  50289.npy  64293.npy  78936.npy  92847.npy\n",
            "07582.npy  21740.npy  36497.npy  50293.npy  64297.npy  78956.npy  92851.npy\n",
            "07591.npy  21746.npy  36498.npy  50312.npy  64301.npy  78960.npy  92854.npy\n",
            "07592.npy  21748.npy  36501.npy  50317.npy  64309.npy  78964.npy  92867.npy\n",
            "07594.npy  21749.npy  36508.npy  50327.npy  64312.npy  78965.npy  92870.npy\n",
            "07596.npy  21753.npy  36521.npy  50328.npy  64318.npy  79014.npy  92874.npy\n",
            "07612.npy  21769.npy  36524.npy  50341.npy  64320.npy  79018.npy  92876.npy\n",
            "07615.npy  21780.npy  36540.npy  50346.npy  64321.npy  79024.npy  93016.npy\n",
            "07618.npy  21804.npy  36541.npy  50348.npy  64325.npy  79031.npy  93024.npy\n",
            "07623.npy  21809.npy  36542.npy  50361.npy  64327.npy  79032.npy  93027.npy\n",
            "07628.npy  21835.npy  36570.npy  50368.npy  64329.npy  79036.npy  93041.npy\n",
            "07632.npy  21836.npy  36571.npy  50371.npy  64350.npy  79042.npy  93042.npy\n",
            "07634.npy  21849.npy  36578.npy  50372.npy  64359.npy  79043.npy  93046.npy\n",
            "07645.npy  21850.npy  36581.npy  50379.npy  64385.npy  79045.npy  93051.npy\n",
            "07649.npy  21853.npy  36590.npy  50386.npy  64390.npy  79053.npy  93057.npy\n",
            "07651.npy  21859.npy  36597.npy  50397.npy  64391.npy  79054.npy  93068.npy\n",
            "07652.npy  21863.npy  36598.npy  50398.npy  64510.npy  79062.npy  93071.npy\n",
            "07653.npy  21869.npy  36710.npy  50413.npy  64513.npy  79068.npy  93072.npy\n",
            "07658.npy  21890.npy  36724.npy  50427.npy  64520.npy  79081.npy  93076.npy\n",
            "07659.npy  21903.npy  36725.npy  50428.npy  64523.npy  79084.npy  93081.npy\n",
            "07685.npy  21904.npy  36729.npy  50438.npy  64531.npy  79085.npy  93082.npy\n",
            "07689.npy  21908.npy  36741.npy  50461.npy  64532.npy  79103.npy  93084.npy\n",
            "07692.npy  21930.npy  36748.npy  50462.npy  64571.npy  79104.npy  93085.npy\n",
            "07693.npy  21936.npy  36749.npy  50467.npy  64590.npy  79105.npy  93086.npy\n",
            "07694.npy  21943.npy  36751.npy  50471.npy  64592.npy  79124.npy  93087.npy\n",
            "07698.npy  21946.npy  36780.npy  50472.npy  64593.npy  79125.npy  93102.npy\n",
            "07824.npy  21948.npy  36781.npy  50473.npy  64701.npy  79128.npy  93124.npy\n",
            "07825.npy  21950.npy  36789.npy  50482.npy  64705.npy  79134.npy  93126.npy\n",
            "07826.npy  21957.npy  36804.npy  50483.npy  64708.npy  79138.npy  93127.npy\n",
            "07834.npy  21963.npy  36805.npy  50486.npy  64709.npy  79140.npy  93128.npy\n",
            "07836.npy  21973.npy  36810.npy  50487.npy  64720.npy  79146.npy  93142.npy\n",
            "07839.npy  21978.npy  36812.npy  50489.npy  64721.npy  79162.npy  93145.npy\n",
            "07849.npy  21986.npy  36814.npy  50491.npy  64723.npy  79165.npy  93146.npy\n",
            "07851.npy  23014.npy  36821.npy  50492.npy  64730.npy  79168.npy  93147.npy\n",
            "07852.npy  23018.npy  36840.npy  50614.npy  64735.npy  79182.npy  93148.npy\n",
            "07853.npy  23045.npy  36842.npy  50617.npy  64752.npy  79185.npy  93152.npy\n",
            "07865.npy  23046.npy  36845.npy  50619.npy  64753.npy  79205.npy  93164.npy\n",
            "07893.npy  23047.npy  36850.npy  50624.npy  64758.npy  79234.npy  93165.npy\n",
            "07895.npy  23048.npy  36870.npy  50643.npy  64780.npy  79243.npy  93168.npy\n",
            "07912.npy  23054.npy  36872.npy  50649.npy  64781.npy  79250.npy  93172.npy\n",
            "07915.npy  23058.npy  36875.npy  50674.npy  64789.npy  79258.npy  93174.npy\n",
            "07916.npy  23059.npy  36892.npy  50679.npy  64803.npy  79280.npy  93184.npy\n",
            "07924.npy  23061.npy  36895.npy  50682.npy  64805.npy  79285.npy  93186.npy\n",
            "07926.npy  23065.npy  36897.npy  50683.npy  64813.npy  79286.npy  93187.npy\n",
            "07931.npy  23068.npy  36910.npy  50684.npy  64820.npy  79302.npy  93208.npy\n",
            "07935.npy  23071.npy  36915.npy  50689.npy  64821.npy  79305.npy  93215.npy\n",
            "07956.npy  23074.npy  36920.npy  50723.npy  64825.npy  79312.npy  93216.npy\n",
            "07965.npy  23075.npy  36924.npy  50732.npy  64839.npy  79314.npy  93245.npy\n",
            "07968.npy  23076.npy  36928.npy  50734.npy  64853.npy  79315.npy  93246.npy\n",
            "07983.npy  23091.npy  36945.npy  50738.npy  64870.npy  79316.npy  93247.npy\n",
            "07984.npy  23094.npy  36947.npy  50739.npy  64890.npy  79321.npy  93251.npy\n",
            "08125.npy  23096.npy  36954.npy  50741.npy  64891.npy  79325.npy  93260.npy\n",
            "08126.npy  23106.npy  36957.npy  50743.npy  64901.npy  79328.npy  93261.npy\n",
            "08134.npy  23109.npy  36972.npy  50746.npy  64902.npy  79346.npy  93268.npy\n",
            "08137.npy  23140.npy  36974.npy  50748.npy  64905.npy  79348.npy  93271.npy\n",
            "08146.npy  23154.npy  36980.npy  50764.npy  64907.npy  79351.npy  93275.npy\n",
            "08147.npy  23158.npy  36981.npy  50768.npy  64910.npy  79352.npy  93280.npy\n",
            "08162.npy  23159.npy  36982.npy  50783.npy  64913.npy  79358.npy  93286.npy\n",
            "08163.npy  23164.npy  36985.npy  50784.npy  64918.npy  79368.npy  93287.npy\n",
            "08164.npy  23174.npy  37018.npy  50789.npy  64925.npy  79384.npy  93401.npy\n",
            "08172.npy  23175.npy  37021.npy  50793.npy  64935.npy  79385.npy  93405.npy\n",
            "08173.npy  23178.npy  37024.npy  50813.npy  64957.npy  79386.npy  93410.npy\n",
            "08175.npy  23187.npy  37042.npy  50817.npy  64958.npy  79405.npy  93416.npy\n",
            "08176.npy  23197.npy  37049.npy  50823.npy  64978.npy  79406.npy  93426.npy\n",
            "08196.npy  23198.npy  37051.npy  50824.npy  64982.npy  79423.npy  93428.npy\n",
            "08219.npy  23401.npy  37052.npy  50834.npy  65012.npy  79425.npy  93462.npy\n",
            "08234.npy  23407.npy  37054.npy  50839.npy  65017.npy  79426.npy  93468.npy\n",
            "08241.npy  23408.npy  37058.npy  50841.npy  65019.npy  79428.npy  93471.npy\n",
            "08243.npy  23415.npy  37059.npy  50861.npy  65024.npy  79451.npy  93481.npy\n",
            "08251.npy  23416.npy  37084.npy  50867.npy  65037.npy  79452.npy  93487.npy\n",
            "08254.npy  23419.npy  37091.npy  50871.npy  65041.npy  79461.npy  93502.npy\n",
            "08259.npy  23450.npy  37092.npy  50873.npy  65043.npy  79463.npy  93506.npy\n",
            "08261.npy  23456.npy  37096.npy  50876.npy  65071.npy  79465.npy  93512.npy\n",
            "08267.npy  23459.npy  37105.npy  50879.npy  65073.npy  79485.npy  93521.npy\n",
            "08275.npy  23467.npy  37128.npy  50894.npy  65078.npy  79512.npy  93524.npy\n",
            "08276.npy  23476.npy  37129.npy  50918.npy  65087.npy  79516.npy  93528.npy\n",
            "08291.npy  23478.npy  37140.npy  50927.npy  65092.npy  79531.npy  93541.npy\n",
            "08294.npy  23485.npy  37145.npy  50931.npy  65093.npy  79562.npy  93542.npy\n",
            "08295.npy  23498.npy  37146.npy  50934.npy  65094.npy  79568.npy  93547.npy\n",
            "08297.npy  23501.npy  37160.npy  50937.npy  65098.npy  79581.npy  93548.npy\n",
            "08317.npy  23504.npy  37180.npy  50938.npy  65102.npy  79584.npy  93568.npy\n",
            "08345.npy  23510.npy  37182.npy  50942.npy  65103.npy  79603.npy  93572.npy\n",
            "08349.npy  23514.npy  37184.npy  50961.npy  65108.npy  79605.npy  93578.npy\n",
            "08357.npy  23519.npy  37185.npy  50962.npy  65124.npy  79608.npy  93586.npy\n",
            "08367.npy  23564.npy  37186.npy  50964.npy  65129.npy  79610.npy  93605.npy\n",
            "08369.npy  23570.npy  37190.npy  50984.npy  65139.npy  79612.npy  93610.npy\n",
            "08372.npy  23576.npy  37198.npy  51023.npy  65140.npy  79613.npy  93614.npy\n",
            "08375.npy  23578.npy  37204.npy  51029.npy  65147.npy  79614.npy  93615.npy\n",
            "08376.npy  23581.npy  37208.npy  51032.npy  65148.npy  79615.npy  93618.npy\n",
            "08395.npy  23615.npy  37214.npy  51037.npy  65149.npy  79623.npy  93620.npy\n",
            "08415.npy  23619.npy  37250.npy  51046.npy  65178.npy  79624.npy  93647.npy\n",
            "08423.npy  23645.npy  37251.npy  51049.npy  65179.npy  79638.npy  93648.npy\n",
            "08439.npy  23648.npy  37254.npy  51073.npy  65183.npy  79652.npy  93650.npy\n",
            "08451.npy  23657.npy  37256.npy  51076.npy  65187.npy  79658.npy  93658.npy\n",
            "08452.npy  23681.npy  37259.npy  51086.npy  65189.npy  79680.npy  93671.npy\n",
            "08462.npy  23690.npy  37260.npy  51089.npy  65201.npy  79681.npy  93685.npy\n",
            "08469.npy  23694.npy  37265.npy  51092.npy  65203.npy  79682.npy  93687.npy\n",
            "08471.npy  23698.npy  37284.npy  51094.npy  65204.npy  79803.npy  93702.npy\n",
            "08476.npy  23705.npy  37285.npy  51208.npy  65207.npy  79806.npy  93718.npy\n",
            "08495.npy  23715.npy  37289.npy  51209.npy  65213.npy  79815.npy  93725.npy\n",
            "08516.npy  23745.npy  37294.npy  51237.npy  65217.npy  79823.npy  93728.npy\n",
            "08517.npy  23746.npy  37295.npy  51243.npy  65231.npy  79824.npy  93740.npy\n",
            "08523.npy  23748.npy  37296.npy  51248.npy  65234.npy  79825.npy  93741.npy\n",
            "08524.npy  23751.npy  37298.npy  51249.npy  65237.npy  79830.npy  93750.npy\n",
            "08531.npy  23754.npy  37402.npy  51263.npy  65238.npy  79832.npy  93761.npy\n",
            "08534.npy  23759.npy  37405.npy  51269.npy  65240.npy  79834.npy  93764.npy\n",
            "08537.npy  23760.npy  37408.npy  51270.npy  65248.npy  79843.npy  93786.npy\n",
            "08541.npy  23761.npy  37416.npy  51278.npy  65273.npy  79845.npy  93820.npy\n",
            "08546.npy  23785.npy  37419.npy  51283.npy  65274.npy  79846.npy  93826.npy\n",
            "08563.npy  23790.npy  37420.npy  51284.npy  65283.npy  79850.npy  93827.npy\n",
            "08564.npy  23794.npy  37425.npy  51293.npy  65289.npy  79853.npy  93841.npy\n",
            "08567.npy  23795.npy  37426.npy  51304.npy  65297.npy  79854.npy  93852.npy\n",
            "08569.npy  23806.npy  37450.npy  51306.npy  65298.npy  79856.npy  93854.npy\n",
            "08571.npy  23807.npy  37451.npy  51309.npy  65301.npy  79861.npy  93857.npy\n",
            "08591.npy  23810.npy  37459.npy  51348.npy  65308.npy  79864.npy  93861.npy\n",
            "08593.npy  23815.npy  37465.npy  51349.npy  65312.npy  79865.npy  93867.npy\n",
            "08597.npy  23817.npy  37491.npy  51360.npy  65319.npy  80123.npy  93874.npy\n",
            "08613.npy  23841.npy  37498.npy  51367.npy  65320.npy  80126.npy  93875.npy\n",
            "08617.npy  23845.npy  37502.npy  51369.npy  65328.npy  80134.npy  94013.npy\n",
            "08621.npy  23849.npy  37504.npy  51376.npy  65341.npy  80152.npy  94015.npy\n",
            "08624.npy  23851.npy  37508.npy  51387.npy  65342.npy  80153.npy  94017.npy\n",
            "08627.npy  23854.npy  37509.npy  51390.npy  65349.npy  80154.npy  94018.npy\n",
            "08632.npy  23859.npy  37542.npy  51397.npy  65371.npy  80156.npy  94021.npy\n",
            "08637.npy  23864.npy  37546.npy  51403.npy  65378.npy  80162.npy  94026.npy\n",
            "08641.npy  23875.npy  37548.npy  51407.npy  65380.npy  80165.npy  94031.npy\n",
            "08645.npy  23897.npy  37560.npy  51437.npy  65389.npy  80169.npy  94062.npy\n",
            "08649.npy  23901.npy  37569.npy  51460.npy  65392.npy  80172.npy  94065.npy\n",
            "08652.npy  23905.npy  37580.npy  51462.npy  65403.npy  80173.npy  94067.npy\n",
            "08653.npy  23907.npy  37592.npy  51469.npy  65407.npy  80174.npy  94068.npy\n",
            "08654.npy  23908.npy  37601.npy  51472.npy  65413.npy  80179.npy  94071.npy\n",
            "08657.npy  23910.npy  37609.npy  51473.npy  65429.npy  80193.npy  94078.npy\n",
            "08671.npy  23940.npy  37610.npy  51476.npy  65432.npy  80195.npy  94081.npy\n",
            "08674.npy  23954.npy  37615.npy  51480.npy  65473.npy  80196.npy  94083.npy\n",
            "08691.npy  23957.npy  37619.npy  51482.npy  65478.npy  80197.npy  94085.npy\n",
            "08693.npy  23961.npy  37620.npy  51489.npy  65491.npy  80214.npy  94086.npy\n",
            "08695.npy  23968.npy  37621.npy  51493.npy  65703.npy  80217.npy  94087.npy\n",
            "08716.npy  23971.npy  37628.npy  51496.npy  65704.npy  80231.npy  94106.npy\n",
            "08719.npy  24019.npy  37629.npy  51497.npy  65708.npy  80237.npy  94107.npy\n",
            "08732.npy  24038.npy  37641.npy  51498.npy  65712.npy  80239.npy  94108.npy\n",
            "08741.npy  24039.npy  37642.npy  51602.npy  65728.npy  80241.npy  94130.npy\n",
            "08742.npy  24051.npy  37649.npy  51607.npy  65730.npy  80243.npy  94132.npy\n",
            "08743.npy  24061.npy  37658.npy  51608.npy  65732.npy  80246.npy  94138.npy\n",
            "08761.npy  24067.npy  37681.npy  51624.npy  65734.npy  80247.npy  94150.npy\n",
            "08793.npy  24069.npy  37692.npy  51627.npy  65739.npy  80251.npy  94152.npy\n",
            "08795.npy  24083.npy  37802.npy  51628.npy  65740.npy  80254.npy  94153.npy\n",
            "08917.npy  24095.npy  37806.npy  51629.npy  65741.npy  80256.npy  94156.npy\n",
            "08924.npy  24098.npy  37809.npy  51630.npy  65743.npy  80259.npy  94158.npy\n",
            "08934.npy  24106.npy  37814.npy  51647.npy  65782.npy  80265.npy  94162.npy\n",
            "08937.npy  24108.npy  37815.npy  51679.npy  65789.npy  80269.npy  94170.npy\n",
            "08941.npy  24109.npy  37819.npy  51680.npy  65794.npy  80276.npy  94175.npy\n",
            "08954.npy  24130.npy  37825.npy  51682.npy  65801.npy  80279.npy  94176.npy\n",
            "08957.npy  24135.npy  37829.npy  51683.npy  65804.npy  80297.npy  94178.npy\n",
            "08961.npy  24157.npy  37845.npy  51687.npy  65807.npy  80316.npy  94180.npy\n",
            "08962.npy  24165.npy  37859.npy  51693.npy  65809.npy  80317.npy  94185.npy\n",
            "08964.npy  24178.npy  37860.npy  51694.npy  65814.npy  80319.npy  94205.npy\n",
            "09124.npy  24189.npy  37862.npy  51697.npy  65817.npy  80321.npy  94216.npy\n",
            "09126.npy  24197.npy  37864.npy  51706.npy  65819.npy  80327.npy  94230.npy\n",
            "09128.npy  24309.npy  37865.npy  51708.npy  65824.npy  80341.npy  94236.npy\n",
            "09132.npy  24316.npy  37895.npy  51726.npy  65829.npy  80347.npy  94237.npy\n",
            "09138.npy  24317.npy  37901.npy  51730.npy  65832.npy  80356.npy  94251.npy\n",
            "09148.npy  24319.npy  37902.npy  51734.npy  65843.npy  80361.npy  94256.npy\n",
            "09152.npy  24351.npy  37904.npy  51736.npy  65871.npy  80365.npy  94267.npy\n",
            "09154.npy  24356.npy  37915.npy  51746.npy  65892.npy  80379.npy  94278.npy\n",
            "09156.npy  24357.npy  37918.npy  51763.npy  65904.npy  80392.npy  94281.npy\n",
            "09162.npy  24368.npy  37921.npy  51768.npy  65908.npy  80395.npy  94287.npy\n",
            "09176.npy  24376.npy  37924.npy  51783.npy  65914.npy  80397.npy  94302.npy\n",
            "09184.npy  24380.npy  37928.npy  51789.npy  65917.npy  80425.npy  94305.npy\n",
            "09185.npy  24389.npy  37945.npy  51802.npy  65930.npy  80426.npy  94325.npy\n",
            "09187.npy  24390.npy  37948.npy  51804.npy  65931.npy  80436.npy  94350.npy\n",
            "09213.npy  24395.npy  37951.npy  51807.npy  65940.npy  80451.npy  94351.npy\n",
            "09215.npy  24396.npy  37965.npy  51809.npy  65948.npy  80453.npy  94352.npy\n",
            "09217.npy  24397.npy  37980.npy  51836.npy  65978.npy  80463.npy  94356.npy\n",
            "09238.npy  24501.npy  37984.npy  51839.npy  65980.npy  80473.npy  94357.npy\n",
            "09241.npy  24519.npy  38019.npy  51846.npy  65981.npy  80512.npy  94358.npy\n",
            "09247.npy  24537.npy  38029.npy  51863.npy  67014.npy  80513.npy  94360.npy\n",
            "09248.npy  24538.npy  38041.npy  51864.npy  67021.npy  80521.npy  94361.npy\n",
            "09251.npy  24567.npy  38045.npy  51870.npy  67024.npy  80532.npy  94370.npy\n",
            "09263.npy  24570.npy  38046.npy  51890.npy  67029.npy  80534.npy  94372.npy\n",
            "09265.npy  24573.npy  38047.npy  51892.npy  67031.npy  80537.npy  94378.npy\n",
            "09267.npy  24576.npy  38051.npy  51894.npy  67034.npy  80539.npy  94380.npy\n",
            "09268.npy  24579.npy  38054.npy  51903.npy  67041.npy  80567.npy  94387.npy\n",
            "09273.npy  24580.npy  38057.npy  51924.npy  67043.npy  80576.npy  94501.npy\n",
            "09283.npy  24581.npy  38069.npy  51927.npy  67048.npy  80591.npy  94506.npy\n",
            "09284.npy  24586.npy  38071.npy  51928.npy  67051.npy  80593.npy  94507.npy\n",
            "09285.npy  24593.npy  38072.npy  51930.npy  67054.npy  80597.npy  94508.npy\n",
            "09286.npy  24603.npy  38076.npy  51943.npy  67058.npy  80612.npy  94512.npy\n",
            "09312.npy  24607.npy  38094.npy  51948.npy  67059.npy  80613.npy  94517.npy\n",
            "09315.npy  24630.npy  38095.npy  51964.npy  67082.npy  80614.npy  94518.npy\n",
            "09321.npy  24631.npy  38105.npy  51968.npy  67084.npy  80615.npy  94523.npy\n",
            "09324.npy  24650.npy  38109.npy  51972.npy  67091.npy  80625.npy  94532.npy\n",
            "09326.npy  24651.npy  38127.npy  51973.npy  67092.npy  80627.npy  94560.npy\n",
            "09347.npy  24653.npy  38129.npy  51978.npy  67103.npy  80629.npy  94563.npy\n",
            "09352.npy  24658.npy  38145.npy  52018.npy  67109.npy  80649.npy  94571.npy\n",
            "09357.npy  24687.npy  38147.npy  52019.npy  67124.npy  80652.npy  94578.npy\n",
            "09364.npy  24689.npy  38154.npy  52031.npy  67125.npy  80653.npy  94580.npy\n",
            "09368.npy  24698.npy  38156.npy  52034.npy  67129.npy  80672.npy  94581.npy\n",
            "09374.npy  24713.npy  38157.npy  52036.npy  67130.npy  80713.npy  94586.npy\n",
            "09376.npy  24730.npy  38159.npy  52037.npy  67132.npy  80714.npy  94602.npy\n",
            "09382.npy  24738.npy  38162.npy  52049.npy  67138.npy  80724.npy  94605.npy\n",
            "09384.npy  24751.npy  38164.npy  52068.npy  67142.npy  80734.npy  94610.npy\n",
            "09385.npy  24756.npy  38170.npy  52071.npy  67143.npy  80735.npy  94615.npy\n",
            "09387.npy  24765.npy  38179.npy  52078.npy  67148.npy  80736.npy  94617.npy\n",
            "09413.npy  24769.npy  38190.npy  52079.npy  67149.npy  80742.npy  94620.npy\n",
            "09423.npy  24781.npy  38196.npy  52089.npy  67159.npy  80759.npy  94625.npy\n",
            "09426.npy  24783.npy  38201.npy  52091.npy  67180.npy  80764.npy  94628.npy\n",
            "09432.npy  24791.npy  38209.npy  52096.npy  67183.npy  80765.npy  94638.npy\n",
            "09457.npy  24806.npy  38210.npy  52097.npy  67190.npy  80769.npy  94650.npy\n",
            "09462.npy  24810.npy  38215.npy  52104.npy  67193.npy  80792.npy  94675.npy\n",
            "09465.npy  24815.npy  38217.npy  52106.npy  67194.npy  80912.npy  94678.npy\n",
            "09467.npy  24835.npy  38245.npy  52108.npy  67195.npy  80914.npy  94687.npy\n",
            "09468.npy  24836.npy  38246.npy  52130.npy  67198.npy  80916.npy  94710.npy\n",
            "09478.npy  24851.npy  38251.npy  52139.npy  67201.npy  80924.npy  94720.npy\n",
            "09482.npy  24853.npy  38259.npy  52140.npy  67205.npy  80925.npy  94721.npy\n",
            "09486.npy  24857.npy  38271.npy  52164.npy  67208.npy  80926.npy  94723.npy\n",
            "09513.npy  24859.npy  38401.npy  52168.npy  67214.npy  80927.npy  94725.npy\n",
            "09514.npy  24860.npy  38402.npy  52183.npy  67219.npy  80932.npy  94731.npy\n",
            "09516.npy  24861.npy  38409.npy  52301.npy  67241.npy  80935.npy  94732.npy\n",
            "09518.npy  24865.npy  38410.npy  52304.npy  67243.npy  80941.npy  94738.npy\n",
            "09523.npy  24870.npy  38416.npy  52316.npy  67250.npy  80942.npy  94780.npy\n",
            "09527.npy  24875.npy  38417.npy  52318.npy  67254.npy  80943.npy  94783.npy\n",
            "09531.npy  24876.npy  38419.npy  52340.npy  67259.npy  80945.npy  94786.npy\n",
            "09547.npy  24891.npy  38427.npy  52341.npy  67283.npy  80947.npy  94802.npy\n",
            "09561.npy  24905.npy  38460.npy  52349.npy  67298.npy  80954.npy  94813.npy\n",
            "09563.npy  24907.npy  38461.npy  52361.npy  67309.npy  80957.npy  94815.npy\n",
            "09572.npy  24918.npy  38465.npy  52379.npy  67312.npy  80965.npy  94823.npy\n",
            "09587.npy  24931.npy  38469.npy  52386.npy  67318.npy  80967.npy  94836.npy\n",
            "09615.npy  24958.npy  38475.npy  52394.npy  67319.npy  80971.npy  94850.npy\n",
            "09617.npy  24967.npy  38490.npy  52401.npy  67325.npy  80972.npy  94860.npy\n",
            "09618.npy  24971.npy  38491.npy  52406.npy  67328.npy  80974.npy  94870.npy\n",
            "09623.npy  24973.npy  38495.npy  52407.npy  67342.npy  80976.npy  94871.npy\n",
            "09624.npy  24975.npy  38496.npy  52416.npy  67348.npy  81026.npy  94872.npy\n",
            "09634.npy  24980.npy  38509.npy  52431.npy  67350.npy  81027.npy  94873.npy\n",
            "09638.npy  24986.npy  38514.npy  52437.npy  67352.npy  81035.npy  94876.npy\n",
            "09641.npy  25013.npy  38516.npy  52439.npy  67354.npy  81036.npy  95021.npy\n",
            "09642.npy  25017.npy  38527.npy  52469.npy  67384.npy  81043.npy  95024.npy\n",
            "09648.npy  25037.npy  38529.npy  52473.npy  67385.npy  81047.npy  95031.npy\n",
            "09657.npy  25061.npy  38546.npy  52476.npy  67394.npy  81054.npy  95034.npy\n",
            "09675.npy  25063.npy  38547.npy  52479.npy  67398.npy  81059.npy  95038.npy\n",
            "09682.npy  25067.npy  38549.npy  52483.npy  67402.npy  81063.npy  95048.npy\n",
            "09687.npy  25068.npy  38574.npy  52486.npy  67413.npy  81064.npy  95062.npy\n",
            "09713.npy  25071.npy  38590.npy  52487.npy  67415.npy  81069.npy  95064.npy\n",
            "09715.npy  25079.npy  38609.npy  52490.npy  67420.npy  81075.npy  95067.npy\n",
            "09716.npy  25093.npy  38612.npy  52496.npy  67431.npy  81079.npy  95071.npy\n",
            "09718.npy  25096.npy  38621.npy  52603.npy  67435.npy  81092.npy  95072.npy\n",
            "09723.npy  25097.npy  38624.npy  52607.npy  67438.npy  81093.npy  95078.npy\n",
            "09731.npy  25103.npy  38625.npy  52609.npy  67439.npy  81094.npy  95081.npy\n",
            "09738.npy  25107.npy  38629.npy  52610.npy  67481.npy  81095.npy  95086.npy\n",
            "09756.npy  25109.npy  38641.npy  52613.npy  67485.npy  81096.npy  95087.npy\n",
            "09765.npy  25137.npy  38647.npy  52614.npy  67491.npy  81097.npy  95102.npy\n",
            "09768.npy  25149.npy  38654.npy  52617.npy  67492.npy  81203.npy  95103.npy\n",
            "09781.npy  25164.npy  38674.npy  52619.npy  67498.npy  81206.npy  95108.npy\n",
            "09785.npy  25168.npy  38697.npy  52631.npy  67504.npy  81207.npy  95130.npy\n",
            "09814.npy  25170.npy  38701.npy  52634.npy  67512.npy  81243.npy  95163.npy\n",
            "09821.npy  25174.npy  38702.npy  52640.npy  67521.npy  81245.npy  95173.npy\n",
            "09823.npy  25178.npy  38704.npy  52641.npy  67528.npy  81249.npy  95174.npy\n",
            "09824.npy  25179.npy  38706.npy  52649.npy  67531.npy  81250.npy  95176.npy\n",
            "09827.npy  25180.npy  38712.npy  52670.npy  67541.npy  81254.npy  95184.npy\n",
            "09831.npy  25189.npy  38714.npy  52681.npy  67543.npy  81256.npy  95186.npy\n",
            "09834.npy  25193.npy  38720.npy  52687.npy  67548.npy  81257.npy  95201.npy\n",
            "09841.npy  25197.npy  38724.npy  52691.npy  67580.npy  81259.npy  95208.npy\n",
            "09843.npy  25198.npy  38741.npy  52698.npy  67581.npy  81260.npy  95214.npy\n",
            "09851.npy  25310.npy  38742.npy  52706.npy  67582.npy  81263.npy  95247.npy\n",
            "09852.npy  25314.npy  38752.npy  52708.npy  67593.npy  81265.npy  95260.npy\n",
            "09863.npy  25318.npy  38754.npy  52710.npy  67801.npy  81273.npy  95263.npy\n",
            "09867.npy  25340.npy  38756.npy  52714.npy  67802.npy  81279.npy  95264.npy\n",
            "09875.npy  25341.npy  38761.npy  52716.npy  67809.npy  81293.npy  95267.npy\n",
            "10234.npy  25361.npy  38764.npy  52719.npy  67810.npy  81294.npy  95274.npy\n",
            "10236.npy  25367.npy  38765.npy  52738.npy  67823.npy  81296.npy  95278.npy\n",
            "10238.npy  25368.npy  38790.npy  52743.npy  67831.npy  81304.npy  95281.npy\n",
            "10246.npy  25369.npy  38910.npy  52746.npy  67835.npy  81305.npy  95283.npy\n",
            "10254.npy  25370.npy  38912.npy  52748.npy  67841.npy  81309.npy  95302.npy\n",
            "10256.npy  25376.npy  38914.npy  52761.npy  67842.npy  81320.npy  95306.npy\n",
            "10258.npy  25381.npy  38927.npy  52768.npy  67843.npy  81349.npy  95317.npy\n",
            "10259.npy  25384.npy  38942.npy  52780.npy  67849.npy  81359.npy  95326.npy\n",
            "10263.npy  25386.npy  38954.npy  52783.npy  67852.npy  81365.npy  95341.npy\n",
            "10264.npy  25397.npy  38956.npy  52790.npy  67859.npy  81370.npy  95371.npy\n",
            "10268.npy  25401.npy  38961.npy  52796.npy  67892.npy  81374.npy  95376.npy\n",
            "10269.npy  25416.npy  38962.npy  52801.npy  67894.npy  81390.npy  95380.npy\n",
            "10274.npy  25436.npy  38965.npy  52803.npy  67904.npy  81394.npy  95387.npy\n",
            "10278.npy  25438.npy  38976.npy  52816.npy  67913.npy  81395.npy  95401.npy\n",
            "10283.npy  25461.npy  39012.npy  52819.npy  67915.npy  81403.npy  95402.npy\n",
            "10285.npy  25463.npy  39018.npy  52837.npy  67924.npy  81409.npy  95403.npy\n",
            "10287.npy  25467.npy  39028.npy  52839.npy  67935.npy  81423.npy  95417.npy\n",
            "10289.npy  25468.npy  39041.npy  52847.npy  67938.npy  81436.npy  95426.npy\n",
            "10325.npy  25473.npy  39051.npy  52863.npy  67943.npy  81450.npy  95427.npy\n",
            "10328.npy  25478.npy  39054.npy  52864.npy  67948.npy  81457.npy  95467.npy\n",
            "10329.npy  25481.npy  39056.npy  52867.npy  67951.npy  81467.npy  95478.npy\n",
            "10345.npy  25483.npy  39058.npy  52870.npy  67953.npy  81497.npy  95482.npy\n",
            "10357.npy  25489.npy  39061.npy  52874.npy  67980.npy  81509.npy  95487.npy\n",
            "10358.npy  25493.npy  39067.npy  52890.npy  67983.npy  81523.npy  95604.npy\n",
            "10362.npy  25497.npy  39076.npy  52903.npy  67984.npy  81527.npy  95613.npy\n",
            "10372.npy  25498.npy  39085.npy  52904.npy  68019.npy  81529.npy  95640.npy\n",
            "10384.npy  25610.npy  39102.npy  52910.npy  68025.npy  81532.npy  95672.npy\n",
            "10385.npy  25614.npy  39106.npy  52914.npy  68034.npy  81536.npy  95680.npy\n",
            "10386.npy  25634.npy  39107.npy  52918.npy  68039.npy  81540.npy  95683.npy\n",
            "10389.npy  25639.npy  39120.npy  52931.npy  68042.npy  81546.npy  95684.npy\n",
            "10395.npy  25640.npy  39127.npy  52936.npy  68043.npy  81576.npy  95687.npy\n",
            "10396.npy  25643.npy  39145.npy  52938.npy  68079.npy  81579.npy  95712.npy\n",
            "10398.npy  25683.npy  39148.npy  52947.npy  68094.npy  81602.npy  95714.npy\n",
            "10423.npy  25687.npy  39156.npy  52948.npy  68120.npy  81603.npy  95718.npy\n",
            "10436.npy  25690.npy  39158.npy  52960.npy  68127.npy  81624.npy  95720.npy\n",
            "10453.npy  25693.npy  39164.npy  52964.npy  68137.npy  81625.npy  95723.npy\n",
            "10458.npy  25706.npy  39168.npy  52970.npy  68145.npy  81630.npy  95724.npy\n",
            "10459.npy  25709.npy  39170.npy  52971.npy  68154.npy  81645.npy  95726.npy\n",
            "10462.npy  25713.npy  39175.npy  52974.npy  68159.npy  81673.npy  95742.npy\n",
            "10463.npy  25716.npy  39182.npy  52978.npy  68172.npy  81675.npy  95763.npy\n",
            "10465.npy  25718.npy  39185.npy  52983.npy  68179.npy  81679.npy  95764.npy\n",
            "10475.npy  25719.npy  39201.npy  52986.npy  68190.npy  81690.npy  95780.npy\n",
            "10476.npy  25731.npy  39206.npy  53012.npy  68192.npy  81692.npy  95783.npy\n",
            "10478.npy  25748.npy  39214.npy  53017.npy  68193.npy  81705.npy  95801.npy\n",
            "10479.npy  25749.npy  39241.npy  53027.npy  68194.npy  81706.npy  95803.npy\n",
            "10482.npy  25760.npy  39247.npy  53046.npy  68201.npy  81720.npy  95810.npy\n",
            "10483.npy  25768.npy  39250.npy  53048.npy  68204.npy  81724.npy  95812.npy\n",
            "10489.npy  25780.npy  39256.npy  53062.npy  68213.npy  81734.npy  95814.npy\n",
            "10492.npy  25781.npy  39265.npy  53064.npy  68230.npy  81752.npy  95817.npy\n",
            "10493.npy  25783.npy  39280.npy  53068.npy  68231.npy  81759.npy  95821.npy\n",
            "10524.npy  25784.npy  39281.npy  53084.npy  68245.npy  81760.npy  95830.npy\n",
            "10527.npy  25791.npy  39285.npy  53089.npy  68249.npy  81764.npy  95831.npy\n",
            "10528.npy  25806.npy  39287.npy  53096.npy  68253.npy  81792.npy  95843.npy\n",
            "10538.npy  25807.npy  39401.npy  53102.npy  68254.npy  81902.npy  95846.npy\n",
            "10539.npy  25816.npy  39405.npy  53108.npy  68257.npy  81904.npy  95861.npy\n",
            "10547.npy  25830.npy  39408.npy  53124.npy  68259.npy  81934.npy  95872.npy\n",
            "10548.npy  25837.npy  39416.npy  53140.npy  68270.npy  81940.npy  95874.npy\n",
            "10562.npy  25841.npy  39420.npy  53147.npy  68294.npy  81942.npy  96015.npy\n",
            "10564.npy  25847.npy  39421.npy  53160.npy  68307.npy  81943.npy  96023.npy\n",
            "10567.npy  25849.npy  39425.npy  53172.npy  68309.npy  81946.npy  96034.npy\n",
            "10572.npy  25861.npy  39427.npy  53180.npy  68321.npy  81947.npy  96037.npy\n",
            "10574.npy  25874.npy  39452.npy  53182.npy  68324.npy  81950.npy  96042.npy\n",
            "10576.npy  25893.npy  39465.npy  53184.npy  68325.npy  81953.npy  96054.npy\n",
            "10579.npy  25901.npy  39470.npy  53187.npy  68327.npy  81956.npy  96058.npy\n",
            "10582.npy  25904.npy  39472.npy  53194.npy  68329.npy  81962.npy  96071.npy\n",
            "10583.npy  25907.npy  39475.npy  53204.npy  68342.npy  81963.npy  96073.npy\n",
            "10584.npy  25908.npy  39478.npy  53206.npy  68349.npy  81964.npy  96078.npy\n",
            "10589.npy  25913.npy  39482.npy  53210.npy  68370.npy  81970.npy  96081.npy\n",
            "10596.npy  25914.npy  39504.npy  53214.npy  68371.npy  81972.npy  96082.npy\n",
            "10625.npy  25917.npy  39524.npy  53219.npy  68374.npy  81975.npy  96083.npy\n",
            "10639.npy  25943.npy  39526.npy  53241.npy  68379.npy  81976.npy  96087.npy\n",
            "10649.npy  25948.npy  39527.npy  53249.npy  68390.npy  82014.npy  96103.npy\n",
            "10652.npy  25968.npy  39528.npy  53260.npy  68391.npy  82016.npy  96104.npy\n",
            "10653.npy  25971.npy  39540.npy  53261.npy  68394.npy  82037.npy  96120.npy\n",
            "10659.npy  25983.npy  39541.npy  53268.npy  68401.npy  82043.npy  96123.npy\n",
            "10673.npy  25984.npy  39542.npy  53270.npy  68407.npy  82045.npy  96127.npy\n",
            "10675.npy  25986.npy  39547.npy  53274.npy  68409.npy  82076.npy  96143.npy\n",
            "10682.npy  26014.npy  39560.npy  53278.npy  68417.npy  82079.npy  96150.npy\n",
            "10689.npy  26018.npy  39562.npy  53280.npy  68423.npy  82094.npy  96152.npy\n",
            "10692.npy  26031.npy  39564.npy  53289.npy  68429.npy  82097.npy  96153.npy\n",
            "10693.npy  26034.npy  39576.npy  53291.npy  68431.npy  82104.npy  96172.npy\n",
            "10725.npy  26043.npy  39578.npy  53296.npy  68450.npy  82134.npy  96180.npy\n",
            "10732.npy  26049.npy  39580.npy  53407.npy  68459.npy  82139.npy  96182.npy\n",
            "10734.npy  26057.npy  39601.npy  53418.npy  68472.npy  82150.npy  96183.npy\n",
            "10743.npy  26073.npy  39607.npy  53419.npy  68503.npy  82154.npy  96184.npy\n",
            "10748.npy  26078.npy  39612.npy  53420.npy  68507.npy  82156.npy  96185.npy\n",
            "10749.npy  26085.npy  39624.npy  53426.npy  68512.npy  82163.npy  96204.npy\n",
            "10752.npy  26087.npy  39625.npy  53427.npy  68517.npy  82165.npy  96208.npy\n",
            "10759.npy  26091.npy  39640.npy  53467.npy  68520.npy  82169.npy  96213.npy\n",
            "10765.npy  26095.npy  39645.npy  53469.npy  68521.npy  82179.npy  96230.npy\n",
            "10785.npy  26137.npy  39652.npy  53471.npy  68527.npy  82190.npy  96231.npy\n",
            "10786.npy  26140.npy  39658.npy  53476.npy  68530.npy  82195.npy  96235.npy\n",
            "10792.npy  26149.npy  39675.npy  53481.npy  68531.npy  82301.npy  96238.npy\n",
            "10793.npy  26153.npy  39704.npy  53482.npy  68540.npy  82310.npy  96240.npy\n",
            "10827.npy  26170.npy  39706.npy  53486.npy  68549.npy  82316.npy  96250.npy\n",
            "10829.npy  26174.npy  39725.npy  53491.npy  68579.npy  82346.npy  96253.npy\n",
            "10834.npy  26179.npy  39726.npy  53492.npy  68592.npy  82356.npy  96258.npy\n",
            "10835.npy  26183.npy  39741.npy  53607.npy  68594.npy  82359.npy  96270.npy\n",
            "10839.npy  26184.npy  39748.npy  53609.npy  68597.npy  82367.npy  96280.npy\n",
            "10843.npy  26185.npy  39765.npy  53618.npy  68713.npy  82369.npy  96284.npy\n",
            "10852.npy  26187.npy  39784.npy  53624.npy  68715.npy  82379.npy  96301.npy\n",
            "10853.npy  26189.npy  39785.npy  53628.npy  68719.npy  82396.npy  96302.npy\n",
            "10857.npy  26197.npy  39805.npy  53642.npy  68720.npy  82403.npy  96305.npy\n",
            "10865.npy  26198.npy  39815.npy  53649.npy  68721.npy  82406.npy  96307.npy\n",
            "10867.npy  26305.npy  39816.npy  53678.npy  68730.npy  82413.npy  96312.npy\n",
            "10893.npy  26307.npy  39817.npy  53679.npy  68731.npy  82419.npy  96317.npy\n",
            "10895.npy  26314.npy  39821.npy  53682.npy  68739.npy  82437.npy  96325.npy\n",
            "10926.npy  26315.npy  39826.npy  53692.npy  68742.npy  82450.npy  96328.npy\n",
            "10932.npy  26318.npy  39827.npy  53694.npy  68745.npy  82457.npy  96340.npy\n",
            "10935.npy  26319.npy  39842.npy  53706.npy  68749.npy  82460.npy  96342.npy\n",
            "10936.npy  26340.npy  39852.npy  53714.npy  68751.npy  82461.npy  96345.npy\n",
            "10938.npy  26347.npy  39854.npy  53720.npy  68752.npy  82469.npy  96354.npy\n",
            "10943.npy  26357.npy  39860.npy  53740.npy  68753.npy  82476.npy  96380.npy\n",
            "10946.npy  26358.npy  39861.npy  53768.npy  68901.npy  82501.npy  96381.npy\n",
            "10947.npy  26370.npy  39862.npy  53769.npy  68903.npy  82503.npy  96402.npy\n",
            "10948.npy  26371.npy  39867.npy  53782.npy  68913.npy  82507.npy  96405.npy\n",
            "10952.npy  26374.npy  39870.npy  53806.npy  68917.npy  82509.npy  96407.npy\n",
            "10956.npy  26375.npy  40125.npy  53810.npy  68921.npy  82510.npy  96415.npy\n",
            "10962.npy  26378.npy  40127.npy  53812.npy  68934.npy  82531.npy  96417.npy\n",
            "10963.npy  26379.npy  40136.npy  53814.npy  68937.npy  82539.npy  96420.npy\n",
            "10967.npy  26381.npy  40137.npy  53817.npy  68941.npy  82540.npy  96421.npy\n",
            "10972.npy  26385.npy  40153.npy  53820.npy  68943.npy  82541.npy  96423.npy\n",
            "10976.npy  26390.npy  40158.npy  53821.npy  68947.npy  82547.npy  96431.npy\n",
            "10983.npy  26391.npy  40159.npy  53827.npy  68950.npy  82549.npy  96435.npy\n",
            "10985.npy  26394.npy  40167.npy  53849.npy  68951.npy  82563.npy  96450.npy\n",
            "12034.npy  26397.npy  40182.npy  53876.npy  68954.npy  82590.npy  96452.npy\n",
            "12035.npy  26405.npy  40193.npy  53879.npy  68971.npy  82591.npy  96470.npy\n",
            "12036.npy  26407.npy  40216.npy  53901.npy  68973.npy  82593.npy  96471.npy\n",
            "12039.npy  26409.npy  40217.npy  53904.npy  68975.npy  82596.npy  96472.npy\n",
            "12045.npy  26410.npy  40218.npy  53908.npy  69012.npy  82603.npy  96475.npy\n",
            "12054.npy  26417.npy  40239.npy  53914.npy  69013.npy  82609.npy  96485.npy\n",
            "12059.npy  26418.npy  40256.npy  53917.npy  69014.npy  82631.npy  96502.npy\n",
            "12063.npy  26419.npy  40257.npy  53924.npy  69017.npy  82635.npy  96507.npy\n",
            "12067.npy  26438.npy  40259.npy  53927.npy  69018.npy  82637.npy  96508.npy\n",
            "12068.npy  26439.npy  40265.npy  53941.npy  69024.npy  82647.npy  96510.npy\n",
            "12074.npy  26453.npy  40268.npy  53942.npy  69025.npy  82651.npy  96513.npy\n",
            "12086.npy  26459.npy  40269.npy  53948.npy  69027.npy  82657.npy  96514.npy\n",
            "12093.npy  26478.npy  40286.npy  53962.npy  69035.npy  82673.npy  96517.npy\n",
            "12094.npy  26489.npy  40291.npy  53967.npy  69041.npy  82674.npy  96524.npy\n",
            "12096.npy  26490.npy  40293.npy  53968.npy  69042.npy  82675.npy  96531.npy\n",
            "12309.npy  26493.npy  40297.npy  53976.npy  69043.npy  82679.npy  96541.npy\n",
            "12348.npy  26495.npy  40312.npy  53980.npy  69052.npy  82693.npy  96570.npy\n",
            "12349.npy  26497.npy  40315.npy  54016.npy  69054.npy  82697.npy  96573.npy\n",
            "12356.npy  26507.npy  40316.npy  54018.npy  69057.npy  82704.npy  96578.npy\n",
            "12358.npy  26531.npy  40321.npy  54019.npy  69073.npy  82705.npy  96581.npy\n",
            "12374.npy  26534.npy  40326.npy  54023.npy  69074.npy  82714.npy  96583.npy\n",
            "12376.npy  26537.npy  40329.npy  54028.npy  69078.npy  82719.npy  96587.npy\n",
            "12389.npy  26538.npy  40351.npy  54038.npy  69081.npy  82730.npy  96701.npy\n",
            "12407.npy  26543.npy  40372.npy  54039.npy  69084.npy  82736.npy  96704.npy\n",
            "12430.npy  26547.npy  40375.npy  54062.npy  69085.npy  82740.npy  96708.npy\n",
            "12450.npy  26549.npy  40378.npy  54068.npy  69087.npy  82743.npy  96710.npy\n",
            "12453.npy  26571.npy  40385.npy  54069.npy  69123.npy  82749.npy  96713.npy\n",
            "12460.npy  26573.npy  40512.npy  54082.npy  69125.npy  82756.npy  96720.npy\n",
            "12468.npy  26579.npy  40516.npy  54083.npy  69128.npy  82760.npy  96730.npy\n",
            "12483.npy  26598.npy  40529.npy  54092.npy  69130.npy  82761.npy  96734.npy\n",
            "12489.npy  26708.npy  40539.npy  54093.npy  69134.npy  82763.npy  96735.npy\n",
            "12490.npy  26709.npy  40561.npy  54098.npy  69137.npy  82765.npy  96745.npy\n",
            "12495.npy  26715.npy  40563.npy  54102.npy  69140.npy  82793.npy  96752.npy\n",
            "12504.npy  26735.npy  40567.npy  54108.npy  69150.npy  82794.npy  96784.npy\n",
            "12507.npy  26738.npy  40569.npy  54129.npy  69158.npy  82914.npy  96785.npy\n",
            "12509.npy  26741.npy  40572.npy  54130.npy  69174.npy  82916.npy  96801.npy\n",
            "12534.npy  26750.npy  40573.npy  54138.npy  69175.npy  82940.npy  96802.npy\n",
            "12536.npy  26759.npy  40576.npy  54170.npy  69178.npy  82941.npy  96804.npy\n",
            "12546.npy  26781.npy  40578.npy  54179.npy  69203.npy  82943.npy  96810.npy\n",
            "12547.npy  26784.npy  40592.npy  54190.npy  69205.npy  82945.npy  96820.npy\n",
            "12548.npy  26794.npy  40597.npy  54196.npy  69234.npy  82946.npy  96821.npy\n",
            "12549.npy  26795.npy  40618.npy  54201.npy  69235.npy  82947.npy  96824.npy\n",
            "12570.npy  26803.npy  40621.npy  54206.npy  69237.npy  82950.npy  96840.npy\n",
            "12576.npy  26831.npy  40627.npy  54210.npy  69250.npy  82960.npy  96845.npy\n",
            "12580.npy  26835.npy  40629.npy  54261.npy  69251.npy  82963.npy  96857.npy\n",
            "12589.npy  26839.npy  40632.npy  54263.npy  69253.npy  82964.npy  96872.npy\n",
            "12597.npy  26841.npy  40651.npy  54270.npy  69258.npy  82973.npy  96873.npy\n",
            "12603.npy  26890.npy  40653.npy  54276.npy  69304.npy  83014.npy  96874.npy\n",
            "12604.npy  26891.npy  40679.npy  54286.npy  69305.npy  83024.npy  97013.npy\n",
            "12607.npy  26894.npy  40681.npy  54289.npy  69307.npy  83025.npy  97015.npy\n",
            "12608.npy  26903.npy  40693.npy  54290.npy  69308.npy  83045.npy  97021.npy\n",
            "12637.npy  26905.npy  40712.npy  54301.npy  69312.npy  83046.npy  97023.npy\n",
            "12643.npy  26910.npy  40716.npy  54310.npy  69324.npy  83052.npy  97025.npy\n",
            "12647.npy  26914.npy  40718.npy  54317.npy  69327.npy  83056.npy  97026.npy\n",
            "12648.npy  26930.npy  40721.npy  54318.npy  69342.npy  83059.npy  97035.npy\n",
            "12649.npy  26940.npy  40723.npy  54319.npy  69348.npy  83061.npy  97041.npy\n",
            "12650.npy  26943.npy  40726.npy  54321.npy  69351.npy  83064.npy  97045.npy\n",
            "12657.npy  26945.npy  40728.npy  54367.npy  69357.npy  83074.npy  97051.npy\n",
            "12673.npy  26947.npy  40731.npy  54370.npy  69370.npy  83076.npy  97058.npy\n",
            "12675.npy  26950.npy  40735.npy  54376.npy  69380.npy  83091.npy  97068.npy\n",
            "12678.npy  26973.npy  40752.npy  54389.npy  69402.npy  83095.npy  97102.npy\n",
            "12694.npy  26980.npy  40756.npy  54397.npy  69403.npy  83125.npy  97103.npy\n",
            "12704.npy  26983.npy  40758.npy  54601.npy  69405.npy  83127.npy  97105.npy\n",
            "12736.npy  26984.npy  40759.npy  54602.npy  69410.npy  83140.npy  97108.npy\n",
            "12748.npy  26985.npy  40761.npy  54608.npy  69412.npy  83145.npy  97125.npy\n",
            "12750.npy  26987.npy  40791.npy  54610.npy  69415.npy  83150.npy  97128.npy\n",
            "12754.npy  27014.npy  40792.npy  54613.npy  69418.npy  83152.npy  97132.npy\n",
            "12763.npy  27016.npy  40796.npy  54617.npy  69421.npy  83170.npy  97142.npy\n",
            "12784.npy  27034.npy  40813.npy  54621.npy  69423.npy  83175.npy  97153.npy\n",
            "12785.npy  27038.npy  40815.npy  54629.npy  69425.npy  83201.npy  97160.npy\n",
            "12793.npy  27041.npy  40819.npy  54632.npy  69427.npy  83205.npy  97162.npy\n",
            "12807.npy  27043.npy  40821.npy  54639.npy  69428.npy  83206.npy  97180.npy\n",
            "12834.npy  27048.npy  40826.npy  54670.npy  69450.npy  83207.npy  97183.npy\n",
            "12836.npy  27056.npy  40829.npy  54672.npy  69470.npy  83215.npy  97184.npy\n",
            "12837.npy  27058.npy  40832.npy  54679.npy  69472.npy  83216.npy  97201.npy\n",
            "12839.npy  27059.npy  40837.npy  54682.npy  69480.npy  83219.npy  97204.npy\n",
            "12843.npy  27068.npy  40839.npy  54683.npy  69487.npy  83246.npy  97208.npy\n",
            "12867.npy  27069.npy  40852.npy  54690.npy  69503.npy  83256.npy  97210.npy\n",
            "12873.npy  27083.npy  40856.npy  54691.npy  69512.npy  83257.npy  97213.npy\n",
            "12876.npy  27105.npy  40857.npy  54692.npy  69518.npy  83264.npy  97230.npy\n",
            "12890.npy  27106.npy  40862.npy  54697.npy  69520.npy  83271.npy  97236.npy\n",
            "12894.npy  27134.npy  40865.npy  54708.npy  69521.npy  83290.npy  97245.npy\n",
            "12907.npy  27139.npy  40871.npy  54709.npy  69523.npy  83295.npy  97250.npy\n",
            "12935.npy  27150.npy  40875.npy  54710.npy  69524.npy  83296.npy  97256.npy\n",
            "12936.npy  27153.npy  40896.npy  54713.npy  69528.npy  83402.npy  97258.npy\n",
            "12953.npy  27156.npy  40897.npy  54719.npy  69530.npy  83415.npy  97261.npy\n",
            "12956.npy  27163.npy  40913.npy  54720.npy  69534.npy  83427.npy  97283.npy\n",
            "12957.npy  27169.npy  40916.npy  54726.npy  69542.npy  83429.npy  97301.npy\n",
            "12958.npy  27180.npy  40917.npy  54728.npy  69548.npy  83456.npy  97305.npy\n",
            "12967.npy  27185.npy  40918.npy  54731.npy  69582.npy  83461.npy  97314.npy\n",
            "12968.npy  27195.npy  40927.npy  54761.npy  69584.npy  83476.npy  97315.npy\n",
            "12973.npy  27304.npy  40932.npy  54780.npy  69708.npy  83490.npy  97316.npy\n",
            "12983.npy  27308.npy  40953.npy  54781.npy  69710.npy  83492.npy  97320.npy\n",
            "13026.npy  27309.npy  40965.npy  54782.npy  69713.npy  83497.npy  97321.npy\n",
            "13042.npy  27318.npy  40967.npy  54789.npy  69720.npy  83504.npy  97325.npy\n",
            "13045.npy  27341.npy  40982.npy  54790.npy  69728.npy  83509.npy  97345.npy\n",
            "13046.npy  27351.npy  40987.npy  54793.npy  69732.npy  83510.npy  97348.npy\n",
            "13049.npy  27360.npy  41023.npy  54801.npy  69750.npy  83517.npy  97361.npy\n",
            "13058.npy  27365.npy  41032.npy  54812.npy  69751.npy  83527.npy  97365.npy\n",
            "13067.npy  27384.npy  41035.npy  54816.npy  69752.npy  83547.npy  97384.npy\n",
            "13068.npy  27385.npy  41037.npy  54817.npy  69753.npy  83549.npy  97386.npy\n",
            "13069.npy  27391.npy  41057.npy  54819.npy  69758.npy  83560.npy  97402.npy\n",
            "13074.npy  27395.npy  41058.npy  54820.npy  69780.npy  83570.npy  97403.npy\n",
            "13076.npy  27398.npy  41062.npy  54823.npy  69804.npy  83571.npy  97413.npy\n",
            "13085.npy  27401.npy  41063.npy  54830.npy  69807.npy  83579.npy  97415.npy\n",
            "13086.npy  27405.npy  41067.npy  54831.npy  69813.npy  83602.npy  97423.npy\n",
            "13095.npy  27408.npy  41068.npy  54837.npy  69815.npy  83607.npy  97426.npy\n",
            "13096.npy  27410.npy  41078.npy  54891.npy  69823.npy  83609.npy  97436.npy\n",
            "13098.npy  27416.npy  41086.npy  54893.npy  69842.npy  83615.npy  97438.npy\n",
            "13209.npy  27418.npy  41087.npy  54896.npy  69870.npy  83619.npy  97453.npy\n",
            "13248.npy  27430.npy  41092.npy  54912.npy  69873.npy  83624.npy  97456.npy\n",
            "13249.npy  27436.npy  41203.npy  54917.npy  70123.npy  83650.npy  97463.npy\n",
            "13269.npy  27438.npy  41206.npy  54920.npy  70125.npy  83651.npy  97465.npy\n",
            "13270.npy  27461.npy  41208.npy  54927.npy  70126.npy  83657.npy  97481.npy\n",
            "13276.npy  27468.npy  41209.npy  54930.npy  70132.npy  83670.npy  97486.npy\n",
            "13285.npy  27480.npy  41250.npy  54931.npy  70136.npy  83675.npy  97501.npy\n",
            "13289.npy  27485.npy  41258.npy  54936.npy  70142.npy  83692.npy  97502.npy\n",
            "13297.npy  27489.npy  41263.npy  54962.npy  70143.npy  83701.npy  97503.npy\n",
            "13402.npy  27490.npy  41265.npy  54971.npy  70146.npy  83704.npy  97508.npy\n",
            "13407.npy  27496.npy  41268.npy  54972.npy  70148.npy  83719.npy  97510.npy\n",
            "13426.npy  27498.npy  41270.npy  54973.npy  70153.npy  83720.npy  97514.npy\n",
            "13428.npy  27501.npy  41276.npy  54976.npy  70158.npy  83726.npy  97520.npy\n",
            "13450.npy  27503.npy  41280.npy  54978.npy  70162.npy  83740.npy  97521.npy\n",
            "13458.npy  27506.npy  41296.npy  54981.npy  70164.npy  83745.npy  97523.npy\n",
            "13459.npy  27514.npy  41298.npy  54983.npy  70169.npy  83756.npy  97524.npy\n",
            "13460.npy  27518.npy  41305.npy  56019.npy  70183.npy  83760.npy  97531.npy\n",
            "13465.npy  27519.npy  41308.npy  56024.npy  70189.npy  83764.npy  97542.npy\n",
            "13469.npy  27534.npy  41325.npy  56048.npy  70193.npy  83765.npy  97543.npy\n",
            "13470.npy  27548.npy  41358.npy  56071.npy  70194.npy  83794.npy  97546.npy\n",
            "13476.npy  27563.npy  41362.npy  56081.npy  70196.npy  83901.npy  97562.npy\n",
            "13478.npy  27568.npy  41368.npy  56082.npy  70213.npy  83905.npy  97564.npy\n",
            "13482.npy  27580.npy  41370.npy  56091.npy  70214.npy  83907.npy  97568.npy\n",
            "13486.npy  27591.npy  41372.npy  56098.npy  70231.npy  83914.npy  97583.npy\n",
            "13492.npy  27603.npy  41376.npy  56104.npy  70235.npy  83916.npy  97586.npy\n",
            "13495.npy  27604.npy  41379.npy  56107.npy  70243.npy  83920.npy  97601.npy\n",
            "13506.npy  27605.npy  41382.npy  56108.npy  70254.npy  83921.npy  97604.npy\n",
            "13520.npy  27608.npy  41385.npy  56123.npy  70264.npy  83924.npy  97610.npy\n",
            "13528.npy  27613.npy  41387.npy  56124.npy  70269.npy  83927.npy  97623.npy\n",
            "13542.npy  27614.npy  41389.npy  56128.npy  70281.npy  83941.npy  97625.npy\n",
            "13548.npy  27630.npy  41502.npy  56130.npy  70284.npy  83946.npy  97628.npy\n",
            "13570.npy  27634.npy  41503.npy  56134.npy  70285.npy  83954.npy  97635.npy\n",
            "13576.npy  27635.npy  41506.npy  56138.npy  70294.npy  83957.npy  97643.npy\n",
            "13579.npy  27648.npy  41523.npy  56143.npy  70296.npy  83960.npy  97683.npy\n",
            "13590.npy  27659.npy  41527.npy  56149.npy  70315.npy  83964.npy  97685.npy\n",
            "13592.npy  27685.npy  41537.npy  56179.npy  70318.npy  83965.npy  97804.npy\n",
            "13596.npy  27801.npy  41538.npy  56180.npy  70319.npy  83972.npy  97805.npy\n",
            "13602.npy  27803.npy  41539.npy  56183.npy  70324.npy  83976.npy  97810.npy\n",
            "13607.npy  27806.npy  41562.npy  56184.npy  70329.npy  84013.npy  97814.npy\n",
            "13609.npy  27810.npy  41568.npy  56187.npy  70346.npy  84015.npy  97815.npy\n",
            "13620.npy  27816.npy  41573.npy  56189.npy  70352.npy  84016.npy  97816.npy\n",
            "13624.npy  27835.npy  41578.npy  56193.npy  70356.npy  84021.npy  97821.npy\n",
            "13640.npy  27836.npy  41589.npy  56207.npy  70362.npy  84036.npy  97823.npy\n",
            "13647.npy  27843.npy  41597.npy  56208.npy  70368.npy  84057.npy  97824.npy\n",
            "13650.npy  27854.npy  41602.npy  56210.npy  70381.npy  84059.npy  97826.npy\n",
            "13652.npy  27856.npy  41603.npy  56213.npy  70384.npy  84061.npy  97831.npy\n",
            "13657.npy  27860.npy  41605.npy  56219.npy  70385.npy  84069.npy  97832.npy\n",
            "13674.npy  27861.npy  41607.npy  56231.npy  70395.npy  84072.npy  97836.npy\n",
            "13675.npy  27864.npy  41623.npy  56240.npy  70396.npy  84076.npy  97842.npy\n",
            "13679.npy  27891.npy  41637.npy  56241.npy  70415.npy  84091.npy  97845.npy\n",
            "13690.npy  27895.npy  41638.npy  56247.npy  70423.npy  84097.npy  97853.npy\n",
            "13692.npy  27901.npy  41650.npy  56248.npy  70426.npy  84102.npy  97860.npy\n",
            "13695.npy  27905.npy  41652.npy  56249.npy  70429.npy  84103.npy  98016.npy\n",
            "13697.npy  27913.npy  41657.npy  56274.npy  70432.npy  84106.npy  98023.npy\n",
            "13720.npy  27914.npy  41672.npy  56280.npy  70435.npy  84107.npy  98026.npy\n",
            "13726.npy  27915.npy  41678.npy  56281.npy  70452.npy  84120.npy  98032.npy\n",
            "13745.npy  27918.npy  41679.npy  56287.npy  70456.npy  84125.npy  98034.npy\n",
            "13748.npy  27934.npy  41680.npy  56290.npy  70459.npy  84150.npy  98035.npy\n",
            "13750.npy  27943.npy  41689.npy  56291.npy  70465.npy  84152.npy  98047.npy\n",
            "13764.npy  27946.npy  41690.npy  56294.npy  70492.npy  84153.npy  98057.npy\n",
            "13765.npy  27948.npy  41692.npy  56304.npy  70495.npy  84162.npy  98064.npy\n",
            "13769.npy  27950.npy  41697.npy  56310.npy  70496.npy  84169.npy  98067.npy\n",
            "13792.npy  27953.npy  41706.npy  56327.npy  70512.npy  84172.npy  98071.npy\n",
            "13795.npy  27956.npy  41720.npy  56328.npy  70519.npy  84176.npy  98072.npy\n",
            "13798.npy  27960.npy  41728.npy  56340.npy  70528.npy  84179.npy  98075.npy\n",
            "13802.npy  27963.npy  41739.npy  56374.npy  70529.npy  84192.npy  98103.npy\n",
            "13806.npy  27965.npy  41756.npy  56378.npy  70531.npy  84209.npy  98105.npy\n",
            "13809.npy  27980.npy  41769.npy  56387.npy  70532.npy  84219.npy  98124.npy\n",
            "13842.npy  27981.npy  41782.npy  56389.npy  70534.npy  84236.npy  98125.npy\n",
            "13849.npy  27986.npy  41795.npy  56392.npy  70539.npy  84237.npy  98134.npy\n",
            "13852.npy  28017.npy  41796.npy  56401.npy  70541.npy  84265.npy  98137.npy\n",
            "13856.npy  28019.npy  41802.npy  56409.npy  70543.npy  84269.npy  98145.npy\n",
            "13857.npy  28039.npy  41803.npy  56413.npy  70549.npy  84271.npy  98154.npy\n",
            "13859.npy  28043.npy  41806.npy  56417.npy  70561.npy  84273.npy  98157.npy\n",
            "13865.npy  28045.npy  41823.npy  56418.npy  70562.npy  84276.npy  98163.npy\n",
            "13870.npy  28046.npy  41830.npy  56419.npy  70564.npy  84291.npy  98170.npy\n",
            "13874.npy  28049.npy  41832.npy  56421.npy  70582.npy  84295.npy  98176.npy\n",
            "13875.npy  28051.npy  41835.npy  56423.npy  70592.npy  84297.npy  98203.npy\n",
            "13876.npy  28059.npy  41852.npy  56428.npy  70594.npy  84302.npy  98215.npy\n",
            "13890.npy  28061.npy  41853.npy  56429.npy  70598.npy  84305.npy  98235.npy\n",
            "13894.npy  28063.npy  41856.npy  56430.npy  70612.npy  84316.npy  98241.npy\n",
            "13895.npy  28067.npy  41857.npy  56432.npy  70613.npy  84317.npy  98243.npy\n",
            "13902.npy  28074.npy  41860.npy  56438.npy  70614.npy  84321.npy  98245.npy\n",
            "13906.npy  28076.npy  41869.npy  56470.npy  70623.npy  84325.npy  98246.npy\n",
            "13907.npy  28096.npy  41873.npy  56472.npy  70624.npy  84327.npy  98256.npy\n",
            "13924.npy  28134.npy  41875.npy  56473.npy  70628.npy  84329.npy  98270.npy\n",
            "13945.npy  28146.npy  41890.npy  56480.npy  70634.npy  84351.npy  98271.npy\n",
            "13958.npy  28147.npy  41896.npy  56482.npy  70643.npy  84360.npy  98276.npy\n",
            "13960.npy  28154.npy  41903.npy  56489.npy  70649.npy  84361.npy  98305.npy\n",
            "13962.npy  28157.npy  41907.npy  56490.npy  70652.npy  84362.npy  98307.npy\n",
            "13964.npy  28160.npy  41908.npy  56497.npy  70654.npy  84370.npy  98314.npy\n",
            "13968.npy  28163.npy  41923.npy  56708.npy  70658.npy  84379.npy  98316.npy\n",
            "13970.npy  28173.npy  41925.npy  56709.npy  70691.npy  84392.npy  98340.npy\n",
            "13976.npy  28175.npy  41936.npy  56712.npy  70821.npy  84395.npy  98345.npy\n",
            "13986.npy  28190.npy  41958.npy  56713.npy  70825.npy  84502.npy  98346.npy\n",
            "14026.npy  28195.npy  41962.npy  56719.npy  70826.npy  84510.npy  98350.npy\n",
            "14028.npy  28196.npy  41972.npy  56721.npy  70829.npy  84516.npy  98351.npy\n",
            "14037.npy  28197.npy  41983.npy  56723.npy  70834.npy  84517.npy  98357.npy\n",
            "14038.npy  28315.npy  41986.npy  56738.npy  70835.npy  84521.npy  98362.npy\n",
            "14052.npy  28317.npy  42015.npy  56739.npy  70841.npy  84523.npy  98364.npy\n",
            "14059.npy  28319.npy  42019.npy  56741.npy  70842.npy  84527.npy  98367.npy\n",
            "14067.npy  28360.npy  42036.npy  56749.npy  70843.npy  84531.npy  98374.npy\n",
            "14072.npy  28369.npy  42039.npy  56789.npy  70845.npy  84532.npy  98406.npy\n",
            "14078.npy  28374.npy  42056.npy  56790.npy  70849.npy  84537.npy  98412.npy\n",
            "14079.npy  28391.npy  42058.npy  56791.npy  70851.npy  84563.npy  98416.npy\n",
            "14082.npy  28395.npy  42063.npy  56792.npy  70864.npy  84576.npy  98421.npy\n",
            "14083.npy  28396.npy  42065.npy  56807.npy  70865.npy  84592.npy  98425.npy\n",
            "14086.npy  28397.npy  42073.npy  56812.npy  70912.npy  84593.npy  98426.npy\n",
            "14087.npy  28401.npy  42078.npy  56819.npy  70914.npy  84596.npy  98427.npy\n",
            "14092.npy  28403.npy  42083.npy  56821.npy  70923.npy  84609.npy  98457.npy\n",
            "14096.npy  28405.npy  42086.npy  56827.npy  70925.npy  84620.npy  98461.npy\n",
            "14097.npy  28406.npy  42087.npy  56830.npy  70928.npy  84629.npy  98463.npy\n",
            "14236.npy  28431.npy  42091.npy  56834.npy  70932.npy  84639.npy  98473.npy\n",
            "14238.npy  28435.npy  42093.npy  56837.npy  70934.npy  84652.npy  98501.npy\n",
            "14239.npy  28436.npy  42105.npy  56841.npy  70935.npy  84653.npy  98504.npy\n",
            "14259.npy  28437.npy  42137.npy  56843.npy  70936.npy  84670.npy  98506.npy\n",
            "14260.npy  28439.npy  42153.npy  56870.npy  70941.npy  84673.npy  98514.npy\n",
            "14263.npy  28450.npy  42156.npy  56871.npy  70942.npy  84692.npy  98517.npy\n",
            "14267.npy  28451.npy  42160.npy  56872.npy  70948.npy  84703.npy  98523.npy\n",
            "14268.npy  28459.npy  42163.npy  56873.npy  70953.npy  84709.npy  98526.npy\n",
            "14275.npy  28461.npy  42167.npy  56891.npy  70981.npy  84719.npy  98530.npy\n",
            "14276.npy  28463.npy  42168.npy  56907.npy  70984.npy  84721.npy  98531.npy\n",
            "14278.npy  28470.npy  42169.npy  56908.npy  71028.npy  84729.npy  98537.npy\n",
            "14283.npy  28476.npy  42170.npy  56910.npy  71038.npy  84736.npy  98542.npy\n",
            "14296.npy  28479.npy  42173.npy  56912.npy  71042.npy  84756.npy  98543.npy\n",
            "14297.npy  28491.npy  42175.npy  56917.npy  71045.npy  84759.npy  98547.npy\n",
            "14306.npy  28495.npy  42178.npy  56921.npy  71046.npy  84762.npy  98561.npy\n",
            "14327.npy  28504.npy  42179.npy  56928.npy  71048.npy  84790.npy  98564.npy\n",
            "14329.npy  28507.npy  42183.npy  56930.npy  71052.npy  84901.npy  98567.npy\n",
            "14360.npy  28516.npy  42185.npy  56931.npy  71058.npy  84907.npy  98571.npy\n",
            "14362.npy  28517.npy  42187.npy  56938.npy  71063.npy  84910.npy  98573.npy\n",
            "14372.npy  28534.npy  42189.npy  56940.npy  71082.npy  84913.npy  98574.npy\n",
            "14375.npy  28546.npy  42307.npy  56942.npy  71083.npy  84916.npy  98601.npy\n",
            "14378.npy  28561.npy  42308.npy  56948.npy  71084.npy  84921.npy  98603.npy\n",
            "14379.npy  28563.npy  42309.npy  56972.npy  71086.npy  84926.npy  98624.npy\n",
            "14386.npy  28567.npy  42315.npy  56973.npy  71089.npy  84927.npy  98625.npy\n",
            "14389.npy  28574.npy  42317.npy  56974.npy  71095.npy  84951.npy  98627.npy\n",
            "14397.npy  28579.npy  42319.npy  56980.npy  71206.npy  84965.npy  98632.npy\n",
            "14507.npy  28590.npy  42351.npy  56981.npy  71230.npy  84970.npy  98637.npy\n",
            "14520.npy  28597.npy  42368.npy  56987.npy  71236.npy  84971.npy  98642.npy\n",
            "14523.npy  28601.npy  42376.npy  57012.npy  71245.npy  85012.npy  98643.npy\n",
            "14530.npy  28604.npy  42379.npy  57013.npy  71249.npy  85023.npy  98653.npy\n",
            "14569.npy  28605.npy  42380.npy  57029.npy  71253.npy  85026.npy  98654.npy\n",
            "14570.npy  28610.npy  42387.npy  57034.npy  71259.npy  85034.npy  98657.npy\n",
            "14573.npy  28613.npy  42391.npy  57042.npy  71263.npy  85042.npy  98670.npy\n",
            "14580.npy  28614.npy  42398.npy  57043.npy  71265.npy  85062.npy  98701.npy\n",
            "14583.npy  28617.npy  42503.npy  57049.npy  71280.npy  85072.npy  98702.npy\n",
            "14589.npy  28637.npy  42507.npy  57063.npy  71285.npy  85073.npy  98714.npy\n",
            "14590.npy  28639.npy  42509.npy  57064.npy  71289.npy  85092.npy  98716.npy\n",
            "14598.npy  28641.npy  42510.npy  57084.npy  71294.npy  85093.npy  98720.npy\n",
            "14602.npy  28657.npy  42538.npy  57089.npy  71302.npy  85094.npy  98721.npy\n",
            "14603.npy  28659.npy  42568.npy  57093.npy  71305.npy  85096.npy  98723.npy\n",
            "14620.npy  28670.npy  42580.npy  57094.npy  71308.npy  85097.npy  98724.npy\n",
            "14625.npy  28674.npy  42589.npy  57103.npy  71309.npy  85107.npy  98726.npy\n",
            "14632.npy  28690.npy  42591.npy  57120.npy  71320.npy  85126.npy  98731.npy\n",
            "14650.npy  28691.npy  42597.npy  57124.npy  71325.npy  85132.npy  98734.npy\n",
            "14652.npy  28695.npy  42598.npy  57128.npy  71340.npy  85136.npy  98736.npy\n",
            "14657.npy  28704.npy  42601.npy  57132.npy  71348.npy  85139.npy  98751.npy\n",
            "14658.npy  28716.npy  42603.npy  57134.npy  71354.npy  85146.npy  98752.npy\n",
            "14675.npy  28731.npy  42608.npy  57142.npy  71358.npy  85147.npy  98754.npy\n",
            "14679.npy  28735.npy  42613.npy  57148.npy  71360.npy  85167.npy  98756.npy\n",
            "14685.npy  28739.npy  42615.npy  57160.npy  71364.npy  85172.npy  98762.npy\n",
            "14690.npy  28751.npy  42618.npy  57162.npy  71365.npy  85173.npy  98764.npy\n",
            "14695.npy  28756.npy  42631.npy  57164.npy  71368.npy  85190.npy\n",
            "14697.npy  28759.npy  42635.npy  57169.npy  71380.npy  85192.npy\n",
            "14726.npy  28763.npy  42638.npy  57180.npy  71389.npy  85204.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWtlK_rNE-hr"
      },
      "source": [
        "import numpy as np\n",
        "data = np.load(\"/content/grid-feats-vqa/output/features/hateful_memes/01235.npy\", allow_pickle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMGaBDdWGTJO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        },
        "outputId": "801abb27-c18e-4668-c1eb-329fd2042d0b"
      },
      "source": [
        "data.item(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bbox': array([[259.67154, 161.62383, 278.1095 , 173.00247],\n",
              "        [252.43747, 161.38416, 272.26633, 180.26404],\n",
              "        [257.69632, 153.35738, 278.7746 , 171.86337],\n",
              "        [316.04187,  91.28587, 339.1968 , 105.36515],\n",
              "        [249.3239 , 166.94267, 270.6754 , 180.54642],\n",
              "        [268.07816, 146.82487, 280.08188, 184.44374],\n",
              "        [241.51526, 187.56104, 260.1389 , 204.11038],\n",
              "        [259.67154, 161.62383, 278.1095 , 173.00247],\n",
              "        [281.35248, 159.39366, 305.5214 , 179.62006]], dtype=float32),\n",
              " 'cls_prob': array([[3.7774025e-05, 2.7411718e-07, 1.4468035e-04, ..., 3.3372964e-07,\n",
              "         8.6575026e-05, 2.7763026e-05],\n",
              "        [1.9372252e-04, 3.7264224e-06, 1.1961237e-04, ..., 5.5164464e-06,\n",
              "         6.1771495e-04, 1.5503154e-04],\n",
              "        [3.2860064e-04, 8.9784944e-06, 3.0677838e-04, ..., 9.0119511e-06,\n",
              "         4.8007708e-04, 2.8090217e-04],\n",
              "        ...,\n",
              "        [7.5297671e-06, 1.0568231e-05, 2.4204587e-03, ..., 1.0737324e-06,\n",
              "         1.0481614e-03, 2.0361526e-04],\n",
              "        [3.7774025e-05, 2.7411718e-07, 1.4468035e-04, ..., 3.3372964e-07,\n",
              "         8.6575026e-05, 2.7763026e-05],\n",
              "        [3.3140287e-04, 1.2135213e-05, 9.6791147e-05, ..., 1.9391166e-05,\n",
              "         5.7531660e-04, 1.2530982e-04]], dtype=float32),\n",
              " 'features': array([[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 2.9262066e-02,\n",
              "         0.0000000e+00, 9.2111212e-01],\n",
              "        [1.8246397e-02, 0.0000000e+00, 0.0000000e+00, ..., 2.4117902e-03,\n",
              "         0.0000000e+00, 3.3645335e-01],\n",
              "        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 2.9818058e-01,\n",
              "         0.0000000e+00, 5.6965965e-01],\n",
              "        ...,\n",
              "        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
              "         0.0000000e+00, 0.0000000e+00],\n",
              "        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 2.9262066e-02,\n",
              "         0.0000000e+00, 9.2111212e-01],\n",
              "        [2.8870112e-01, 0.0000000e+00, 0.0000000e+00, ..., 2.2228556e+00,\n",
              "         8.8536263e-02, 4.5928392e+00]], dtype=float32),\n",
              " 'image_height': 366,\n",
              " 'image_width': 550,\n",
              " 'num_boxes': 9,\n",
              " 'objects': array([1535, 1535, 1535, 1099,  965, 1535,   64,  965, 1400])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paTNe9abIcqY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4c387dbc-d16f-4000-8506-c24ff5416045"
      },
      "source": [
        "data.item(0)[\"bbox\"].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30nhpSTzt272"
      },
      "source": [
        "##### <font color='PeachPuff'> <b> Extract image features using [`airsplay/py-bottom-up-attention`](https://github.com/airsplay/py-bottom-up-attention) </b> </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdUESoKqHZ1g"
      },
      "source": [
        "###### **Install packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZMo24Yjof56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "3bd3f3c3-ae32-4fec-fb4d-9a15e7eca8b8"
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/\")\n",
        "!git clone https://github.com/airsplay/py-bottom-up-attention.git\n",
        "os.chdir(\"py-bottom-up-attention/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'py-bottom-up-attention'...\n",
            "remote: Enumerating objects: 1991, done.\u001b[K\n",
            "remote: Total 1991 (delta 0), reused 0 (delta 0), pack-reused 1991\u001b[K\n",
            "Receiving objects: 100% (1991/1991), 8.94 MiB | 34.95 MiB/s, done.\n",
            "Resolving deltas: 100% (1225/1225), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8GLLejupnTF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        },
        "outputId": "6566cd09-aebc-48b7-fc41-43c932063a0b"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/facebookresearch/fvcore.git (from -r requirements.txt (line 1))\n",
            "  Cloning https://github.com/facebookresearch/fvcore.git to /tmp/pip-req-build-xlnh10sa\n",
            "  Running command git clone -q https://github.com/facebookresearch/fvcore.git /tmp/pip-req-build-xlnh10sa\n",
            "Collecting torch==1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4MB 21kB/s \n",
            "\u001b[?25hCollecting torchvision==0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/90/6141bf41f5655c78e24f40f710fdd4f8a8aff6c8b7c6f0328240f649bdbe/torchvision-0.5.0-cp36-cp36m-manylinux1_x86_64.whl (4.0MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0MB 50.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (1.18.5)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (0.29.21)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (4.1.2.30)\n",
            "Collecting yacs>=0.1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/38/4f/fe9a4d472aa867878ce3bb7efb16654c5d63672b86dc0e6e953a67018433/yacs-0.1.8-py3-none-any.whl\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1.2->-r requirements.txt (line 1)) (5.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1.2->-r requirements.txt (line 1)) (4.50.0)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1.2->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1.2->-r requirements.txt (line 1)) (7.0.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1.2->-r requirements.txt (line 1)) (0.8.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0->-r requirements.txt (line 3)) (1.15.0)\n",
            "Building wheels for collected packages: fvcore\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.2-cp36-none-any.whl size=47852 sha256=24b1d001258f8393663bdda6fffa7d564ed8b1102e2c172b9f5f3d334f000faa\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-q_jd815u/wheels/48/53/79/3c6485543a4455a0006f5db590ab9957622b6227011941de06\n",
            "Successfully built fvcore\n",
            "\u001b[31mERROR: mmf 1.0.0rc12 has requirement torch==1.6.0, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: mmf 1.0.0rc12 has requirement torchvision==0.7.0, but you'll have torchvision 0.5.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, torchvision, yacs, portalocker, fvcore\n",
            "  Found existing installation: torch 1.6.0+cu101\n",
            "    Uninstalling torch-1.6.0+cu101:\n",
            "      Successfully uninstalled torch-1.6.0+cu101\n",
            "  Found existing installation: torchvision 0.7.0+cu101\n",
            "    Uninstalling torchvision-0.7.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.7.0+cu101\n",
            "Successfully installed fvcore-0.1.2 portalocker-2.0.0 torch-1.4.0 torchvision-0.5.0 yacs-0.1.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGqAbI9qpyCh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "aae5d8ea-2848-4b6a-dfeb-cfd273cda8e3"
      },
      "source": [
        "!pip install 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
        "# Install detectron2\n",
        "!python setup.py build develop"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n",
            "  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-0j280n2k\n",
            "  Running command git clone -q https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-0j280n2k\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (50.3.0)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (0.29.21)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.4.7)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.18.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib>=2.1.0->pycocotools==2.0) (1.15.0)\n",
            "Building wheels for collected packages: pycocotools\n",
            "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0-cp36-cp36m-linux_x86_64.whl size=266454 sha256=9655c7c86ccf9a18d1c64fdb5ea74841877be994fae1953f90005aedcd78f6af\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ik2jc_lf/wheels/90/51/41/646daf401c3bc408ff10de34ec76587a9b3ebfac8d21ca5c3a\n",
            "Successfully built pycocotools\n",
            "Installing collected packages: pycocotools\n",
            "  Found existing installation: pycocotools 2.0.2\n",
            "    Uninstalling pycocotools-2.0.2:\n",
            "      Successfully uninstalled pycocotools-2.0.2\n",
            "Successfully installed pycocotools-2.0\n",
            "running build\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib.linux-x86_64-3.6\n",
            "creating build/lib.linux-x86_64-3.6/detectron2\n",
            "copying detectron2/__init__.py -> build/lib.linux-x86_64-3.6/detectron2\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/engine\n",
            "copying detectron2/engine/launch.py -> build/lib.linux-x86_64-3.6/detectron2/engine\n",
            "copying detectron2/engine/hooks.py -> build/lib.linux-x86_64-3.6/detectron2/engine\n",
            "copying detectron2/engine/defaults.py -> build/lib.linux-x86_64-3.6/detectron2/engine\n",
            "copying detectron2/engine/train_loop.py -> build/lib.linux-x86_64-3.6/detectron2/engine\n",
            "copying detectron2/engine/__init__.py -> build/lib.linux-x86_64-3.6/detectron2/engine\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/data\n",
            "copying detectron2/data/build.py -> build/lib.linux-x86_64-3.6/detectron2/data\n",
            "copying detectron2/data/detection_utils.py -> build/lib.linux-x86_64-3.6/detectron2/data\n",
            "copying detectron2/data/__init__.py -> build/lib.linux-x86_64-3.6/detectron2/data\n",
            "copying detectron2/data/common.py -> build/lib.linux-x86_64-3.6/detectron2/data\n",
            "copying detectron2/data/catalog.py -> build/lib.linux-x86_64-3.6/detectron2/data\n",
            "copying detectron2/data/dataset_mapper.py -> build/lib.linux-x86_64-3.6/detectron2/data\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/evaluation\n",
            "copying detectron2/evaluation/lvis_evaluation.py -> build/lib.linux-x86_64-3.6/detectron2/evaluation\n",
            "copying detectron2/evaluation/cityscapes_evaluation.py -> build/lib.linux-x86_64-3.6/detectron2/evaluation\n",
            "copying detectron2/evaluation/panoptic_evaluation.py -> build/lib.linux-x86_64-3.6/detectron2/evaluation\n",
            "copying detectron2/evaluation/pascal_voc_evaluation.py -> build/lib.linux-x86_64-3.6/detectron2/evaluation\n",
            "copying detectron2/evaluation/evaluator.py -> build/lib.linux-x86_64-3.6/detectron2/evaluation\n",
            "copying detectron2/evaluation/testing.py -> build/lib.linux-x86_64-3.6/detectron2/evaluation\n",
            "copying detectron2/evaluation/__init__.py -> build/lib.linux-x86_64-3.6/detectron2/evaluation\n",
            "copying detectron2/evaluation/coco_evaluation.py -> build/lib.linux-x86_64-3.6/detectron2/evaluation\n",
            "copying detectron2/evaluation/sem_seg_evaluation.py -> build/lib.linux-x86_64-3.6/detectron2/evaluation\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/structures\n",
            "copying detectron2/structures/image_list.py -> build/lib.linux-x86_64-3.6/detectron2/structures\n",
            "copying detectron2/structures/instances.py -> build/lib.linux-x86_64-3.6/detectron2/structures\n",
            "copying detectron2/structures/boxes.py -> build/lib.linux-x86_64-3.6/detectron2/structures\n",
            "copying detectron2/structures/__init__.py -> build/lib.linux-x86_64-3.6/detectron2/structures\n",
            "copying detectron2/structures/masks.py -> build/lib.linux-x86_64-3.6/detectron2/structures\n",
            "copying detectron2/structures/rotated_boxes.py -> build/lib.linux-x86_64-3.6/detectron2/structures\n",
            "copying detectron2/structures/keypoints.py -> build/lib.linux-x86_64-3.6/detectron2/structures\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/utils\n",
            "copying detectron2/utils/visualizer.py -> build/lib.linux-x86_64-3.6/detectron2/utils\n",
            "copying detectron2/utils/memory.py -> build/lib.linux-x86_64-3.6/detectron2/utils\n",
            "copying detectron2/utils/colormap.py -> build/lib.linux-x86_64-3.6/detectron2/utils\n",
            "copying detectron2/utils/logger.py -> build/lib.linux-x86_64-3.6/detectron2/utils\n",
            "copying detectron2/utils/comm.py -> build/lib.linux-x86_64-3.6/detectron2/utils\n",
            "copying detectron2/utils/video_visualizer.py -> build/lib.linux-x86_64-3.6/detectron2/utils\n",
            "copying detectron2/utils/events.py -> build/lib.linux-x86_64-3.6/detectron2/utils\n",
            "copying detectron2/utils/__init__.py -> build/lib.linux-x86_64-3.6/detectron2/utils\n",
            "copying detectron2/utils/env.py -> build/lib.linux-x86_64-3.6/detectron2/utils\n",
            "copying detectron2/utils/serialize.py -> build/lib.linux-x86_64-3.6/detectron2/utils\n",
            "copying detectron2/utils/registry.py -> build/lib.linux-x86_64-3.6/detectron2/utils\n",
            "copying detectron2/utils/collect_env.py -> build/lib.linux-x86_64-3.6/detectron2/utils\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/modeling\n",
            "copying detectron2/modeling/postprocessing.py -> build/lib.linux-x86_64-3.6/detectron2/modeling\n",
            "copying detectron2/modeling/matcher.py -> build/lib.linux-x86_64-3.6/detectron2/modeling\n",
            "copying detectron2/modeling/sampling.py -> build/lib.linux-x86_64-3.6/detectron2/modeling\n",
            "copying detectron2/modeling/box_regression.py -> build/lib.linux-x86_64-3.6/detectron2/modeling\n",
            "copying detectron2/modeling/test_time_augmentation.py -> build/lib.linux-x86_64-3.6/detectron2/modeling\n",
            "copying detectron2/modeling/__init__.py -> build/lib.linux-x86_64-3.6/detectron2/modeling\n",
            "copying detectron2/modeling/anchor_generator.py -> build/lib.linux-x86_64-3.6/detectron2/modeling\n",
            "copying detectron2/modeling/poolers.py -> build/lib.linux-x86_64-3.6/detectron2/modeling\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/model_zoo\n",
            "copying detectron2/model_zoo/__init__.py -> build/lib.linux-x86_64-3.6/detectron2/model_zoo\n",
            "copying detectron2/model_zoo/model_zoo.py -> build/lib.linux-x86_64-3.6/detectron2/model_zoo\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/layers\n",
            "copying detectron2/layers/deform_conv.py -> build/lib.linux-x86_64-3.6/detectron2/layers\n",
            "copying detectron2/layers/roi_align.py -> build/lib.linux-x86_64-3.6/detectron2/layers\n",
            "copying detectron2/layers/shape_spec.py -> build/lib.linux-x86_64-3.6/detectron2/layers\n",
            "copying detectron2/layers/mask_ops.py -> build/lib.linux-x86_64-3.6/detectron2/layers\n",
            "copying detectron2/layers/wrappers.py -> build/lib.linux-x86_64-3.6/detectron2/layers\n",
            "copying detectron2/layers/batch_norm.py -> build/lib.linux-x86_64-3.6/detectron2/layers\n",
            "copying detectron2/layers/__init__.py -> build/lib.linux-x86_64-3.6/detectron2/layers\n",
            "copying detectron2/layers/rotated_boxes.py -> build/lib.linux-x86_64-3.6/detectron2/layers\n",
            "copying detectron2/layers/nms.py -> build/lib.linux-x86_64-3.6/detectron2/layers\n",
            "copying detectron2/layers/roi_align_rotated.py -> build/lib.linux-x86_64-3.6/detectron2/layers\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/solver\n",
            "copying detectron2/solver/build.py -> build/lib.linux-x86_64-3.6/detectron2/solver\n",
            "copying detectron2/solver/lr_scheduler.py -> build/lib.linux-x86_64-3.6/detectron2/solver\n",
            "copying detectron2/solver/__init__.py -> build/lib.linux-x86_64-3.6/detectron2/solver\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/config\n",
            "copying detectron2/config/defaults.py -> build/lib.linux-x86_64-3.6/detectron2/config\n",
            "copying detectron2/config/config.py -> build/lib.linux-x86_64-3.6/detectron2/config\n",
            "copying detectron2/config/__init__.py -> build/lib.linux-x86_64-3.6/detectron2/config\n",
            "copying detectron2/config/compat.py -> build/lib.linux-x86_64-3.6/detectron2/config\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/checkpoint\n",
            "copying detectron2/checkpoint/detection_checkpoint.py -> build/lib.linux-x86_64-3.6/detectron2/checkpoint\n",
            "copying detectron2/checkpoint/c2_model_loading.py -> build/lib.linux-x86_64-3.6/detectron2/checkpoint\n",
            "copying detectron2/checkpoint/__init__.py -> build/lib.linux-x86_64-3.6/detectron2/checkpoint\n",
            "copying detectron2/checkpoint/catalog.py -> build/lib.linux-x86_64-3.6/detectron2/checkpoint\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/data/transforms\n",
            "copying detectron2/data/transforms/transform.py -> build/lib.linux-x86_64-3.6/detectron2/data/transforms\n",
            "copying detectron2/data/transforms/__init__.py -> build/lib.linux-x86_64-3.6/detectron2/data/transforms\n",
            "copying detectron2/data/transforms/transform_gen.py -> build/lib.linux-x86_64-3.6/detectron2/data/transforms\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/data/datasets\n",
            "copying detectron2/data/datasets/builtin.py -> build/lib.linux-x86_64-3.6/detectron2/data/datasets\n",
            "copying detectron2/data/datasets/register_coco.py -> build/lib.linux-x86_64-3.6/detectron2/data/datasets\n",
            "copying detectron2/data/datasets/lvis.py -> build/lib.linux-x86_64-3.6/detectron2/data/datasets\n",
            "copying detectron2/data/datasets/cityscapes.py -> build/lib.linux-x86_64-3.6/detectron2/data/datasets\n",
            "copying detectron2/data/datasets/__init__.py -> build/lib.linux-x86_64-3.6/detectron2/data/datasets\n",
            "copying detectron2/data/datasets/builtin_meta.py -> build/lib.linux-x86_64-3.6/detectron2/data/datasets\n",
            "copying detectron2/data/datasets/lvis_v0_5_categories.py -> build/lib.linux-x86_64-3.6/detectron2/data/datasets\n",
            "copying detectron2/data/datasets/pascal_voc.py -> build/lib.linux-x86_64-3.6/detectron2/data/datasets\n",
            "copying detectron2/data/datasets/coco.py -> build/lib.linux-x86_64-3.6/detectron2/data/datasets\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/data/samplers\n",
            "copying detectron2/data/samplers/grouped_batch_sampler.py -> build/lib.linux-x86_64-3.6/detectron2/data/samplers\n",
            "copying detectron2/data/samplers/__init__.py -> build/lib.linux-x86_64-3.6/detectron2/data/samplers\n",
            "copying detectron2/data/samplers/distributed_sampler.py -> build/lib.linux-x86_64-3.6/detectron2/data/samplers\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/modeling/backbone\n",
            "copying detectron2/modeling/backbone/build.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/backbone\n",
            "copying detectron2/modeling/backbone/backbone.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/backbone\n",
            "copying detectron2/modeling/backbone/fpn.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/backbone\n",
            "copying detectron2/modeling/backbone/__init__.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/backbone\n",
            "copying detectron2/modeling/backbone/resnet.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/backbone\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/modeling/proposal_generator\n",
            "copying detectron2/modeling/proposal_generator/build.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/proposal_generator\n",
            "copying detectron2/modeling/proposal_generator/rpn.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/proposal_generator\n",
            "copying detectron2/modeling/proposal_generator/rpn_outputs.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/proposal_generator\n",
            "copying detectron2/modeling/proposal_generator/rrpn.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/proposal_generator\n",
            "copying detectron2/modeling/proposal_generator/__init__.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/proposal_generator\n",
            "copying detectron2/modeling/proposal_generator/proposal_utils.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/proposal_generator\n",
            "copying detectron2/modeling/proposal_generator/rrpn_outputs.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/proposal_generator\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/modeling/roi_heads\n",
            "copying detectron2/modeling/roi_heads/box_head.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/roi_heads\n",
            "copying detectron2/modeling/roi_heads/keypoint_head.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/roi_heads\n",
            "copying detectron2/modeling/roi_heads/rotated_fast_rcnn.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/roi_heads\n",
            "copying detectron2/modeling/roi_heads/cascade_rcnn.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/roi_heads\n",
            "copying detectron2/modeling/roi_heads/fast_rcnn.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/roi_heads\n",
            "copying detectron2/modeling/roi_heads/__init__.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/roi_heads\n",
            "copying detectron2/modeling/roi_heads/roi_heads.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/roi_heads\n",
            "copying detectron2/modeling/roi_heads/mask_head.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/roi_heads\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/modeling/meta_arch\n",
            "copying detectron2/modeling/meta_arch/rcnn.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/meta_arch\n",
            "copying detectron2/modeling/meta_arch/build.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/meta_arch\n",
            "copying detectron2/modeling/meta_arch/semantic_seg.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/meta_arch\n",
            "copying detectron2/modeling/meta_arch/retinanet.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/meta_arch\n",
            "copying detectron2/modeling/meta_arch/__init__.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/meta_arch\n",
            "copying detectron2/modeling/meta_arch/panoptic_fpn.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/meta_arch\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs\n",
            "copying detectron2/model_zoo/configs/Base-RetinaNet.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs\n",
            "copying detectron2/model_zoo/configs/Base-RCNN-C4.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs\n",
            "copying detectron2/model_zoo/configs/Base-RCNN-FPN.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs\n",
            "copying detectron2/model_zoo/configs/Base-RCNN-DilatedC5.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/VG-Detection\n",
            "copying detectron2/model_zoo/configs/VG-Detection/faster_rcnn_R_101_C4_attr_caffemaxpool.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/VG-Detection\n",
            "copying detectron2/model_zoo/configs/VG-Detection/faster_rcnn_R_101_C4_caffe.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/VG-Detection\n",
            "copying detectron2/model_zoo/configs/VG-Detection/faster_rcnn_R_101_C4_caffemaxpool.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/VG-Detection\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/Detectron1-Comparisons\n",
            "copying detectron2/model_zoo/configs/Detectron1-Comparisons/mask_rcnn_R_50_FPN_noaug_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/Detectron1-Comparisons\n",
            "copying detectron2/model_zoo/configs/Detectron1-Comparisons/keypoint_rcnn_R_50_FPN_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/Detectron1-Comparisons\n",
            "copying detectron2/model_zoo/configs/Detectron1-Comparisons/faster_rcnn_R_50_FPN_noaug_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/Detectron1-Comparisons\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/semantic_R_50_FPN_instant_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/mask_rcnn_R_50_DC5_inference_acc_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/panoptic_fpn_R_50_inference_acc_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/semantic_R_50_FPN_inference_acc_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/panoptic_fpn_R_50_instant_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/rpn_R_50_FPN_instant_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/keypoint_rcnn_R_50_FPN_training_acc_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/keypoint_rcnn_R_50_FPN_normalized_training_acc_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/rpn_R_50_FPN_inference_acc_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/mask_rcnn_R_50_C4_training_acc_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/mask_rcnn_R_50_FPN_training_acc_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/mask_rcnn_R_50_C4_inference_acc_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/mask_rcnn_R_50_C4_instant_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/mask_rcnn_R_50_FPN_inference_acc_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/mask_rcnn_R_50_FPN_instant_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/panoptic_fpn_R_50_training_acc_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/fast_rcnn_R_50_FPN_inference_acc_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/keypoint_rcnn_R_50_FPN_inference_acc_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/semantic_R_50_FPN_training_acc_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/keypoint_rcnn_R_50_FPN_instant_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/retinanet_R_50_FPN_instant_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/fast_rcnn_R_50_FPN_instant_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/retinanet_R_50_FPN_inference_acc_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/LVIS-InstanceSegmentation\n",
            "copying detectron2/model_zoo/configs/LVIS-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/LVIS-InstanceSegmentation\n",
            "copying detectron2/model_zoo/configs/LVIS-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/LVIS-InstanceSegmentation\n",
            "copying detectron2/model_zoo/configs/LVIS-InstanceSegmentation/mask_rcnn_R_101_FPN_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/LVIS-InstanceSegmentation\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/PascalVOC-Detection\n",
            "copying detectron2/model_zoo/configs/PascalVOC-Detection/faster_rcnn_R_50_FPN.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/PascalVOC-Detection\n",
            "copying detectron2/model_zoo/configs/PascalVOC-Detection/faster_rcnn_R_50_C4.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/PascalVOC-Detection\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/Misc\n",
            "copying detectron2/model_zoo/configs/Misc/mask_rcnn_R_50_FPN_3x_dconv_c3-c5.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/Misc\n",
            "copying detectron2/model_zoo/configs/Misc/panoptic_fpn_R_101_dconv_cascade_gn_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/Misc\n",
            "copying detectron2/model_zoo/configs/Misc/mask_rcnn_R_50_FPN_1x_dconv_c3-c5.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/Misc\n",
            "copying detectron2/model_zoo/configs/Misc/scratch_mask_rcnn_R_50_FPN_3x_gn.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/Misc\n",
            "copying detectron2/model_zoo/configs/Misc/semantic_R_50_FPN_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/Misc\n",
            "copying detectron2/model_zoo/configs/Misc/cascade_mask_rcnn_R_50_FPN_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/Misc\n",
            "copying detectron2/model_zoo/configs/Misc/cascade_mask_rcnn_X_152_32x8d_FPN_IN5k_gn_dconv.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/Misc\n",
            "copying detectron2/model_zoo/configs/Misc/cascade_mask_rcnn_R_50_FPN_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/Misc\n",
            "copying detectron2/model_zoo/configs/Misc/mask_rcnn_R_50_FPN_1x_cls_agnostic.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/Misc\n",
            "copying detectron2/model_zoo/configs/Misc/mask_rcnn_R_50_FPN_3x_gn.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/Misc\n",
            "copying detectron2/model_zoo/configs/Misc/mask_rcnn_R_50_FPN_3x_syncbn.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/Misc\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_101_C4_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_101_DC5_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_C4_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_DC5_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_C4_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_DC5_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Keypoints\n",
            "copying detectron2/model_zoo/configs/COCO-Keypoints/keypoint_rcnn_X_101_32x8d_FPN_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Keypoints\n",
            "copying detectron2/model_zoo/configs/COCO-Keypoints/Base-Keypoint-RCNN-FPN.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Keypoints\n",
            "copying detectron2/model_zoo/configs/COCO-Keypoints/keypoint_rcnn_R_50_FPN_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Keypoints\n",
            "copying detectron2/model_zoo/configs/COCO-Keypoints/keypoint_rcnn_R_101_FPN_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Keypoints\n",
            "copying detectron2/model_zoo/configs/COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Keypoints\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/Cityscapes\n",
            "copying detectron2/model_zoo/configs/Cityscapes/mask_rcnn_R_50_FPN.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/Cityscapes\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Detection\n",
            "copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_R_101_C4_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Detection\n",
            "copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_R_50_DC5_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Detection\n",
            "copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Detection\n",
            "copying detectron2/model_zoo/configs/COCO-Detection/retinanet_R_50_FPN_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Detection\n",
            "copying detectron2/model_zoo/configs/COCO-Detection/rpn_R_50_FPN_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Detection\n",
            "copying detectron2/model_zoo/configs/COCO-Detection/retinanet_R_101_FPN_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Detection\n",
            "copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Detection\n",
            "copying detectron2/model_zoo/configs/COCO-Detection/rpn_R_50_C4_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Detection\n",
            "copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_R_50_DC5_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Detection\n",
            "copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Detection\n",
            "copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Detection\n",
            "copying detectron2/model_zoo/configs/COCO-Detection/fast_rcnn_R_50_FPN_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Detection\n",
            "copying detectron2/model_zoo/configs/COCO-Detection/retinanet_R_50_FPN_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Detection\n",
            "copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_R_101_DC5_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Detection\n",
            "copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_R_50_C4_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Detection\n",
            "copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_R_50_C4_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Detection\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-PanopticSegmentation\n",
            "copying detectron2/model_zoo/configs/COCO-PanopticSegmentation/panoptic_fpn_R_50_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-PanopticSegmentation\n",
            "copying detectron2/model_zoo/configs/COCO-PanopticSegmentation/panoptic_fpn_R_50_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-PanopticSegmentation\n",
            "copying detectron2/model_zoo/configs/COCO-PanopticSegmentation/Base-Panoptic-FPN.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-PanopticSegmentation\n",
            "copying detectron2/model_zoo/configs/COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-PanopticSegmentation\n",
            "running build_ext\n",
            "building 'detectron2._C' extension\n",
            "creating build/temp.linux-x86_64-3.6\n",
            "creating build/temp.linux-x86_64-3.6/content\n",
            "creating build/temp.linux-x86_64-3.6/content/py-bottom-up-attention\n",
            "creating build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2\n",
            "creating build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers\n",
            "creating build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc\n",
            "creating build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated\n",
            "creating build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/box_iou_rotated\n",
            "creating build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlign\n",
            "creating build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlignRotated\n",
            "creating build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/deformable\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DWITH_CUDA -I/content/py-bottom-up-attention/detectron2/layers/csrc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c /content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp -o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/vision.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint detectron2::deform_conv_forward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, int, int, int, int, int, int, int, int, int, int, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::\u001b[01;35m\u001b[Kd\u001b[m\u001b[Keprecated_AT_CHECK();                 \\\n",
            "                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:136:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(weight.type().is_cuda(), \"weight tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::deprecated_AT_CHECK(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                 \\\n",
            "                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:136:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(weight.type().is_cuda(), \"weight tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::\u001b[01;35m\u001b[Kd\u001b[m\u001b[Keprecated_AT_CHECK();                 \\\n",
            "                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:137:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(offset.type().is_cuda(), \"offset tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::deprecated_AT_CHECK(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                 \\\n",
            "                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:137:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(offset.type().is_cuda(), \"offset tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint detectron2::deform_conv_backward_input(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, int, int, int, int, int, int, int, int, int, int, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::\u001b[01;35m\u001b[Kd\u001b[m\u001b[Keprecated_AT_CHECK();                 \\\n",
            "                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:184:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(input.type().is_cuda(), \"input tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::deprecated_AT_CHECK(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                 \\\n",
            "                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:184:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(input.type().is_cuda(), \"input tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::\u001b[01;35m\u001b[Kd\u001b[m\u001b[Keprecated_AT_CHECK();                 \\\n",
            "                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:185:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(weight.type().is_cuda(), \"weight tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::deprecated_AT_CHECK(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                 \\\n",
            "                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:185:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(weight.type().is_cuda(), \"weight tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::\u001b[01;35m\u001b[Kd\u001b[m\u001b[Keprecated_AT_CHECK();                 \\\n",
            "                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:186:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(offset.type().is_cuda(), \"offset tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::deprecated_AT_CHECK(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                 \\\n",
            "                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:186:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(offset.type().is_cuda(), \"offset tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint detectron2::deform_conv_backward_filter(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, int, int, int, int, int, int, int, int, int, int, float, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::\u001b[01;35m\u001b[Kd\u001b[m\u001b[Keprecated_AT_CHECK();                 \\\n",
            "                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:234:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(input.type().is_cuda(), \"input tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::deprecated_AT_CHECK(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                 \\\n",
            "                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:234:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(input.type().is_cuda(), \"input tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::\u001b[01;35m\u001b[Kd\u001b[m\u001b[Keprecated_AT_CHECK();                 \\\n",
            "                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:235:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(offset.type().is_cuda(), \"offset tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::deprecated_AT_CHECK(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                 \\\n",
            "                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:235:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(offset.type().is_cuda(), \"offset tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid detectron2::modulated_deform_conv_forward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, int, int, int, int, int, int, int, int, int, int, bool)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::\u001b[01;35m\u001b[Kd\u001b[m\u001b[Keprecated_AT_CHECK();                 \\\n",
            "                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:284:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(weight.type().is_cuda(), \"weight tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::deprecated_AT_CHECK(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                 \\\n",
            "                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:284:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(weight.type().is_cuda(), \"weight tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::\u001b[01;35m\u001b[Kd\u001b[m\u001b[Keprecated_AT_CHECK();                 \\\n",
            "                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:285:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(bias.type().is_cuda(), \"bias tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::deprecated_AT_CHECK(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                 \\\n",
            "                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:285:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(bias.type().is_cuda(), \"bias tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::\u001b[01;35m\u001b[Kd\u001b[m\u001b[Keprecated_AT_CHECK();                 \\\n",
            "                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:286:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(offset.type().is_cuda(), \"offset tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::deprecated_AT_CHECK(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                 \\\n",
            "                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:286:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(offset.type().is_cuda(), \"offset tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid detectron2::modulated_deform_conv_backward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, int, int, int, int, int, int, int, int, int, int, bool)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::\u001b[01;35m\u001b[Kd\u001b[m\u001b[Keprecated_AT_CHECK();                 \\\n",
            "                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:341:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(input.type().is_cuda(), \"input tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::deprecated_AT_CHECK(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                 \\\n",
            "                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:341:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(input.type().is_cuda(), \"input tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::\u001b[01;35m\u001b[Kd\u001b[m\u001b[Keprecated_AT_CHECK();                 \\\n",
            "                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:342:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(weight.type().is_cuda(), \"weight tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::deprecated_AT_CHECK(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                 \\\n",
            "                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:342:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(weight.type().is_cuda(), \"weight tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::\u001b[01;35m\u001b[Kd\u001b[m\u001b[Keprecated_AT_CHECK();                 \\\n",
            "                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:343:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(bias.type().is_cuda(), \"bias tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::deprecated_AT_CHECK(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                 \\\n",
            "                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:343:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(bias.type().is_cuda(), \"bias tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::\u001b[01;35m\u001b[Kd\u001b[m\u001b[Keprecated_AT_CHECK();                 \\\n",
            "                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:344:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(offset.type().is_cuda(), \"offset tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::deprecated_AT_CHECK(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                 \\\n",
            "                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:344:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(offset.type().is_cuda(), \"offset tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DWITH_CUDA -I/content/py-bottom-up-attention/detectron2/layers/csrc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c /content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cpu.cpp -o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cpu.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cpu.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cpu.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:103:56:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     at::ScalarType _st = ::detail::scalar_type(the_type\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                    \\\n",
            "                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cpu.cpp:67:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K(dets.type(), \"nms_rotated\", [&] {\n",
            "   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:31:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline at::ScalarType \u001b[01;36m\u001b[Kscalar_type\u001b[m\u001b[K(const at::DeprecatedTypeProperties &t) {\n",
            "                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DWITH_CUDA -I/content/py-bottom-up-attention/detectron2/layers/csrc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c /content/py-bottom-up-attention/detectron2/layers/csrc/box_iou_rotated/box_iou_rotated_cpu.cpp -o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/box_iou_rotated/box_iou_rotated_cpu.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DWITH_CUDA -I/content/py-bottom-up-attention/detectron2/layers/csrc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c /content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlign/ROIAlign_cpu.cpp -o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlign/ROIAlign_cpu.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlign/ROIAlign.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlign/ROIAlign_cpu.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlign/ROIAlign_cpu.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:116:56:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     at::ScalarType _st = ::detail::scalar_type(the_type\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                    \\\n",
            "                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlign/ROIAlign_cpu.cpp:429:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"ROIAlign_forward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:31:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline at::ScalarType \u001b[01;36m\u001b[Kscalar_type\u001b[m\u001b[K(const at::DeprecatedTypeProperties &t) {\n",
            "                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlign/ROIAlign_cpu.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:116:56:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     at::ScalarType _st = ::detail::scalar_type(the_type\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                    \\\n",
            "                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlign/ROIAlign_cpu.cpp:481:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_DISPATCH_FLOATING_TYPES_AND_HALF(grad.type(), \"ROIAlign_forward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:31:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline at::ScalarType \u001b[01;36m\u001b[Kscalar_type\u001b[m\u001b[K(const at::DeprecatedTypeProperties &t) {\n",
            "                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DWITH_CUDA -I/content/py-bottom-up-attention/detectron2/layers/csrc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c /content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp -o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:116:56:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     at::ScalarType _st = ::detail::scalar_type(the_type\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                    \\\n",
            "                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:446:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_DISPATCH_FLOATING_TYPES_AND_HALF(\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:31:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline at::ScalarType \u001b[01;36m\u001b[Kscalar_type\u001b[m\u001b[K(const at::DeprecatedTypeProperties &t) {\n",
            "                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:116:56:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     at::ScalarType _st = ::detail::scalar_type(the_type\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                    \\\n",
            "                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:497:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_DISPATCH_FLOATING_TYPES_AND_HALF(\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:31:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline at::ScalarType \u001b[01;36m\u001b[Kscalar_type\u001b[m\u001b[K(const at::DeprecatedTypeProperties &t) {\n",
            "                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "/usr/local/cuda/bin/nvcc -DWITH_CUDA -I/content/py-bottom-up-attention/detectron2/layers/csrc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c /content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++11\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda_kernel.cu:486:100:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n",
            "                                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:31:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda_kernel.cu:555:101:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n",
            "                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:31:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda_kernel.cu:625:101:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n",
            "                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:31:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda_kernel.cu:1098:100:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n",
            "                                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:31:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda_kernel.cu:1168:101:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n",
            "                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:31:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda_kernel.cu:1241:101:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n",
            "                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:31:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "/usr/local/cuda/bin/nvcc -DWITH_CUDA -I/content/py-bottom-up-attention/detectron2/layers/csrc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c /content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu -o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++11\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint detectron2::deform_conv_forward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, int, int, int, int, int, int, int, int, int, int, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:136:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(weight.\u001b[01;35m\u001b[Ktype().is_cuda(), \"\u001b[m\u001b[Kweight tensor is not on GPU!\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:136:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(weight.type().is_cuda(), \"w\u001b[01;35m\u001b[Ke\u001b[m\u001b[Kight tensor is not on GPU!\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:137:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(offset.\u001b[01;35m\u001b[Ktype().is_cuda(), \"\u001b[m\u001b[Koffset tensor is not on GPU!\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:137:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(offset.type().is_cuda(), \"o\u001b[01;35m\u001b[Kf\u001b[m\u001b[Kfset tensor is not on GPU!\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint detectron2::deform_conv_backward_input(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, int, int, int, int, int, int, int, int, int, int, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:184:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(input.t\u001b[01;35m\u001b[Kype().is_cuda(), \"i\u001b[m\u001b[Knput tensor is not on GPU!\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:184:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(input.type().is_cuda(), \"in\u001b[01;35m\u001b[Kp\u001b[m\u001b[Kut tensor is not on GPU!\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:185:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(weight.\u001b[01;35m\u001b[Ktype().is_cuda(), \"\u001b[m\u001b[Kweight tensor is not on GPU!\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:185:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(weight.type().is_cuda(), \"w\u001b[01;35m\u001b[Ke\u001b[m\u001b[Kight tensor is not on GPU!\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:186:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(offset.\u001b[01;35m\u001b[Ktype().is_cuda(), \"\u001b[m\u001b[Koffset tensor is not on GPU!\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:186:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(offset.type().is_cuda(), \"o\u001b[01;35m\u001b[Kf\u001b[m\u001b[Kfset tensor is not on GPU!\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint detectron2::deform_conv_backward_filter(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, int, int, int, int, int, int, int, int, int, int, float, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:234:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(input.t\u001b[01;35m\u001b[Kype().is_cuda(), \"i\u001b[m\u001b[Knput tensor is not on GPU!\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:234:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(input.type().is_cuda(), \"in\u001b[01;35m\u001b[Kp\u001b[m\u001b[Kut tensor is not on GPU!\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:235:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(offset.\u001b[01;35m\u001b[Ktype().is_cuda(), \"\u001b[m\u001b[Koffset tensor is not on GPU!\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:235:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(offset.type().is_cuda(), \"o\u001b[01;35m\u001b[Kf\u001b[m\u001b[Kfset tensor is not on GPU!\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid detectron2::modulated_deform_conv_forward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, int, int, int, int, int, int, int, int, int, int, bool)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:284:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(weight.\u001b[01;35m\u001b[Ktype().is_cuda(), \"\u001b[m\u001b[Kweight tensor is not on GPU!\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:284:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(weight.type().is_cuda(), \"w\u001b[01;35m\u001b[Ke\u001b[m\u001b[Kight tensor is not on GPU!\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:285:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(bias.ty\u001b[01;35m\u001b[Kpe().is_cuda(), \"bi\u001b[m\u001b[Kas tensor is not on GPU!\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:285:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(bias.type().is_cuda(), \"bia\u001b[01;35m\u001b[Ks\u001b[m\u001b[K tensor is not on GPU!\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:286:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(offset.\u001b[01;35m\u001b[Ktype().is_cuda(), \"\u001b[m\u001b[Koffset tensor is not on GPU!\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:286:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(offset.type().is_cuda(), \"o\u001b[01;35m\u001b[Kf\u001b[m\u001b[Kfset tensor is not on GPU!\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid detectron2::modulated_deform_conv_backward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, int, int, int, int, int, int, int, int, int, int, bool)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:341:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(input.t\u001b[01;35m\u001b[Kype().is_cuda(), \"i\u001b[m\u001b[Knput tensor is not on GPU!\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:341:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(input.type().is_cuda(), \"in\u001b[01;35m\u001b[Kp\u001b[m\u001b[Kut tensor is not on GPU!\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:342:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(weight.\u001b[01;35m\u001b[Ktype().is_cuda(), \"\u001b[m\u001b[Kweight tensor is not on GPU!\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:342:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(weight.type().is_cuda(), \"w\u001b[01;35m\u001b[Ke\u001b[m\u001b[Kight tensor is not on GPU!\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:343:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(bias.ty\u001b[01;35m\u001b[Kpe().is_cuda(), \"bi\u001b[m\u001b[Kas tensor is not on GPU!\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:343:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(bias.type().is_cuda(), \"bia\u001b[01;35m\u001b[Ks\u001b[m\u001b[K tensor is not on GPU!\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:344:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(offset.\u001b[01;35m\u001b[Ktype().is_cuda(), \"\u001b[m\u001b[Koffset tensor is not on GPU!\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:344:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(offset.type().is_cuda(), \"o\u001b[01;35m\u001b[Kf\u001b[m\u001b[Kfset tensor is not on GPU!\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid detectron2::shape_check(at::Tensor, at::Tensor, at::Tensor*, at::Tensor, int, int, int, int, int, int, int, int, int, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:155:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K                  \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:155:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:161:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(weight.is\u001b[01;35m\u001b[K_contiguous(), \"wei\u001b[m\u001b[Kght tensor has to be contiguous\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:161:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(weight.is_contiguous(), \"weig\u001b[01;35m\u001b[Kh\u001b[m\u001b[Kt tensor has to be contiguous\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:163:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K                  \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:163:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:169:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K                  \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:169:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:178:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K                  \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:178:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:184:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K                  \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:184:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:201:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K                  \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:201:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:215:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K                  \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:215:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:230:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K                  \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:230:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:236:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K                  \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:236:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:240:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K                  \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:240:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:249:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K                  \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:249:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:254:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(\n",
            "                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K                  \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:254:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:260:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(\n",
            "                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K                  \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:260:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint detectron2::deform_conv_forward_cuda(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, int, int, int, int, int, int, int, int, int, int, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:338:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK((offset.s\u001b[01;35m\u001b[Kize(0) == batchSize\u001b[m\u001b[K), \"invalid batch size of offset\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:338:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK((offset.size(0) == batchSize)\u001b[01;35m\u001b[K,\u001b[m\u001b[K \"invalid batch size of offset\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint detectron2::deform_conv_backward_input_cuda(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, int, int, int, int, int, int, int, int, int, int, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:503:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK((offset.s\u001b[01;35m\u001b[Kize(0) == batchSize\u001b[m\u001b[K), 3, \"invalid batch size of offset\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:503:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK((offset.size(0) == batchSize)\u001b[01;35m\u001b[K,\u001b[m\u001b[K 3, \"invalid batch size of offset\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint detectron2::deform_conv_backward_parameters_cuda(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, int, int, int, int, int, int, int, int, int, int, float, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:696:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK((offset.s\u001b[01;35m\u001b[Kize(0) == batchSize\u001b[m\u001b[K), \"invalid batch size of offset\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:696:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK((offset.size(0) == batchSize)\u001b[01;35m\u001b[K,\u001b[m\u001b[K \"invalid batch size of offset\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid detectron2::modulated_deform_conv_cuda_forward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, int, int, int, int, int, int, int, int, int, int, bool)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:823:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(input.is_\u001b[01;35m\u001b[Kcontiguous(), \"inpu\u001b[m\u001b[Kt tensor has to be contiguous\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:823:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(input.is_contiguous(), \"input\u001b[01;35m\u001b[K \u001b[m\u001b[Ktensor has to be contiguous\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:824:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(weight.is\u001b[01;35m\u001b[K_contiguous(), \"wei\u001b[m\u001b[Kght tensor has to be contiguous\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:824:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(weight.is_contiguous(), \"weig\u001b[01;35m\u001b[Kh\u001b[m\u001b[Kt tensor has to be contiguous\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid detectron2::modulated_deform_conv_cuda_backward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, int, int, int, int, int, int, int, int, int, int, bool)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:953:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(input.is_\u001b[01;35m\u001b[Kcontiguous(), \"inpu\u001b[m\u001b[Kt tensor has to be contiguous\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:953:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(input.is_contiguous(), \"input\u001b[01;35m\u001b[K \u001b[m\u001b[Ktensor has to be contiguous\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:954:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(weight.is\u001b[01;35m\u001b[K_contiguous(), \"wei\u001b[m\u001b[Kght tensor has to be contiguous\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:954:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(weight.is_contiguous(), \"weig\u001b[01;35m\u001b[Kh\u001b[m\u001b[Kt tensor has to be contiguous\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "/usr/local/cuda/bin/nvcc -DWITH_CUDA -I/content/py-bottom-up-attention/detectron2/layers/csrc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c /content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cuda.cu -o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++11\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cuda.cu:97:104:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n",
            "                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:31:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cuda.cu:97:350:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n",
            "                                                                                                                                                                                                                                                                                                                                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cuda.cu:97:402:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cuda.cu:97:639:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cuda.cu:97:691:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cuda.cu:97:939:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cuda.cu:97:991:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kat::Tensor detectron2::nms_rotated_cuda(const at::Tensor&, const at::Tensor&, float)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cuda.cu:107:84:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   unsigned long long* mask_host = (unsigned long long*)mask_cpu.data<int64_t>();\n",
            "                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cuda.cu:114:46:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   int64_t* keep_out = keep.data<int64_t>();\n",
            "                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "/usr/local/cuda/bin/nvcc -DWITH_CUDA -I/content/py-bottom-up-attention/detectron2/layers/csrc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c /content/py-bottom-up-attention/detectron2/layers/csrc/box_iou_rotated/box_iou_rotated_cuda.cu -o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/box_iou_rotated/box_iou_rotated_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++11\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/cuda/bin/nvcc -DWITH_CUDA -I/content/py-bottom-up-attention/detectron2/layers/csrc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c /content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlign/ROIAlign_cuda.cu -o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlign/ROIAlign_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++11\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/cuda/bin/nvcc -DWITH_CUDA -I/content/py-bottom-up-attention/detectron2/layers/csrc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c /content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cuda.cu -o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++11\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/cuda/bin/nvcc -DWITH_CUDA -I/content/py-bottom-up-attention/detectron2/layers/csrc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c /content/py-bottom-up-attention/detectron2/layers/csrc/cuda_version.cu -o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/cuda_version.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++11\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/vision.o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cpu.o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/box_iou_rotated/box_iou_rotated_cpu.o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlign/ROIAlign_cpu.o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda_kernel.o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cuda.o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/box_iou_rotated/box_iou_rotated_cuda.o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlign/ROIAlign_cuda.o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cuda.o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/cuda_version.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/detectron2/_C.cpython-36m-x86_64-linux-gnu.so\n",
            "running develop\n",
            "running egg_info\n",
            "creating detectron2.egg-info\n",
            "writing detectron2.egg-info/PKG-INFO\n",
            "writing dependency_links to detectron2.egg-info/dependency_links.txt\n",
            "writing requirements to detectron2.egg-info/requires.txt\n",
            "writing top-level names to detectron2.egg-info/top_level.txt\n",
            "writing manifest file 'detectron2.egg-info/SOURCES.txt'\n",
            "writing manifest file 'detectron2.egg-info/SOURCES.txt'\n",
            "running build_ext\n",
            "copying build/lib.linux-x86_64-3.6/detectron2/_C.cpython-36m-x86_64-linux-gnu.so -> detectron2\n",
            "Creating /usr/local/lib/python3.6/dist-packages/detectron2.egg-link (link to .)\n",
            "Adding detectron2 0.1 to easy-install.pth file\n",
            "\n",
            "Installed /content/py-bottom-up-attention\n",
            "Processing dependencies for detectron2==0.1\n",
            "Searching for imagesize==1.2.0\n",
            "Best match: imagesize 1.2.0\n",
            "Adding imagesize 1.2.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for tensorboard==2.3.0\n",
            "Best match: tensorboard 2.3.0\n",
            "Adding tensorboard 2.3.0 to easy-install.pth file\n",
            "Installing tensorboard script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for tqdm==4.50.0\n",
            "Best match: tqdm 4.50.0\n",
            "Adding tqdm 4.50.0 to easy-install.pth file\n",
            "Installing tqdm script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for matplotlib==3.2.2\n",
            "Best match: matplotlib 3.2.2\n",
            "Adding matplotlib 3.2.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for cloudpickle==1.3.0\n",
            "Best match: cloudpickle 1.3.0\n",
            "Adding cloudpickle 1.3.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for tabulate==0.8.7\n",
            "Best match: tabulate 0.8.7\n",
            "Adding tabulate 0.8.7 to easy-install.pth file\n",
            "Installing tabulate script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for yacs==0.1.8\n",
            "Best match: yacs 0.1.8\n",
            "Adding yacs 0.1.8 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Pillow==7.0.0\n",
            "Best match: Pillow 7.0.0\n",
            "Adding Pillow 7.0.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for termcolor==1.1.0\n",
            "Best match: termcolor 1.1.0\n",
            "Adding termcolor 1.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Werkzeug==1.0.1\n",
            "Best match: Werkzeug 1.0.1\n",
            "Adding Werkzeug 1.0.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Markdown==3.2.2\n",
            "Best match: Markdown 3.2.2\n",
            "Adding Markdown 3.2.2 to easy-install.pth file\n",
            "Installing markdown_py script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for setuptools==50.3.0\n",
            "Best match: setuptools 50.3.0\n",
            "Adding setuptools 50.3.0 to easy-install.pth file\n",
            "Installing easy_install script to /usr/local/bin\n",
            "Installing easy_install-3.8 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for protobuf==3.12.4\n",
            "Best match: protobuf 3.12.4\n",
            "Adding protobuf 3.12.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for google-auth-oauthlib==0.4.1\n",
            "Best match: google-auth-oauthlib 0.4.1\n",
            "Adding google-auth-oauthlib 0.4.1 to easy-install.pth file\n",
            "Installing google-oauthlib-tool script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for six==1.15.0\n",
            "Best match: six 1.15.0\n",
            "Adding six 1.15.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for grpcio==1.32.0\n",
            "Best match: grpcio 1.32.0\n",
            "Adding grpcio 1.32.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for numpy==1.18.5\n",
            "Best match: numpy 1.18.5\n",
            "Adding numpy 1.18.5 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.6 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for absl-py==0.10.0\n",
            "Best match: absl-py 0.10.0\n",
            "Adding absl-py 0.10.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for tensorboard-plugin-wit==1.7.0\n",
            "Best match: tensorboard-plugin-wit 1.7.0\n",
            "Adding tensorboard-plugin-wit 1.7.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for google-auth==1.17.2\n",
            "Best match: google-auth 1.17.2\n",
            "Adding google-auth 1.17.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for wheel==0.35.1\n",
            "Best match: wheel 0.35.1\n",
            "Adding wheel 0.35.1 to easy-install.pth file\n",
            "Installing wheel script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for requests==2.23.0\n",
            "Best match: requests 2.23.0\n",
            "Adding requests 2.23.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for cycler==0.10.0\n",
            "Best match: cycler 0.10.0\n",
            "Adding cycler 0.10.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for python-dateutil==2.8.1\n",
            "Best match: python-dateutil 2.8.1\n",
            "Adding python-dateutil 2.8.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for kiwisolver==1.2.0\n",
            "Best match: kiwisolver 1.2.0\n",
            "Adding kiwisolver 1.2.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for pyparsing==2.4.7\n",
            "Best match: pyparsing 2.4.7\n",
            "Adding pyparsing 2.4.7 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for PyYAML==5.3.1\n",
            "Best match: PyYAML 5.3.1\n",
            "Adding PyYAML 5.3.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for importlib-metadata==2.0.0\n",
            "Best match: importlib-metadata 2.0.0\n",
            "Adding importlib-metadata 2.0.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for requests-oauthlib==1.3.0\n",
            "Best match: requests-oauthlib 1.3.0\n",
            "Adding requests-oauthlib 1.3.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for pyasn1-modules==0.2.8\n",
            "Best match: pyasn1-modules 0.2.8\n",
            "Adding pyasn1-modules 0.2.8 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for rsa==4.6\n",
            "Best match: rsa 4.6\n",
            "Adding rsa 4.6 to easy-install.pth file\n",
            "Installing pyrsa-decrypt script to /usr/local/bin\n",
            "Installing pyrsa-encrypt script to /usr/local/bin\n",
            "Installing pyrsa-keygen script to /usr/local/bin\n",
            "Installing pyrsa-priv2pub script to /usr/local/bin\n",
            "Installing pyrsa-sign script to /usr/local/bin\n",
            "Installing pyrsa-verify script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for cachetools==4.1.1\n",
            "Best match: cachetools 4.1.1\n",
            "Adding cachetools 4.1.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for chardet==3.0.4\n",
            "Best match: chardet 3.0.4\n",
            "Adding chardet 3.0.4 to easy-install.pth file\n",
            "Installing chardetect script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for certifi==2020.6.20\n",
            "Best match: certifi 2020.6.20\n",
            "Adding certifi 2020.6.20 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for urllib3==1.24.3\n",
            "Best match: urllib3 1.24.3\n",
            "Adding urllib3 1.24.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for idna==2.10\n",
            "Best match: idna 2.10\n",
            "Adding idna 2.10 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for zipp==3.2.0\n",
            "Best match: zipp 3.2.0\n",
            "Adding zipp 3.2.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for oauthlib==3.1.0\n",
            "Best match: oauthlib 3.1.0\n",
            "Adding oauthlib 3.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for pyasn1==0.4.8\n",
            "Best match: pyasn1 0.4.8\n",
            "Adding pyasn1 0.4.8 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Finished processing dependencies for detectron2==0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8J2EhNrHcM6"
      },
      "source": [
        "###### **Extract!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgWtSfIJf-qX"
      },
      "source": [
        "os.chdir(\"/content/hateful_memes/region_feature_extraction/\")\n",
        "!python extract.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDjsV5jDAQpi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "44fa8676-ea51-43e1-808f-e5694f806126"
      },
      "source": [
        "!ls /content/features/ | wc -l"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16522\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEwh34kXBzOc"
      },
      "source": [
        "*What is the size of the training data?*\n",
        "> \n",
        "\n",
        "*Did you use additional dataset?*\n",
        "> Yes. I used Memotion as an additional dataset\n",
        "\n",
        "*Did you use pre-trained models?*\n",
        "> Yes. I used `VisualBERT` which was pre-trained on `Masked Conceptual Captions` dataset, see <font color='magenta'> <b> IV. Fine-tuning pre-trained VisualBERT models on Hateful Memes </b> </font>. Then, the model was fine-tuned on the HM dataset. The pre-trained model is available from MMF: [See all the available pre-trained VisualBERT models from MMF](https://github.com/facebookresearch/mmf/tree/master/projects/pretrain_vl_right)\n",
        "\n",
        "*Did you use default image features provided by MMF?*\n",
        "> No. I extracted our own image features using Facebook's Detectron model, which uses ResNet-152 as its backbone. See <font color='magenta'> <b> III. Feature Extraction </b> </font> part in the notebook.\n",
        "\n",
        "*What is the impact of `Majority Voting` technique in ROC-AUC score? And what do you think about the reason behind?*\n",
        ">\n",
        "\n",
        "**\n",
        ">"
      ]
    }
  ]
}